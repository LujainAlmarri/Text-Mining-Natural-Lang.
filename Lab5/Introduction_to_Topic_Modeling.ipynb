{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AoMY4Uew_AG"
      },
      "source": [
        "## Introduction\n",
        "##### How to get started with topic modeling using LDA in Python\n",
        "** **\n",
        "Topic Models, in a nutshell, are a type of statistical language models used for uncovering hidden structure in a collection of texts. In a practical and more intuitively, you can think of it as a task of:\n",
        "\n",
        "- **Dimensionality Reduction**, where rather than representing a text T in its feature space as {Word_i: count(Word_i, T) for Word_i in Vocabulary}, you can represent it in a topic space as {Topic_i: Weight(Topic_i, T) for Topic_i in Topics}\n",
        "- **Unsupervised Learning**, where it can be compared to clustering, as in the case of clustering, the number of topics, like the number of clusters, is an output parameter. By doing topic modeling, we build clusters of words rather than clusters of texts. A text is thus a mixture of all the topics, each having a specific weight\n",
        "- **Tagging**, abstract “topics” that occur in a collection of documents that best represents the information in them.\n",
        "\n",
        "There are several existing algorithms you can use to perform the topic modeling. The most common of it are, Latent Semantic Analysis (LSA/LSI), Probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "In this tutorial, we’ll take a closer look at LDA, and implement our first topic model using the sklearn implementation in python 2.7\n",
        "\n",
        "### Theoretical Overview\n",
        "LDA is a generative probabilistic model that assumes each topic is a mixture over an underlying set of words, and each document is a mixture of over a set of topic probabilities.\n",
        "\n",
        "![LDA_Model](https://github.com/chdoig/pytexas2015-topic-modeling/blob/master/images/lda-4.png?raw=true)\n",
        "\n",
        "We can describe the generative process of LDA as, given the M number of documents, N number of words, and prior K number of topics, the model trains to output:\n",
        "\n",
        "- `psi`, the distribution of words for each topic K\n",
        "- `phi`, the distribution of topics for each document i\n",
        "\n",
        "#### Parameters of LDA\n",
        "\n",
        "- `Alpha parameter` is Dirichlet prior concentration parameter that represents document-topic density — with a higher alpha, documents are assumed to be made up of more topics and result in more specific topic distribution per document.\n",
        "- `Beta parameter` is the same prior concentration parameter that represents topic-word density — with high beta, topics are assumed to made of up most of the words and result in a more specific word distribution per topic.\n",
        "\n",
        "**To read more: https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVCEZZosw_AK"
      },
      "source": [
        "** **\n",
        "### LDA Implementation\n",
        "\n",
        "1. [Loading data](#load_data)\n",
        "2. [Data cleaning](#clean_data)\n",
        "3. [Exploratory analysis](#eda)\n",
        "4. [Prepare data for LDA analysis](#data_preparation)\n",
        "5. [LDA model training](#train_model)\n",
        "6. [Analyzing LDA model results](#results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF_uTWBRw_AK"
      },
      "source": [
        "** **\n",
        "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_158/img/nips_logo.png\" alt=\"The logo of NIPS (Neural Information Processing Systems)\">\n",
        "\n",
        "Let’s start by looking at the content of the file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXsrZbHuw_AL"
      },
      "source": [
        "** **\n",
        "#### Step 1: Loading Data <a class=\"anchor\\\" id=\"load_data\"></a>\n",
        "** **\n",
        "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "Let’s start by looking at the content of the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "MoyKR38Ew_AL",
        "outputId": "89bd76e0-58ee-457e-f0a0-293fb7abff44"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-782ec90f-f53a-4a25-9859-be81cbd6255a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-782ec90f-f53a-4a25-9859-be81cbd6255a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-782ec90f-f53a-4a25-9859-be81cbd6255a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-782ec90f-f53a-4a25-9859-be81cbd6255a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-13f1ba82-1f3e-4988-8fbb-ced73a146900\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-13f1ba82-1f3e-4988-8fbb-ced73a146900')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-13f1ba82-1f3e-4988-8fbb-ced73a146900 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "papers",
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 6560,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1901,\n        \"min\": 1,\n        \"max\": 6603,\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          3087,\n          78,\n          5412\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1987,\n        \"max\": 2016,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          1992,\n          1990,\n          2012\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"Natural Actor-Critic for Road Traffic Optimisation\",\n          \"Learning Representations by Recirculation\",\n          \"Quantized Kernel Learning for Feature Matching\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Oral\",\n          \"Spotlight\",\n          \"Poster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"3087-natural-actor-critic-for-road-traffic-optimisation.pdf\",\n          \"78-learning-representations-by-recirculation.pdf\",\n          \"5412-quantized-kernel-learning-for-feature-matching.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3244,\n        \"samples\": [\n          \"Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP decomposition algorithms based on sketching. We build on the idea of count sketches, but introduce many novel ideas which are unique to tensors. We develop novel methods for randomized com- putation of tensor contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in decomposition methods such as ten- sor power iterations and alternating least squares. We also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine these sketching ideas with existing whitening and tensor power iter- ative techniques to obtain the fastest algorithm on both sparse and dense tensors. The quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc. We apply the method for topic mod- eling and obtain competitive results.\",\n          \"Many spectral unmixing methods rely on the non-negative decomposition of spectral data onto a dictionary of spectral templates. In particular, state-of-the-art music transcription systems decompose the spectrogram of the input signal onto a dictionary of representative note spectra. The typical measures of fit used to quantify the adequacy of the decomposition compare the data and template entries frequency-wise. As such, small displacements of energy from a frequency bin to another as well as variations of timber can disproportionally harm the fit. We address these issues by means of optimal transportation and propose a new measure of fit that treats the frequency distributions of energy holistically as opposed to frequency-wise. Building on the harmonic nature of sound, the new measure is invariant to shifts of energy to harmonically-related frequencies, as well as to small and local displacements of energy. Equipped with this new measure of fit, the dictionary of note templates can be considerably simplified to a set of Dirac vectors located at the target fundamental frequencies (musical pitch values). This in turns gives ground to a very fast and simple decomposition algorithm that achieves state-of-the-art performance on real musical data.\",\n          \"The problem of  multiclass boosting is considered. A new framework,based on multi-dimensional codewords and predictors is introduced. The optimal set of codewords is derived, and a margin enforcing loss proposed. The resulting risk is minimized by gradient descent on a multidimensional functional space. Two algorithms are proposed: 1) CD-MCBoost, based on coordinate descent, updates one predictor component at a time, 2) GD-MCBoost, based on gradient descent, updates all components jointly. The algorithms differ in the weak learners that they support but are both shown to be 1) Bayes consistent, 2) margin enforcing, and 3) convergent to the global minimum of the risk. They also reduce to AdaBoost when there are only two classes. Experiments show that both methods outperform previous multiclass boosting approaches on a number of datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6553,\n        \"samples\": [\n          \"550\\n\\nAckley and Littman\\n\\nGeneralization and scaling in reinforcement\\nlearning\\nDavid H. Ackley\\nMichael L. Littman\\nCognitive Science Research Group\\nBellcore\\nMorristown, NJ 07960\\n\\nABSTRACT\\nIn associative reinforcement learning, an environment generates input\\nvectors, a learning system generates possible output vectors, and a reinforcement function computes feedback signals from the input-output\\npairs. The task is to discover and remember input-output pairs that\\ngenerate rewards. Especially difficult cases occur when rewards are\\nrare, since the expected time for any algorithm can grow exponentially\\nwith the size of the problem. Nonetheless, if a reinforcement function\\npossesses regularities, and a learning algorithm exploits them, learning\\ntime can be reduced below that of non-generalizing algorithms. This\\npaper describes a neural network algorithm called complementary reinforcement back-propagation (CRBP), and reports simulation results\\non problems designed to offer differing opportunities for generalization.\\n\\n1\\n\\nREINFORCEMENT LEARNING REQUIRES SEARCH\\n\\nReinforcement learning (Sutton, 1984; Barto & Anandan, 1985; Ackley, 1988; Allen,\\n1989) requires more from a learner than does the more familiar supervised learning\\nparadigm. Supervised learning supplies the correct answers to the learner, whereas\\nreinforcement learning requires the learner to discover the correct outputs before\\nthey can be stored. The reinforcement paradigm divides neatly into search and\\nlearning aspects: When rewarded the system makes internal adjustments to learn\\nthe discovered input-output pair; when punished the system makes internal adjustments to search elsewhere.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n1.1\\n\\nMAKING REINFORCEMENT INTO ERROR\\n\\nFollowing work by Anderson (1986) and Williams (1988), we extend the backpropagation algorithm to associative reinforcement learning. Start with a \\\"garden variety\\\" backpropagation network: A vector i of n binary input units propagates\\nthrough zero or more layers of hidden units, ultimately reaching a vector 8 of m\\nsigmoid units, each taking continuous values in the range (0,1). Interpret each 8j\\nas the probability that an associated random bit OJ takes on value 1. Let us call\\nthe continuous, deterministic vector 8 the search vector to distinguish it from the\\nstochastic binary output vector o.\\nGiven an input vector, we forward propagate to produce a search vector 8, and\\nthen perform m independent Bernoulli trials to produce an output vector o. The\\ni - 0 pair is evaluated by the reinforcement function and reward or punishment\\nensues. Suppose reward occurs. We therefore want to make 0 more likely given i.\\nBackpropagation will do just that if we take 0 as the desired target to produce an\\nerror vector (0 - 8) and adjust weights normally.\\nNow suppose punishment occurs, indicating 0 does not correspond with i. By choice\\nof error vector, backpropagation allows us to push the search vector in any direction;\\nwhich way should we go? In absence of problem-specific information, we cannot pick\\nan appropriate direction with certainty. Any decision will involve assumptions. A\\nvery minimal \\\"don't be like 0\\\" assumption-employed in Anderson (1986), Williams\\n(1988), and Ackley (1989)-pushes s directly away from 0 by taking (8 - 0) as the\\nerror vector. A slightly stronger \\\"be like not-o\\\" assumption-employed in Barto &\\nAnandan (1985) and Ackley (1987)-pushes s directly toward the complement of 0\\nby taking ((1 - 0) - 8) as the error vector. Although the two approaches always\\nagree on the signs of the error terms, they differ in magnitudes. In this work,\\nwe explore the second possibility, embodied in an algorithm called complementary\\nreinforcement back-propagation ( CRBP).\\nFigure 1 summarizes the CRBP algorithm. The algorithm in the figure reflects three\\nmodifications to the basic approach just sketched. First, in step 2, instead of using\\nthe 8j'S directly as probabilities, we found it advantageous to \\\"stretch\\\" the values\\nusing a parameter v. When v < 1, it is not necessary for the 8i'S to reach zero or\\none to produce a deterministic output. Second, in step 6, we found it important\\nto use a smaller learning rate for punishment compared to reward. Third, consider\\nstep 7: Another forward propagation is performed, another stochastic binary output vector 0* is generated (using the procedure from step 2), and 0* is compared\\nto o. If they are identical and punishment occurred, or if they are different and\\nreward occurred, then another error vector is generated and another weight update\\nis performed. This loop continues until a different output is generated (in the case\\nof failure) or until the original output is regenerated (in the case of success). This\\nmodification improved performance significantly, and added only a small percentage\\nto the total number of weight updates performed.\\n\\n551\\n\\n\\f552\\n\\nAckley and Littman\\n\\nO. Build a back propagation network with input dimensionality n and output\\ndimensionality m. Let t = 0 and te = O.\\n1. Pick random i E 2n and forward propagate to produce a/s.\\n2. Generate a binary output vector o. Given a uniform random variable ~ E [0,1]\\nand parameter 0 < v < 1,\\nOJ\\n\\n=\\n\\n{1,\\n\\n0,\\n\\nif(sj - !)/v+! ~ ~j\\notherwise.\\n\\n3. Compute reinforcement r = f(i,o). Increment t. If r < 0, let te = t.\\n4. Generate output errors ej. If r > 0, let tj = OJ, otherwise let tj = 1- OJ. Let\\nej = (tj - sj)sj(l- Sj).\\n5. Backpropagate errors.\\n6. Update weights. 1:::..Wjk = 1]ekSj, using 1] = 1]+ if r ~ 0, and 1] = 1]- otherwise,\\nwith parameters 1]+,1]- > o.\\n7. Forward propagate again to produce new Sj's. Generate temporary output\\nvector 0*. If (r > 0 and 0* #- 0) or (r < 0 and 0* = 0), go to 4.\\n8. If te ~ t, exit returning te, else go to 1.\\n\\nFigure 1: Complementary Reinforcement Back Propagation-CRBP\\n\\n2\\n\\nON-LINE GENERALIZATION\\n\\nWhen there are many possible outputs and correct pairings are rare, the computational cost associated with the search for the correct answers can be profound.\\nThe search for correct pairings will be accelerated if the search strategy can effectively generalize the reinforcement received on one input to others. The speed of\\nan algorithm on a given problem relative to non-generalizing algorithms provides a\\nmeasure of generalization that we call on-line generalization.\\nO. Let z be an array of length 2n. Set the z[i] to random numbers from 0 to\\n2m - 1. Let t = te = O.\\n1. Pick a random input i E 2n.\\n2. Compute reinforcement r = f(i, z[i]). Increment t.\\n3. If r < 0 let z[i] = (z[i] + 1) mod 2m , and let te = t.\\n4. If te <t:: t exit returning t e, else go to 1.\\n\\nFigure 2: The Table Lookup Reference Algorithm Tref(f, n, m)\\nConsider the table-lookup algorithm Tref(f, n, m) summarized in Figure 2. In this\\nalgorithm, a separate storage location is used for each possible input. This prevents\\nthe memorization of one i - 0 pair from interfering with any other. Similarly,\\nthe selection of a candidate output vector depends only on the slot of the table\\ncorresponding to the given input. The learning speed of T ref depends only on the\\ninput and output dimensionalities and the number of correct outputs associated\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\nwith each input. When a problem possesses n input bits and n output bits, and\\nthere is only one correct output vector for each input vector, Tre{ runs in about 4n\\ntime (counting each input-output judgment as one.) In such cases one expects to\\ntake at least 2n - 1 just to find one correct i - 0 pair, so exponential time cannot be\\navoided without a priori information. How does a generalizing algorithm such as\\nCRBP compare to Trer?\\n\\n3\\n\\nSIMULATIONS ON SCALABLE PROBLEMS\\n\\nWe have tested CRBP on several simple problems designed to offer varying degrees\\nand types of generalization. In all of the simulations in this section, the following\\ndetails apply: Input and output bit counts are equal (n). Parameters are dependent\\non n but independent of the reinforcement function f. '7+ is hand-picked for each\\nn,l 11- = 11+/10 and II = 0.5. All data points are medians of five runs. The stopping\\ncriterion te ~ t is interpreted as te +max(2000, 2n+l) < t. The fit lines in the figures\\nare least squares solutions to a x bn , to two significant digits.\\nAs a notational convenience, let c = ~\\n\\n3.1\\n\\nn\\n\\nE ij\\n\\n;=1\\n\\n-\\n\\nthe fraction of ones in the input.\\n\\nn-MAJORlTY\\n\\nConsider this \\\"majority rules\\\" problem: [if c > ~ then 0 = In else 0 = on]. The i-o\\nmapping is many-to-l. This problem provides an opportunity for what Anderson\\n(1986) called \\\"output generalization\\\": since there are only two correct output states,\\nevery pair of output bits are completely correlated in the cases when reward occurs.\\n\\nG)\\n\\n'iii\\nu\\nrn\\n\\nC)\\n\\n0\\n\\n::::.\\nG)\\n\\nE\\n\\n;\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n\\nTable\\n\\nD\\n\\nCRBP n-n-n\\n\\n+ CRBP n-n\\n\\n10 3\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n456\\n\\n78\\n\\n91011121314\\n\\nn\\nFigure 3: The n-majority problem\\n\\nFigure 3 displays the simulation results. Note that although Trer is faster than\\nCRBP at small values of n, CRBP's slower growth rate (1.6n vs 4.2n ) allows it to\\ncross over and begin outperforming Trer at about 6 bits. Note also--in violation of\\n1 For n = 1 to 12. we used '1+\\n0.219. 0.170. 0.121}.\\n\\n= {2.000. 1.550. 1.130.0.979.0.783.0.709.0.623.0.525.0.280.\\n\\n553\\n\\n\\f554\\n\\nAckley and Littman\\n\\nsome conventional wisdom-that although n-majority is a linearly separable problem, the performance of CRBP with hidden units is better than without. Hidden\\nunits can be helpful--even on linearly separable problems-when there are opportunities for output generalization.\\n\\n3.2\\n\\nn-COPY AND THE 2k -ATTRACTORS FAMILY\\n\\nAs a second example, consider the n-copy problem: [0 = i]. The i-o mapping is now\\n1-1, and the values of output bits in rewarding states are completely uncorrelated,\\nbut the value of each output bit is completely correlated with the value of the\\ncorresponding input bit. Figure 4 displays the simulation results. Once again, at\\n\\nG)\\n\\n'ii\\n\\ntA\\nQ\\n0\\n\\n::::.\\nG)\\n\\n-\\n\\n.5\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n150*2.0I\\\\n\\n\\nD\\n\\n10 3\\n10 2\\n\\n12*2.2I\\\\n\\n\\n+\\n\\nTable\\nCRBP n-n-n\\nCRBP n-n\\n\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 4: The n-copy problem\\nlow values of n, Trer is faster, but CRBP rapidly overtakes Trer as n increases. In\\nn-copy, unlike n-majority, CRBP performs better without hidden units.\\nThe n-majority and n-copy problems are extreme cases of a spectrum. n-majority\\ncan be viewed as a \\\"2-attractors\\\" problem in that there are only two correct\\noutputs-all zeros and all ones-and the correct output is the one that i is closer\\nto in hamming distance. By dividing the input and output bits into two groups\\nand performing the majority function independently on each group, one generates\\na \\\"4-aUractors\\\" problem. In general, by dividing the input and output bits into\\n1 ~ Ie ~ n groups, one generates a \\\"2i:-attractors\\\" problem. When Ie = 1, nmajority results, and when Ie n, n-copy results.\\n\\n=\\n\\nFigure 5 displays simulation results on the n = 8-bit problems generated when Ie is\\nvaried from 1 to n. The advantage of hidden units for low values of Ie is evident,\\nas is the advantage of \\\"shortcut connections\\\" (direct input-to-output weights) for\\nlarger values of Ie. Note also that combination of both hidden units and shortcut\\nconnections performs better than either alone.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n105~--------------------------------~\\n\\nCASP 8-10-8\\n-+- CASP 8-8\\n.... CASP 8-10-Sls\\n-0-\\n\\n... Table\\n\\n3\\n\\n2\\n\\n1\\n\\n5\\n\\n4\\n\\n7\\n\\n6\\n\\n8\\n\\nk\\n\\nFigure 5: The 21:- attractors family at n = 8\\n\\n3.3\\n\\nn-EXCLUDED MIDDLE\\n\\nAll of the functions considered so far have been linearly separable. Consider this\\n\\\"folded majority\\\" function: [if\\n< c < then 0 on else 0 In]. Now, like\\nn-majority, there are only two rewarding output states, but the determination of\\nwhich output state is correct is not linearly separable in the input space. When\\nn = 2, the n-excluded middle problem yields the EQV (i.e., the complement of\\nXOR) function, but whereas functions such as n-parity [if nc is even then 0\\non\\nelse 0 = In] get more non-linear with increasing n, n-excluded middle does not.\\n\\ni\\n\\ni\\n\\n=\\n\\n=\\n\\n=\\n\\n107~------------------------------~~\\n\\n-\\n\\n10 6\\n10 5\\n\\nD)\\n\\n10 4\\n10 3\\n\\nI)\\n\\n'ii\\nu\\nf)\\n\\n.2\\n\\nI)\\n\\nE\\n\\n:::\\n\\nx\\nc\\n\\n17oo*1.6\\\"n\\n\\nTable\\n\\nCRSP n-n-n/s\\n\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 6: The n-excluded middle problem\\nFigure 6 displays the simulation results. CRBP is slowed somewhat compared to\\nthe linearly separable problems, yielding a higher \\\"cross over point\\\" of about 8 bits.\\n\\n555\\n\\n\\f556\\n\\nAckley and Littman\\n\\n4\\n\\nSTRUCTURING DEGENERATE OUTPUT SPACES\\n\\nAll of the scaling problems in the previous section are designed so that there is\\na single correct output for each possible input. This allows for difficult problems\\neven at small sizes, but it rules out an important aspect of generalizing algorithms\\nfor associative reinforcement learning: If there are multiple satisfactory outputs\\nfor given inputs, a generalizing algorithm may impose structure on the mapping it\\nproduces.\\nWe have two demonstrations of this effect, \\\"Bit Count\\\" and \\\"Inverse Arithmetic.\\\"\\nThe Bit Count problem simply states that the number of I-bits in the output should\\nequal the number of I-bits in the input. When n = 9, Tref rapidly finds solutions\\ninvolving hundreds of different output patterns. CRBP is slower--especially with\\nrelatively few hidden units-but it regularly finds solutions involving just 10 output\\npatterns that form a sequence from 09 to 19 with one bit changing per step.\\n0+Ox4=0\\n1+0x4=1\\n2+0x4=2\\n3+0x4=3\\n\\n0+2x4=8\\n1+2x4=9\\n2 + 2 x 4 = 10\\n3+2x4=11\\n\\n4+0x4=4 4+ 2 x 4 =\\n5+0x4=5 5 + 2 x 4 =\\n6+0x4=6 6 + 2 x 4 =\\n7+0x4=7 7 + 2 x 4 =\\n\\n12\\n13\\n14\\n15\\n\\n2+2-4=0 2+2+4=8\\n3+2-4=1 3+2+4=9\\n2+2+4=2 2 + 2 x 4 = 10\\n3+2+4=3 3+2x4=1l\\n6+2-4=4\\n7+2-4=5\\n6+2+4=6\\n7+2-.;-4=7\\n\\n6+\\n7+\\n6+\\n7+\\n\\n2+ 4 =\\n2+ 4 =\\n2x4=\\n2x4=\\n\\n0+4 x 4 = 16 0+6 x 4 =\\n1+4x4=17 1 + 6 x 4 =\\n2 + 4 x 4 = 18 2 + 6 x 4 =\\n3 +4 x 4 = 19 3 + 6 x 4 =\\n\\n24\\n25\\n26\\n27\\n\\n4+4\\n5+ 4\\n6+ 4\\n7+ 4\\n\\n=\\n=\\n=\\n=\\n\\n28\\n29\\n30\\n31\\n24\\n25\\n26\\n27\\n\\nx\\nx\\nx\\nx\\n\\n4=\\n4=\\n4=\\n4=\\n\\n6+ 6 + 4 =\\n7+6+4=\\n2+ 4 x 4 =\\n3+ 4 x 4=\\n\\n12 4 x 4 +\\n13 5 + 4 x\\n14 6 + 4 x\\n15 7 +4 x\\n\\n4=\\n4=\\n4\\n4=\\n\\n=\\n\\n20 4 + 6 x\\n21 5 + 6 x\\n22 6 + 6 x\\n23 7 + 6 x\\n\\n4\\n4\\n4\\n4\\n\\n16\\n17\\n18\\n19\\n\\n0+6 x\\n1+ 6 x\\n2+ 6x\\n3+ 6x\\n\\n4=\\n4=\\n4=\\n4=\\n\\n20\\n21\\n22\\n23\\n\\n4+\\n5+\\n6+\\n7+\\n\\n4 = 28\\n4 = 29\\n4 30\\n4 = 31\\n\\n6\\n6\\n6\\n6\\n\\nx\\nx\\nx\\nx\\n\\n=\\n\\nFigure 7: Sample CRBP solutions to Inverse Arithmetic\\n\\nThe Inverse Arithmetic problem can be summarized as follows: Given i E 25 , find\\n:1:, y, z E 23 and 0, <> E {+(OO)' -(01)' X (10)' +(11)} such that :I: oy<>z = i. In all there are\\n13 bits of output, interpreted as three 3-bit binary numbers and two 2-bit operators,\\nand the task is to pick an output that evaluates to the given 5-bit binary input\\nunder the usual rules: operator precedence, left-right evaluation, integer division,\\nand division by zero fails.\\nAs shown in Figure 7, CRBP sometimes solves this problem essentially by discovering positional notation, and sometimes produces less-globally structured solutions,\\nparticularly as outputs for lower-valued i's, which have a wider range of solutions.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n5\\n\\nCONCLUSIONS\\n\\nSome basic concepts of supervised learning appear in different guises when the\\nparadigm of reinforcement learning is applied to large output spaces. Rather than\\na \\\"learning phase\\\" followed by a \\\"generalization test,\\\" in reinforcement learning\\nthe search problem is a generalization test, performed simultaneously with learning.\\nInformation is put to work as soon as it is acquired.\\nThe problem of of \\\"overfitting\\\" or \\\"learning the noise\\\" seems to be less of an issue,\\nsince learning stops automatically when consistent success is reached. In experiments not reported here we gradually increased the number of hidden units on\\nthe 8-bit copy problem from 8 to 25 without observing the performance decline\\nassociated with \\\"too many free parameters.\\\"\\nThe 2 k -attractors (and 2 k -folds-generalizing Excluded Middle) families provide\\na starter set of sample problems with easily understood and distinctly different\\nextreme cases.\\nIn degenerate output spaces, generalization decisions can be seen directly in the\\ndiscovered mapping. Network analysis is not required to \\\"see how the net does it.\\\"\\nThe possibility of ultimately generating useful new knowledge via reinforcement\\nlearning algorithms cannot be ruled out.\\nReferences\\nAckley, D.H. (1987) A connectionist machine for genetic hillclimbing. Boston, MA: Kluwer\\nAcademic Press.\\nAckley, D.H. (1989) Associative learning via inhibitory search. In D.S. Touretzky (ed.),\\nAdvances in Neural Information Processing Systems 1, 20-28. San Mateo, CA: Morgan\\nKaufmann.\\nAllen, R.B. (1989) Developing agent models with a neural reinforcement technique. IEEE\\nSystems, Man, and Cybernetics Conference. Cambridge, MA.\\nAnderson, C.W. (1986) Learning and problem solving with multilayer connectionist systems. University of Mass. Ph.D. dissertation. COINS TR 86-50. Amherst, MA.\\nBarto, A.G. (1985) Learning by statistical cooperation of self-interested neuron-like computing elements. Human Neurobiology, 4:229-256.\\nBarto, A.G., & Anandan, P. (1985) Pattern recognizing stochastic learning automata.\\nIEEE Transactions on Systems, Man, and Cybernetics, 15, 360-374.\\nRumelhart, D.E., Hinton, G.E., & Williams, R.J. (1986) Learning representations by backpropagating errors. Nature, 323, 533-536.\\nSutton, R.S. (1984) Temporal credit assignment in reinforcement learning. University of\\nMass. Ph.D. dissertation. COINS TR 84-2. Amherst, MA.\\nWilliams, R.J. (1988) Toward a theory of reinforcement-learning connectionist systems.\\nCollege of Computer Science of Northeastern University Technical Report NU-CCS-88-3.\\nBoston, MA.\\n\\n557\\n\\n\\f\",\n          \"Dynamics of Supervised Learning with\\nRestricted Training Sets and Noisy Teachers\\n\\nA.C.C. Coolen\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ntcoolen@mth.kc1.ac.uk\\n\\nC.W.H.Mace\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ncmace@mth.kc1.ac.uk\\n\\nAbstract\\nWe generalize a recent formalism to describe the dynamics of supervised\\nlearning in layered neural networks, in the regime where data recycling\\nis inevitable, to the case of noisy teachers. Our theory generates reliable\\npredictions for the evolution in time of training- and generalization errors, and extends the class of mathematically solvable learning processes\\nin large neural networks to those situations where overfitting can occur.\\n\\n1 Introduction\\nTools from statistical mechanics have been used successfully over the last decade to study\\nthe dynamics of learning in layered neural networks (for reviews see e.g. [1] or [2]). The\\nsimplest theories result upon assuming the data set to be much larger than the number\\nof weight updates made, which rules out recycling and ensures that any distribution of\\nrelevance will be Gaussian. Unfortunately, both in terms of applications and in terms of\\nmathematical interest, this regime is not the most relevant one. Most complications and\\npeculiarities in the dynamics of learning arise precisely due to data recycling, which creates\\nfor the system the possibility to improve performance by memorizing answers rather than\\nby learning an underlying rule. The dynamics of learning with restricted training sets was\\nfirst studied analytically in [3] (linear learning rules) and [4] (systems with binary weights).\\nThe latter studies were ahead of their time, and did not get the attention they deserved just\\nbecause at that stage even the simpler learning dynamics without data recycling had not\\nyet been studied. More recently attention has moved back to the dynamics of learning\\nin the recycling regime. Some studies aimed at developing a general theory [5, 6, 7],\\nsome at finding exact solutions for special cases [8]. All general theories published so far\\nhave in common that they as yet considered realizable scenario's: the rule to be learned\\nwas implementable by the student, and overfitting could not yet occur. The next hurdle is\\nthat where restricted training sets are combined with unrealizable rules. Again some have\\nturned to non-typical but solvable cases, involving Hebbian rules and noisy [9] or 'reverse\\nwedge' teachers [10]. More recently the cavity method has been used to build a general\\ntheory [11] (as yet for batch learning only). In this paper we generalize the general theory\\nlaunched in [6,5,7], which applies to arbitrary learning rules, to the case of noisy teachers.\\nWe will mirror closely the presentation in [6] (dealing with the simpler case of noise-free\\nteachers), and we refer to [5, 7] for background reading on the ideas behind the formalism.\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n238\\n\\n2 Definitions\\nAs in [6, 5] we restrict ourselves for simplicity to perceptrons. A student perceptron operates a linear separation, parametrised by a weight vector J E iRN :\\nS:{-I,I}N -t{-I,I}\\n\\nS(e) = sgn[J?e]\\n\\nIt aims to emulate a teacher o~erating a similar rule, which, however, is characterized by a\\nvariable weight vector BE iR ,drawn at random from a distribution P(B) such as\\nP(B) = >'6[B+B*]\\n\\noutput noise:\\n\\n+ (1->')6[B-B*]\\n\\n(1)\\n\\nP(B) = [~~/NrN e- tN (B-B')2/E2\\n(2)\\nThe parameters>. and ~ control the amount of teacher noise, with the noise-free teacher\\nB = B* recovered in the limits>. -t 0 and ~ -t O. The student modifies J iteratively, using\\nexamples of input vectors which are drawn at random from a fixed (randomly composed)\\nE {-I, I}N with a> 0, and the corresponding\\ntraining set containing p = aN vectors\\nvalues of the teacher outputs. We choose the teacher noise to be consistent, i.e. the answer\\nwill remain the same when that particular question\\ngiven by the teacher to a question\\nre-appears during the learning process. Thus T(e?) = sgn[BJL . e], with p teacher weight\\nvectors BJL, drawn randomly and independently from P(B), and we generalize the training\\nl , B l ), . .. , (e, BP)}. Consistency of teacher noise is natural\\nset accordingly to jj =\\nin terms of applications, and a prerequisite for overfitting phenomena. Averages over the\\ntraining set will be denoted as ( ... ) b; averages over all possible input vectors E {-I, I}N\\nas ( ... )e. We analyze two classes of learning rules, of the form J (? + 1) = J (?) + f).J (?):\\n\\nGaussian weight noise:\\n\\ne\\n\\ne\\n\\ne\\n\\nHe\\n\\ne\\n\\n= 11 {e(?) 9 [J(?)?e(?), B(?)?e(?)] - ,J(?) }\\nf).J(?) = 11 {(e 9 [J(?)?e, B?eDl> - ,J(m) }\\n\\non-line:\\n\\nf).J(?)\\n\\nbatch :\\n\\n(3)\\n\\nIn on-line learning one draws at each step ? a question/answer pair (e (?), B (?)) at random from the training set. In batch learning one iterates a deterministic map which is an\\naverage over all data in the training set. Our performance measures are the training- and\\ngeneralization errors, defined as follows (with the step function O[x > 0] = 1, O[x < 0] = 0):\\nEt(J)\\n\\n= (O[-(J ?e)(B ?em b\\n\\nEg(J)\\n\\n= (O[-(J ?e)(B* ?e)])e\\n\\n(4)\\n\\nWe introduce macroscopic observables, taylored to the present problem, generalizing [5, 6]:\\nQ[J]=J 2,\\nR[J]=J?B*,\\nP[x,y,z;J]=(6[x-J?e]6[y-B*?e]6[z-B?eDl> (5)\\nAs in [5, 6] we eliminate technical subtleties by assuming the number of arguments (x, y, z)\\nfor which P[x, y, z; J] is evaluated to go to infinity after the limit N -t 00 has been taken.\\n\\n3 Derivation of Macroscopic Laws\\nUpon generalizing the calculations in [6, 5], one finds for on-line learning:\\n\\n!\\n!\\n\\nQ = 2'f} !dXdydZ P[x, y, z] xg[x, z] - 2'f},Q + 'f}2!dXdYdZ P[x, y, z] g2[x, z]\\n\\n(6)\\n\\nR = 'f} !dXdydZ P[x, y, z] y9[x, z]- 'f},R\\n\\n(7)\\n\\n:t\\n\\nP[x, y, z] =\\n\\n~\\n\\n!\\n\\ndx' P[x', y, z] {6[x-x' -'f}G[x', z]] -6[x-x']}\\n\\n-'f}! / dx'dy'dz' / dx'dy'dz'9[x', z]A[x, y, z; x',y', z']\\n\\n1\\n+'i'f}2\\n\\n!\\n\\n+ 'f}, :x\\n\\nEP2P[x, y, z]\\ndx'dy'dz' P[x', y', z']92[x', z'] 8x\\n\\n{xP[x , y, z]}\\n\\n(8)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n239\\n\\nThe complexity of the problem is concentrated in a Green's function:\\nA[x, y, Zj x', y', z'] = lim\\nN-+oo\\n\\n(( ([1-6ee , ]6[x-J?e]6[y-B*?e]6[z-B?e] (e?e')6[x' -J?e']6[y' - B*?e']6[y' - B?e'])i?i> )QW;t\\n\\nJ\\n\\nIt involves a conditional average of the form (K[J])QW;t = dJ Pt(JIQ,R,P)K[J], with\\nPt(J) 6[Q-Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] -P[x, y, Zj J]]\\nPt(JIQ,R,P)\\nJdJ Pt(J) 6[Q - Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] - P[x, y, z; J]]\\n\\n=\\n\\nin which Pt (J) is the weight probability density at time t. The solution of (6,7,8) can be\\nused to generate the N -+ 00 performance measures (4) at any time:\\nEt\\n\\n=/\\n\\ndxdydz P[x, y, z]O[-xz]\\n\\nEg\\n\\n= 11\\\"-1 arccos[RIVQ]\\n\\n(9)\\n\\nExpansion of these equations in powers of\\\"\\\" and retaining only the terms linear in \\\"\\\" gives\\nthe corresponding equations describing batch learning. So far this analysis is exact.\\n\\n4\\n\\nClosure of Macroscopic Laws\\n\\nAs in [6, 5] we close our macroscopic laws (6,7,8) by making the two key assumptions\\nunderlying dynamical replica theory:\\n(i) For N -+ 00 our macroscopic observables obey closed dynamic equations.\\n(ii) These equations are self-averaging with respect to the specific realization of D.\\n\\n(i) implies that probability variations within {Q, R, P} subshells are either absent or irrelevant to the macroscopic laws. We may thus make the simplest choice for Pt (J IQ, R, P):\\nPt(JIQ,R,P) -+ 6[Q-Q[J]] 6[R-R[J]]\\n\\nII 6[P[x,y,z]-P[x,y,ZjJ]]\\n\\n(10)\\n\\nxyz\\n\\nThe procedure (10) leads to exact laws if our observables {Q, R, P} indeed obey closed\\nequations for N -+ 00. It is a maximum entropy approximation if not. (ii) allows us\\nto average the macroscopic laws over all training sets; it is observed in simulations, and\\nproven using the formalism of [4]. Our assumptions (10) result in the closure of (6,7,8),\\nsince now the Green's function can be written in terms of {Q, R, Pl. The final ingredient\\nof dynamical replica theory is doing the average of fractions with the replica identity\\n\\n/ JdJ W[JID]GIJID])\\n\\n\\\\\\n\\nJdJ W[JID]\\n\\n= lim\\nsets\\n\\n/dJ I\\n\\n???\\n\\ndJn (G[J 1 ID]\\n\\nn-+O\\n\\nIT\\n\\nW[JO<ID])sets\\n\\na=1\\n\\nOur problem has been reduced to calculating (non-trivial) integrals and averages. One\\nfinds that P[x, y, z] P[x, zly]P[y] with Ply] (211\\\")-!exp[-!y 21With the short-hands\\nDy = P[y]dy and (f(x, y, z)) = Dydxdz P[x, zly]f(x, y, z) we can write the resulting\\nmacroscopic laws, for the case of output noise (1), in the following compact way:\\n\\n=\\n\\nd\\n\\ndt Q = 2\\\",(V - ,Q)\\n\\n[)\\n\\n[)tP[x,zly] =\\n\\n=\\n\\nJ\\n\\n+ rJ2 Z\\n\\nd\\n\\ndtR = \\\",(W - ,R)\\n\\n(11)\\n\\n1 [)x[)22P[x,zIY]\\na1/dx'P[x',zly] {6[x-x'-\\\",G[x',z]]-6[x-x'] }+2\\\",2Z\\n\\n-\\\",:x {P[x,zly]\\n\\n[U(x-RY)+Wy-,x+[V-RW-(Q-R2)U]~[x,y,z])}\\n\\n(12)\\n\\nwith\\n\\nU = (~[x, y, z]9[x, z]),\\n\\nv = (x9[x, z]),\\n\\nW = (y9[x, z]),\\n\\nZ = (9 2[x, z])\\n\\nThe solution of (12) is at any time of the following form:\\n\\nP[x,zly]\\n\\n= (1-,x)6[y-z]P+[xly] + ,x6[y+z]P-[xly]\\n\\n(13)\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n240\\n\\nFinding the function <I> [x, y, z] (in replica symmetric ansatz) requires solving a saddle-point\\nproblem for a scalar observable q and two functions M?[xly]. Upon introducing\\n\\nB = . . :. V. .,. .q.,-Q___R,-2\\nQ(I-q)\\n(with Jdx M?[xly]\\n\\nJdx M?[xly]eBxs J[x, y]\\nJdx M?[xly]eBxs\\n\\n(f[x, y])? =\\n*\\n\\n= 1 for all y) the saddle-point equations acquire the fonn\\np?[Xly] =\\n\\nfor all X, y :\\n\\n((x-Ry)2) + (qQ-R 2)[I-!:.]\\na\\n\\n!\\n\\nDs (O[X -xl);\\n\\n2 !DYDS S[(I-A)(X); + A(X);]\\n= qQ+Q-2R\\n..jqQ_R2\\n\\n(14)\\n(15)\\n\\nThe equations (14) which detennine M?[xly] have the same structure as the corresponding\\n(single) equation in [5, 6], so the proofs in [5, 6] again apply, and the solutions M?[xly],\\ngiven a q in the physical range q E [R2/Q, 1], are unique. The function <I> [x, y, z] is then\\ngiven by\\n<I> [X,\\n\\ny, z]\\n\\n=!\\n\\nDs s\\n{(I-A)O[Z-y](o[X -x)); + AO[Z+Y](o[X -xl);}\\n..jqQ_R2 P[X, zly]\\n(16)\\n\\nWorking out predictions from these equations is generally CPU-intensive, mainly due to\\nthe functional saddle-point equation (14) to be solved at each time step. However, as in [7]\\none can construct useful approximations of the theory, with increasing complexity:\\n\\n(i) Large a approximation (giving the simplest theory, without saddle-point equations)\\n(ii) Conditionally Gaussian approximation for M[xly] (with y-dependent moments)\\n(iii) Annealed approximation of the functional saddle-point equation\\n\\n5 Benchmark Tests: The Limits a --+ 00 and ,\\\\ --+ 0\\nWe first show that in the limit a --+ 00 our theory reduces to the simple (Q, R) formalism\\nof infinite training sets, as worked out for noisy teachers in [12]. Upon making the ansatz\\n\\np?[xly] = P[xly] = [27r(Q-R 2)]-t e- t [x- Rv]2/(Q-R 2)\\n\\n(17)\\n\\none finds\\n\\n<I>[x,y,Z] = (x-Ry)/(Q-R 2)\\n\\nM?[xly] = P[xly],\\n\\nInsertion of our ansatz into (12), followed by rearranging of terms and usage of the above\\nexpression for <I> [x, y, z], shows that (12) is satisfied. The remaining equations (11) involve\\nonly averages over the Gaussian distribution (17), and indeed reduce to those of [12]:\\n\\n~! Q =\\n\\n(I-A) { 2(x9[x, y))\\n1 d\\n--d R\\n1} t\\n\\n+ 1}{92[x, y)) } + A {2(x9[x,-y)) + 1}(92[x,-y)) } - 2,Q\\n\\n= (I-A)(y9[x,y)) + A(y9[x,-yl) -,R\\n\\nNext we turn to the limit A --+ 0 (restricted training sets & noise-free teachers) and show that\\nhere our theory reproduces the fonnalism of [6,5]. Now we make the following ansatz:\\n\\nP+[xly] = P[xly],\\n\\nP[x, zly]\\n\\n= o[z-y]P[xIY]\\n\\n(18)\\n\\nInsertion shows that for A = 0 solutions of this fonn indeed solve our equations, giving\\n<p[x, y, z]--+ <I> [x, y] and M+[xly]\\nM[xly), and leaving us exactly with the fonnalism\\nof [6, 5] describing the case of noise-free teachers and restricted training sets (apart from\\nsome new tenns due to the presence of weight decay, which was absent in [6, 5]).\\n\\n=\\n\\n\\f241\\n\\nSupervised Learning with Restricted Training Sets\\n0. , r------~--__,\\n\\n0..4\\n\\n~-------_____I\\n\\n0..4\\n\\n11>=0.'\\n\\n0..3\\n\\na=4\\n\\n0. ,\\n\\n0..0.\\n\\n--\\n\\n, 0.\\n\\n0.2\\n\\n_ __ ___ _____ _\\n\\na= 1\\n\\n0;=1\\n\\n------- ---- -- --- -\\n\\n0.\\n\\n0;=2\\n\\n=-=\\n-\\n\\n0;=2\\n\\n- - ----- -\\n\\na=4\\na=4\\n\\n= =-=\\n--=-=--=-=--=-=-=-- -=-=-_oed\\n\\na=4\\n\\n,\\n\\n0;=2\\n\\n':::::========:::j\\n\\n0..3\\n\\n-- - ----\\n\\n0;=1\\n\\n:::---- - -----1\\n\\n0;=2\\n\\n0..2\\n\\n11>=0.'\\n\\n~-------~\\n\\n0;=1\\n\\n0.,\\n\\n11>=0,\\n\\n\\\"\\n\\n,\\n\\nno. I\\n\\n0.\\n\\n, 0.\\n\\n\\\"\\n\\nFigure 1: On-line Hebbian learning: conditionally Gaussian approximation versus exact\\nsolution in [9] (.,., = 1, ,X = 0.2). Left: \\\"I = 0.1, right: \\\"I = 0.5. Solid lines: approximated\\ntheory, dashed lines: exact result. Upper curves: Eg as functions of time (here the two\\ntheories agree), lower curves: E t as functions of time.\\n\\n6\\n\\nBenchmark Tests: Hebbian Learning\\n\\nThe special case of Hebbian learning, i.e. Q[x, z] = sgn(z), can be solved exactly at any\\ntime, for arbitrary {a, ,x, \\\"I} [9], providing yet another excellent benchmark for our theory.\\nFor batch execution of Hebbian learning the macroscopic laws are obtained upon expanding\\n(11,12) and retaining only those terms which are linear in.,.,. All integrations can now be\\ndone and all equations solved explicitly, resulting in U =0, Z = 1, W = (I-2,X)J2/7r, and\\n\\nQ\\n\\n= Qo e-2rryt +\\n\\n2Ro(I-2'x) e-17\\\"Yt[I_e-rrrt]\\n\\\"I\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I-e- 17 \\\"Y tF\\n\\\"12\\n\\nR = Ro e- 17\\\"Y t +(I-2'x)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\nq = [aR2+(I_e- 17\\\"Yt)2 i'l]/aQ\\np?[xIY] = [27r(Q-R2)] -t e-tlz-RH sgn(y)[1-e-\\\"..,t]/a\\\"Y]2/(Q-R2)\\n(19)\\nFrom these results, in tum, follow the performance measures Eg = 7r- 1 arccos[ R/ JQ) and\\n\\nE = ! - !(1-,X)!D\\n2\\n\\nt\\n\\n2\\n\\nerf[IYIR+[I-e- 77\\\"Y t ]/a\\\"l] + !,X!D erf[IYIR-[I-e- 17\\\"Y t ]/a\\\"l]\\nY\\nJ2(Q-R2)\\n2\\ny\\nJ2(Q-R2)\\n\\nComparison with the exact solution, calculated along the lines of [9] or, equivalently, obtained upon putting t ?\\nin [9], shows that the above expressions are all exact.\\n\\n.,.,-2\\n\\nFor on-line execution we cannot (yet) solve the functional saddle-point equation in general.\\nHowever, some analytical predictions can still be extracted from (11,12,13):\\n\\nQ = Qo e-217\\\"Yt + 2Ro(I-2,X) e-77\\\"Yt[I_e-17\\\"Yt]\\n\\\"I\\n\\nR = Ro e- 17\\\"Y t + (I-2,X)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\n\\nJ\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I_e- 17\\\"Y t ]2\\n\\\"12\\n\\n+ !L[I_e- 217\\\"Y t ]\\n2\\\"1\\n\\ndx xP?[xIY] = Ry ? sgn(y)[I-e- 17\\\"Y t ]/a\\\"l\\n\\nwith U =0, W = (I-2,X)J2/7r, V = W R+[I-e- 17\\\"Y t ]/a\\\"l, and Z = 1. Comparison with the\\nresults in [9] shows that the above expressions, and thus also that of E g , are all fully exact,\\nat any time. Observables involving P[x, y, z] (including the training error) are not as easily\\nsolved from our equations. Instead we used the conditionally Gaussian approximation\\n(found to be adequate for the noiseless Hebbian case [5, 6, 7]). The result is shown in\\nfigure 1. The agreement is reasonable, but significantly less than that in [6]; apparently\\nteacher noise adds to the deformation of the field distribution away from a Gaussian shape.\\n\\n\\f242\\n\\nA. C. C. Coolen and C. W H. Mac\\n\\n~\\n\\n0.6\\n\\n000000\\n\\n0.4\\n\\n0.4\\n\\nE\\n\\n~\\n\\n0.2\\n\\nI\\ni\\n0.0\\n\\n0\\n\\n4\\n\\n2\\n\\n6\\n\\n10\\n\\n0.0\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\nX\\n\\n0.6\\n\\nf\\n\\n0.4\\n\\n0.4 [\\n\\nE\\n0.2\\n\\n0.2\\n\\n0.0\\n\\nL-o!i6iIII.\\\"\\\"\\\"\\\"\\\"',-\\\"--~_~~_ _--'\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\n\\n2\\n\\n3\\n\\nX\\n\\n,=\\n\\nFigure 2: Large a approximation versus numerical simulations (with N = 10,000), for\\n0 and A = 0.2. Top row: Perceptron rule, with.,., = ~. Bottom row: Adatron rule,\\nwith.,., = ~. Left: training errors E t and generalisation errors Eg as functions of time, for\\naE {~, 1, 2}. Lines: approximated theory, markers: simulations (circles: E t , squares: Eg) .\\nRight: joint distributions for student field and teacher noise p?[x] = dy P[x, y, z = ?y]\\n(upper: P+[x], lower: P-[x]). Histograms: simulations, lines: approximated theory.\\n\\nJ\\n\\n7\\n\\nNon-Linear Learning Rules: Theory versus Simulations\\n\\nIn the case of non-linear learning rules no exact solution is known against which to test our\\nformalism, leaving numerical simulations as the yardstick. We have evaluated numerically\\nthe large a approximation of our theory for Perceptron learning, 9[x, z] = sgn(z)O[-xz],\\nand for Adatron learning, 9[x, z] = sgn(z)lzIO[-xz]. This approximation leads to the\\nfollowing fully explicit equation for the field distributions:\\n\\n1/\\n\\nd\\n-p?[xly]\\n= dt\\na\\n.\\n\\nWith\\n\\nU=\\n\\n' +1\\n\\ndx' p?[x'ly]{o[x-x'-.,.,.1'[x', ?y]] -o[x-x]}\\n\\n_ ~ {P[ I ] [W _\\n.,., 8\\nx y\\ny\\n\\nJ\\n\\nX\\n\\n~ p?[xly]\\n\\n_.,.,2 Z!:I 2\\n2\\nuX\\n\\n,X + U[X?(y)-RY]+(V-RW)[X-X?(y)]]}\\nQ _ R2\\n\\nDydx {(I-A)P+[xly][x-P(y)]9[x,Y]+AP-[xly][x-x-(y)]9[x,-y])\\nV =\\nW=\\nZ=\\n\\n!\\n1\\n1\\n\\nDydx x {(I-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\nDydx y {(1-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\n\\nDydx {(I-A)P+[xly]92[x, Y]+AP-[xly]9 2[x,-yJ)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n243\\n\\nJ\\n\\nand with the short-hands X?(y) = dx xP?[xly). The result of our comparison is shown\\nin figure 2. Note: E t increases monotonically with a, and Eg decreases monotonically\\nwith a, at any t. As in the noise-free formalism [7], the large a approximation appears to\\ncapture the dominant terms both for a -7 00 and for a -7 O. The predicting power of our\\ntheory is mainly limited by numerical constraints. For instance, the Adatron learning rule\\ngenerates singularities at x = 0 in the distributions P?[xly) (especially for small \\\"I) which,\\nalthough predicted by our theory, are almost impossible to capture in numerical solutions.\\n\\n8 Discussion\\nWe have shown how a recent theory to describe the dynamics of supervised learning with\\nrestricted training sets (designed to apply in the data recycling regime, and for arbitrary online and batch learning rules) [5, 6, 7] in large layered neural networks can be generalized\\nsuccessfully in order to deal also with noisy teachers. In our generalized approach the joint\\ndistribution P[x, y, z) for the fields of student, 'clean' teacher, and noisy teacher is taken to\\nbe a dynamical order parameter, in addition to the conventional observables Q and R. From\\nthe order parameter set {Q, R, P} we derive the generalization error Eg and the training\\nerror E t . Following the prescriptions of dynamical replica theory one finds a diffusion\\nequation for P[x, y, z], which we have evaluated by making the replica-symmetric ansatz.\\nWe have carried out several orthogonal benchmark tests of our theory: (i) for a -7 00 (no\\ndata recycling) our theory is exact, (ii) for A -7 0 (no teacher noise) our theory reduces\\nto that of [5, 6, 7], and (iii) for batch Hebbian learning our theory is exact. For on-line\\nHebbian learning our theory is exact with regard to the predictions for Q, R, Eg and the\\ny-dependent conditional averages Jdx xP?[xly), at any time, and a crude approximation\\nof our equations already gives reasonable agreement with the exact results [9] for E t . For\\nnon-linear learning rules (Perceptron and Adatron) we have compared numerical solution\\nof a simple large a aproximation of our equations to numerical simulations, and found\\nsatisfactory agreement. This paper is a preliminary presentation of results obtained in the\\nsecond stage of a research programme aimed at extending our theoretical tools in the arena\\nof learning dynamics, building on [5, 6, 7]. Ongoing work is aimed at systematic application of our theory and its approximations to various types of non-linear learning rules, and\\nat generalization of the theory to multi-layer networks.\\n\\nReferences\\n[1]\\n[2]\\n[3]\\n[4]\\n[5]\\n[6]\\n[7]\\n[8]\\n[9]\\n[10]\\n[11]\\n[12]\\n\\nMace C.W.H. and Coolen AC.C (1998), Statistics and Computing 8, 55\\nSaad D. (ed.) (1998), On-Line Learning in Neural Networks (Cambridge: CUP)\\nHertz J.A., Krogh A and Thorgersson G.I. (1989), J. Phys. A 22, 2133\\nHomerH. (1992a), Z. Phys. B 86, 291 and Homer H. (1992b), Z. Phys. B 87,371\\nCoolen A.C.C. and Saad D. (1998), in On-Line Learning in Neural Networks, Saad\\nD. (ed.), (Cambridge: CUP)\\nCoolen AC.C. and Saad D. (1999), in Advances in Neural Information Processing\\nSystems 11, Kearns D., Solla S.A., Cohn D.A (eds.), (MIT press)\\nCoolen A.C.C. and Saad D. (1999), preprints KCL-MTH-99-32 & KCL-MTH-99-33\\nRae H.C., Sollich P. and Coolen AC.C. (1999), in Advances in Neural Information\\nProcessing Systems 11, Kearns D., Solla S.A., Cohn D.A. (eds.), (MIT press)\\nRae H.C., Sollich P. and Coolen AC.C. (1999),J. Phys. A 32, 3321\\nInoue J.I. (1999) private communication\\nWong K.YM., Li S. and Tong YW. (1999),preprint cond-mat19909004\\nBiehl M., Riegler P. and Stechert M. (1995), Phys. Rev. E 52, 4624\\n\\n\\f\",\n          \"Predicting Action Content On-Line and in\\nReal Time before Action Onset ? an\\nIntracranial Human Study\\n\\nShengxuan Ye\\nCalifornia Institute of Technology\\nPasadena, CA\\nsye@caltech.edu\\n\\nUri Maoz\\nCalifornia Institute of Technology\\nPasadena, CA\\nurim@caltech.edu\\nIan Ross\\nHuntington Hospital\\nPasadena, CA\\nianrossmd@aol.com\\n\\nAdam Mamelak\\nCedars-Sinai Medical Center\\nLos Angeles, CA\\nadam.mamelak@cshs.org\\n\\nChristof Koch\\nCalifornia Institute of Technology\\nPasadena, CA\\nAllen Institute for Brain Science\\nSeattle, WA\\nkoch@klab.caltech.edu\\n\\nAbstract\\nThe ability to predict action content from neural signals in real time before the action occurs has been long sought in the neuroscientific study of decision-making,\\nagency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious,\\nvoluntary action as well as for brain-machine interfaces. Here, epilepsy patients,\\nimplanted with intracranial depth microelectrodes or subdural grid electrodes for\\nclinical purposes, participated in a ?matching-pennies? game against an opponent.\\nIn each trial, subjects were given a 5 s countdown, after which they had to raise\\ntheir left or right hand immediately as the ?go? signal appeared on a computer\\nscreen. They won a fixed amount of money if they raised a different hand than\\ntheir opponent and lost that amount otherwise. The question we here studied was\\nthe extent to which neural precursors of the subjects? decisions can be detected in\\nintracranial local field potentials (LFP) prior to the onset of the action.\\nWe found that combined low-frequency (0.1?5 Hz) LFP signals from 10 electrodes\\nwere predictive of the intended left-/right-hand movements before the onset of the\\ngo signal. Our ORT system predicted which hand the patient would raise 0.5 s\\nbefore the go signal with 68?3% accuracy in two patients. Based on these results,\\nwe constructed an ORT system that tracked up to 30 electrodes simultaneously,\\nand tested it on retrospective data from 7 patients. On average, we could predict\\nthe correct hand choice in 83% of the trials, which rose to 92% if we let the system\\ndrop 3/10 of the trials on which it was less confident. Our system demonstrates?\\nfor the first time?the feasibility of accurately predicting a binary action on single\\ntrials in real time for patients with intracranial recordings, well before the action\\noccurs.\\n\\n1\\n\\n\\f1\\n\\nIntroduction\\n\\nThe work of Benjamin Libet [1, 2] and others [3, 4] has challenged our intuitive notions of the relation between decision making and conscious voluntary action. Using electrocorticography (EEG),\\nthese experiments measured brain potentials from subjects that were instructed to flex their wrist at a\\ntime of their choice and note the position of a rotating dot on a clock when they felt the urge to move.\\nThe results suggested that a slow cortical wave measured over motor areas?termed ?readiness potential? [5], and known to precede voluntary movement [6]?begins a few hundred milliseconds before the average reported time of the subjective ?urge? to move. This suggested that action onset and\\ncontents could be decoded from preparatory motor signals in the brain before the subject becomes\\naware of an intention to move and of the contents of the action. However, the readiness potential\\nwas computed by averaging over 40 or more trials aligned to movement onset after the fact. More\\nrecently, it was shown that action contents can be decoded using functional magnetic-resonance\\nimaging (fMRI) several seconds before movement onset [7]. But, while done on a single-trial basis,\\ndecoding the neural signals took place off-line, after the experiment was concluded, as the sluggish\\nnature of fMRI hemodynamic signals precluded real-time analysis. Moreover, the above studies\\nfocused on arbitrary and meaningless action?purposelessly raising the left or right hand?while\\nwe wanted to investigate prediction of reasoned action in more realistic, everyday situations with\\nconsequences for the subject.\\nIntracranial recordings are good candidates for single-trial, ORT analysis of action onset and contents [8, 9], because of the tight temporal pairing of LFP to the underlying neuronal signals. Moreover, such recordings are known to be cleaner and more robust, with signal-to-noise ratios up to\\n100 times larger than surface recordings like EEG [10, 11]. We therefore took advantage of a rare\\nopportunity to work with epilepsy patients implanted with intracranial electrodes for clinical purposes. Our ORT system (Fig. 1) predicts, with far above chance accuracy, which one of two future\\nactions is about to occur on this one trial and feeds the prediction back to the experimenter, all\\nbefore the onset of the go signal that triggers the patient?s movement (see Experimental Methods).\\nWe achieve relatively high prediction performance using only part of the data?learning from brain\\nactivity in past trials only (Fig. 2) to predict future ones (Fig. 3)?while still running the analysis\\nquickly enough to act upon the prediction before the subject moved.\\n\\n2\\n2.1\\n\\nExperimental Methods\\nSubjects\\n\\nSubjects in this experiment were 8 consenting intractable epilepsy patients that were implanted with\\nintracranial electrodes as part of their presurgical clinical evaluation (ages 18?60, 3 males). They\\nwere inpatients in the neuro-telemetry ward at the Cedars Sinai Medical Center or the Huntington\\nMemorial Hospital, and are designated with CS or HMH after their patient numbers, respectively. Six\\nof them?P12CS, P15CS, P22CS and P29?31HMH were implanted with intracortical depth electrodes targeting their bilateral anterior-cingulate cortex, amygdala, hippocampus and orbitofrontal\\ncortex. These electrodes had eight 40 ?m microwires at their tips, 7 for recording and 1 serving as\\na local ground. Two patients, P15CS and P22CS, had additional microwires in the supplementary\\nmotor area. We utilized the LFP recorded from the microwires in this study. Two other patients,\\nP16CS and P19CS, were implanted with an 8?8 subdural grid (64 electrodes) over parts of their\\ntemporal and prefrontal dorsolateral cortices. The data of one patient?P31HMH?was excluded\\nbecause microwire signals were too noisy for meaningful analysis. The institutional review boards\\nof Cedars Sinai Medical Center, the Huntington Memorial Hospital and the California Institute of\\nTechnology approved the experiments.\\nDuring the experiment, the subject sat in a hospital bed in a semi-inclined ?lounge chair? position.\\nThe stimulus/analysis computer (bottom left of Fig. 4) displaying the game screen (bottom right\\ninset of Fig. 4) was positioned to be easily viewable for the subject. When playing against the\\nexperimenter, the latter sat beside the bed. The response box was placed within easy reach of the\\nsubject (Fig. 4).\\n2\\n\\n\\f2.2\\n\\nExperiment Design\\n\\nAs part of our focus on purposeful, reasoned action, we had the subjects play a matching-pennies\\ngame?a 2-choice version of ?rock paper scissors??either against the experimenter or against a\\ncomputer. The subjects pressed down a button with their left hand and another with their right on a\\nresponse box. Then, in each trial, there was a 5 s countdown followed by a go signal, after which\\nthey had to immediately lift one of their hands. It was agreed beforehand that the patient would win\\nthe trial if she lifted a different hand than her opponent, and lose if she raised the same hand as her\\nopponent. Both players started off with a fixed amount of money, $5, and in each trial $0.10 was\\ndeducted from the loser and awarded to the winner. If a player lifted her hand before the go signal,\\ndid not lift her hand within 500 ms of the go signal, or lifted no hand or both hands at the go signal?\\nan error trial?she lost $0.10 without her opponent gaining any money. The subjects were shown the\\ncountdown, the go signal, the overall score, and various instructions on a stimulus computer placed\\nbefore them (Fig. 4). Each game consisted of 50 trials. If, at the end of the game, the subject had\\nmore money than her opponent, she received that money in cash from the experimenter.\\nBefore the experimental session began, the experimenter explained the rules of the game to the subject, and she could practice playing the game until she was familiar with it. Consequently, patients\\nusually made only few errors during the games (<6% of the trials). Following the tutorial, the subject played 1?3 games against the computer and then once against the experimenter, depending on\\ntheir availability and clinical circumstances. The first 2 games of P12CS were removed because\\nthe subject tended to constantly raise the right hand regardless of winning or losing. Two patients,\\nP15CS and P19CS, were tested in actual ORT conditions. In such sessions?3 games each?the\\nsubjects always played against the experimenter. These ORT games were different from the other\\ngames in two respects. First, a computer screen was placed behind the patient, in a location where\\nshe could not see it. Second, the experimenter was wearing earphones (Fig. 1,4). Half a second before go-signal onset, an arrow pointing towards the hand that the system predicted the experimenter\\nhad to raise to win the trial was displayed on that screen. Simultaneously, a monophonic tone was\\nplayed in the experimenter?s earphone ipsilateral to that hand. The experimenter then lifted that hand\\nat the go signal (see Supplemental Movie).\\n\\nCheetah Machine\\nCollect\\nand save\\ndata\\n\\nPatient\\nwith intracranial electrodes\\n\\nDown\\nsampling\\n\\nBuffer\\n\\n1Gbps\\nRouter\\n\\nTTL Signal\\n\\nThe winner is\\nPlayer 1\\nPLAYER 1 PLAYER 2\\nSCORE 1\\n\\nAnalysis/stimulus machine\\n\\nSCORE 2\\n\\nResponse Box Game Screen\\n\\n/\\nExperimenter\\n\\nResult\\nInterpreta\\ntion\\n\\nAnalysis\\n\\nFiltering\\n\\nDisplay/Sound\\n\\nFigure 1: A schematic diagram of the on-line real-time (ORT) system. Neural signals flow from\\nthe patient through the Cheetah machine to the analysis/stimulus computer, which controls the input\\nand output of the game and computes the prediction of the hand the patient would raise at the go\\nsignal. It displays it on a screen behind the patient and informs the experimenter which hand to raise\\nby playing a tone in his ipsilateral ear using earphones.\\n\\n3\\n\\n\\f3\\n3.1\\n\\nThe real-time system\\nHardware and software overview\\n\\n?V\\n\\n?V\\n\\n?V\\n\\nNeural data from the intracranial electrodes were transferred to a recording system (Neuralynx,\\nDigital Lynx), where it was collected and saved to the local Cheetah machine, down sampled\\nfrom 32 kHz to 2 kHz and buffered. The data were then transferred, through a dedicated 1 Gbps\\nlocal-area network, to the analysis/stimulus machine. This computer first band-pass-filtered the\\ndata to the 0.1?5 Hz range (delta and lower theta bands) using a second-order zero-lag elliptic\\nfilter with an attenuation of 40 dB (cf. Figs. 2a and 2b). We found that this frequency range?\\ngenerally comparable to that of the readiness potential?resulted in optimal prediction performance.\\nIt then ran the analysis algorithm (see below) on the filtered data. This computer also controlled\\nthe game screen, displaying the names of the players, their current scores and various instructions.\\nThe analysis/stimulus computer further\\ncontrolled the response box, which con- (a)\\n800\\nsisted of 4 LED-lit buttons. The buttons of the subject and her opponent\\n600\\nflashed red or blue whenever she or her\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nopponent won, respectively. Addition(b)100\\nally, the analysis/stimulus computer sent\\n0\\na unique transistor-transistor logic (TTL)\\n?100\\n?200\\npulse whenever the game screen changed\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nor a button was pressed on the response\\nbox, which synchronized the timing of (c) 100\\n0\\nthese events with the LFP recordings.\\n?100\\nIn real-time game sessions, the analy?200\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nsis/stimulus computer also displayed the\\nappropriate arrow on the computer screen (d) 1\\nbehind the subject and played the tone\\n0\\nto the appropriate ear of the experimenter\\n?1\\n0.5 s before go-signal onset (Figs. 1,4).\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nThe analysis software was based on a\\nmachine-learning algorithm that trained\\non past-trials data to predict the current\\ntrial and is detailed below. The training phase included the first 70% of the\\ntrials, with the prediction carried out on\\nthe remaining 30% using the trained parameters, together with an online weighting system (see below). The system examined only neural activity, and had no\\naccess to the subject?s left/right-choice\\nhistory. After filtering all the training\\ntrials (Fig. 2b), the system found the\\nmean and standard error over all leftward\\nand rightward training trials, separately\\n(Fig. 2c, left designated in red). It then\\nfound the electrodes and time windows\\nwhere the left/right separation was high\\n(Fig. 2d,e; see below), and trained the classifiers on these time windows (Fig. 2f?g).\\nThe best electrode/time-window/classifier\\n(ETC) combinations were then used to\\npredict the current trial in the prediction\\nphase (Fig. 3). The number of ETCs that\\ncan be actively monitored is currently limited to 10 due to the computational power\\nof the real-time system.\\n\\nEl 49?T1\\n\\n(e)\\n\\nEl 49?T2\\n\\nEl 49?T3\\n\\n1\\n0\\n?1\\n?5\\n\\n?4\\n\\n?3\\n?2\\n?1\\nCountdown to go signal at t=0 (seconds)\\n\\n0\\n\\n(f)\\nClassifier\\nCf1\\n\\nClassifier\\nCf2\\n\\n...\\n\\nClassifier\\nCf6\\n\\nEl 49?T1?Cf1\\nEl 49?T1?Cf2\\nEl 49?T1?Cf6\\n...\\nEl 49?T2?Cf1\\nEl 49?T2?Cf2\\nEl 49?T2?Cf6\\nEl 49?T3?Cf1\\nEl 49?T3?Cf2\\nEl 49?T3?Cf6\\n\\n(g)\\nCombination\\nEl49-T1-Cf2\\n\\nCombination\\nEl49-T2-Cf2\\n\\n...\\n\\nCombination\\nEl49-T2-Cf6\\n\\nFigure 2: The ORT-system?s training phase. Left (in\\nred) and right (in blue) raw signals (a) are low-pass filtered (b). Mean?standard errors of signals preceeding left- and right-hand movments (c) are used to compute a left/right separability index (d), from which time\\nwindows with good separation are found (e). Seven\\nclassifiers are then applied to all the time windows (f)\\nand the best electrode/time-window/classifier combinations are selected (g) and used in the prediction phase\\n(Fig. 3).\\n\\n4\\n\\n\\f?V\\n\\n100\\n0\\n?100\\n?200\\n?5\\n\\n?4\\n\\n?3\\n\\n?2\\n\\n?1\\n\\n0\\n\\nTrained classifiers\\n\\nCombination\\nE l 49?T1?Cf2\\n\\nCombination\\nE l 49?T2?Cf2\\n\\nWeight = 1\\n\\nWeight = 1\\n\\nCombination\\nE l 49?T2?Cf6\\n\\n&\\n\\nWeight = 1\\n\\nPredicted result\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nR\\n\\nL\\n\\n&\\n\\nR\\n\\nL\\nReal result\\n\\nAdjust the weights\\n\\nL\\n\\n==\\n\\nFigure 3: The ORT-system?s prediction phase. A new signal?from 5 to 0.5 seconds before the\\ngo signal?is received in real time, and each electrode/time-window/classifier combination (ETC)\\nclassifies it as resulting in left- or right-hand movement. These predictions are then compared to the\\nactual hand movement, with the weights associated with ETCs that correctly (incorrectly) predicted\\nincreasing (decreasing).\\n\\n3.2\\n\\nComputing optimal left/right-separating time windows\\n\\nThe algorithm focused on finding the time windows with the best left/right separation for the different recording electrodes over the training set (Fig. 2c?e). That is, we wanted to predict whether\\nthe signal aN (t) on trial N will result in a leftward or rightward movement?i.e., whether the label of the N th trial will be Lt or Rt, respectively. For each electrode, we looked at the N ? 1\\nprevious trials a1 (t), a2 (t), . . . , aN ?1 (t), and their associated labels as l1 , l2 , . . . , lN ?1 . Now, let\\nN ?1\\n?1\\nL(t) = {ai (t) | li = Lt}N\\ni=1 and R(t) = {ai (t) | li = Rt}i=1 be the set of previous leftward and\\nrightward trials in the training set, respectively. Furthermore, let Lm (t) (Rm (t)) and Ls (t) (Rs (t))\\nbe the mean and standard error of L(t) (R(t)), respectively. We can now define the normalized\\nrelative left/right separation for each electrode at time t (see Fig. 2d):\\n?\\n[Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)]\\n?\\n?\\nif [Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)] > 0\\n?\\n?\\nLm (t) ? Rm (t)\\n?\\n?\\n?\\n?\\n?\\n[Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)]\\n?(t) =\\n?\\nif [Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)] > 0\\n?\\n?\\n?\\nRm (t) ? Lm (t)\\n?\\n?\\n?\\n?\\n?\\n?\\n0\\notherwise\\nThus, ?(t) > 0 (?(t) < 0) means that the leftward trials tend to be considerably higher (lower)\\nthan rightward trials for that electrode at time t, while ?(t) = 0 suggests no left/right separation at\\ntime t. We define a consecutive time period of |?(t)| > 0 for t < prediction time (the time before\\nthe go signal when we want the system to output a prediction; -0.5 s for the ORT trials) as a time\\nwindow (Fig. 2e). After all time windows are found for all electrodes, time windows lessRthan M ms\\nt\\napart are combined into one. Then, for each time window from t1 to t2 we define a = t12 |?(t)|dt.\\nWe then eliminate all time windows satisfying a < A. We found the values M = 200 ms and\\nA = 4, 500 ?V ? ms to be optimal for real-time analysis. This resulted in 20?30 time windows over\\nall 64 electrodes that we monitored.\\n5\\n\\n\\f1\\n$4.80\\n\\n$5.20\\n\\nP15CS\\n\\nUri\\n\\nFigure 4: The experimental setup in the clinic. At 400 ms before the go signal, the patient and\\nexperimenter are watching the game screen (inset on bottom right) on the analysis/stimulus computer\\n(bottom left) and still pressing down the buttons of the response box. The realtime system already\\ncomputed a prediction, and thus displays an arrow on the screen behind the patient and plays a tone\\nin the experimenter?s ear ipsilateral to the hand it predicts he should raise to beat the patient (see\\nSupplemental Movie).\\n3.3\\n\\nClassifiers selection and ETC determination\\n\\nWe used ensemble learning with 7 types of relatively simple binary classifiers (due to real-time\\nprocessing considerations) on every electrode?s time windows (Fig. 2f). Classifiers A to G would\\nclassify aN (t) as Lt if:\\nP\\nP\\nP\\n(A) Defining aN,M , Lm,M and Rm,M as aN (t), Lm (t) and Rm (t) over time window M ,\\n\\u0001\\n\\u0001\\n\\u0001\\n(i) sign Rm,M 6= sign aN,M = sign Lm,M , or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(ii) sign Rm,M = sign aN,M = sign Lm,M and \\fLm,M \\f > \\fRm,M \\f, or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(iii) sign Rm (t) 6= sign SN,M 6= sign Lm (t) and \\fLm,M \\f < \\fRm,M \\f;\\n\\f\\n\\u0001\\n\\u0001\\f \\f\\n\\u0001\\n\\u0001\\f\\n(B) \\fmean aN (t) ? mean Lm (t) \\f < \\fmean aN (t) ? mean Rm (t) \\f;\\n\\f\\n\\f\\n\\u0001\\n\\u0001\\f\\n\\u0001\\n\\u0001\\f\\n(C) \\fmedian aN (t) ? median Lm (t) \\f < \\fmedian aN (t) ? median Rm (t) \\f over the time\\nwindow;\\n\\f\\n\\f\\n\\f\\n\\f\\n\\f\\n(D) aN (t) ? Lm (t)\\fL2 < \\faN (t) ? Rm (t)\\fL2 over the time window;\\n(E) aN (t) is convex/concave like Lm (t) while Rm (t) is concave/convex, respectively;\\n(F) Linear support-vector machine (SVM) designates it as so; and\\n(G) k-nearest neighbors (KNN) with Euclidean distance designates it as so.\\nEach classifier is optimized for certain types of features. To estimate how well its classification\\nwould generalize from the training to the test set, we trained and tested it using a 70/30 crossvalidation procedure within the training set. We tested each classifier on every time window of every\\nelectrode, discarding those with accuracy <0.68, which left 12.0 ? 1.6% of the original 232 ? 18\\nETCs, on average (?standard error). The training phase therefore ultimately output a set of S binary\\nETC combinations (Fig. 2g) that were used in the prediction phase (Fig. 3).\\n3.4\\n\\nThe prediction-phase weighting system\\n\\nIn the prediction phase, each of the overall S binary ETCs calculates a prediction, ci ? {?1, 1} (for\\nright and left, respectively), independently at the desired prediction time. All classifiers are initially\\n6\\n\\n\\fPS\\ngiven the same weight, w1 = w2 = ? ? ? = wS = 1. We then calculate ? = i=1 wi ? ci and predict\\nleft (right) if ? > d (? < ?d), or declare it an undetermined trial if ?d < ? < d. Here d is the\\ndrop-off threshold for the prediction. Thus the larger d is, the more confident the system needs to be\\nto make a prediction, and the larger the proportion of trials on which the system abstains?the dropoff rate. Weight wi associated with ETCi is increased (decreased) by 0.1 whenever ETCi predicts\\nthe hand movement correctly (incorrectly). A constantly erring ETC would therefore be associated\\nwith an increasingly small and then increasingly negative weight.\\n3.5\\n\\nImplementation\\n\\nThe algorithm was implemented in MATLAB 2011a (MathWorks, Natick, MA) as well as in C++\\non Visual Studio 2008 (Microsoft, Redmond, WA) for enhanced performance. The neural signals\\nwere collected by the Digital Lynx S system using Cheetah 5.4.0 (Neuralynx, Redmond, WA). The\\nsimulated-ORT system was also implemented in MATLAB 2011a. The simulated-ORT analyses\\ncarried out in this paper used real patient data saved on the Digital Lynx system.\\n1\\n\\n0.9\\n\\nDrop rate:\\nNone\\n0.18\\n0\\u0011\\u0016\\u0013\\n\\nPrediction accuracy\\n\\n0.8\\n\\n0.7\\nSignificant accuracy\\n(p=0.05)\\n0.6\\n\\n0.5\\n\\n?5\\n\\n?4.5\\n\\n?4\\n\\n?3.5\\n\\n?3\\n\\n?2.5\\nTime (s)\\n\\n?2\\n\\n?1.5\\n\\n?1\\n\\n?0.5\\n\\n0\\nGo-signal\\nonset\\n\\nFigure 5: Across-subjects average of the prediction accuracy of simulated-ORT versus time before\\nthe go signal. The mean accuracies over time when the system predicts on every trial, is allowed\\nto drop 19% or 30% of the trials, are depicted in blue, green and red, respectively (?standard error\\nshaded). Values above the dashed horizontal line are significant at p = 0.05.\\n\\n4\\n\\nResults\\n\\nWe tested our prediction system in actual real time on 2 patients?P15CS and P19CS (a depth\\nand grid patient, respectively), with a prediction time of 0.5 s before the go signal (see Supplementary Movie). Because of computational limitations, the ORT system could only track 10\\nelectrodes with just 1 ETC per electrode in real time. For P15CS, we achieved an accuracy of\\n72?2% (?standard error; accuracy = number of accurately predicted trials / [total number of trials - number of dropped trials]; p = 10?8 , binomial test) without modifying the weights online during the prediction (see Section 3.4). For P19CS we did not run patient-specific training of the ORT system, and used parameter values that were good on average over previous patients instead. The prediction accuracy was significantly above chance 63?2% (?standard error; p = 7 ? 10?4 , binomial test). To understand how much we could improve our accuracy\\nwith optimized hardware/software, we ran the simulated-ORT at various prediction times along\\n7\\n\\n\\fAccuracy\\n\\nthe 5 s countdown leading to the go signal. We further tested 3 drop-off rates?0, 0.19 and\\n0.30 (Fig. 5; drop-off rate = number of dropped trials / total number of trials; these resulted\\nfrom 3 drop-off thresholds?0, 0.1 and 0.2?respectively, see Section 3.4:). Running offline,\\nwe were able to track 20?30 ETCs, which resulted in considerably higher accuracies (Figs. 5,6).\\nAveraged over all subjects, the accuracy rose from about 65% more than\\n1\\n4 s before the go signal to 83?92%\\nclose to go-signal onset, depending\\n0.9\\non the allowed drop-off rate. In particular, we found that for a predic0.8\\ntion time of 0.5 s before go-signal\\nonset, we could achieve accuracies\\n0.7\\nof 81?5% and 90?3% (?standard\\nerror) for P15CS and P19CS, re0.6\\nspectively, with no drop off (Fig. 6).\\nPatients:\\nP12CS\\nWe also analyzed the weights that\\nP15CS\\nour weighting system assigned to the\\n0.5\\nP16CS\\nP19CS\\ndifferent ETCs. We found that the\\nP22CS\\nempirical distribution of weights to\\nP29HMH\\n0.4\\nP30HMH\\nETCs associated with classifiers A to\\nG was, on average: 0.15, 0.12, 0.16,\\n?5 ?4.5 ?4 ?3.5 ?3 ?2.5 ?2 ?1.5 ?1 ?0.5 0\\n0.22, 0.01, 0.26 and 0.07, respecTime before go signal (at t=0) (seconds)\\ntively. This suggests that the linear\\nSVM and L2-norm comparisons (of\\naN to Lm and Rm ) together make up Figure 6: Simulated-ORT accuracy over time for individual\\nnearly half of the overall weights at- patients with no drop off.\\ntributed to the classifiers, while the\\ncurrent concave/convex measure is of\\nlittle use as a classifier.\\n\\n5\\n\\nDiscussion\\n\\nWe constructed an ORT system that, based on intracranial recordings, predicted which hand a person would raise well before movement onset at accuracies much greater than chance in a competitive environment. We further tested this system off-line, which suggested that with optimized\\nhardware/software, such action contents would be predictable in real time at relatively high accuracies already several seconds before movement onset. Both our prediction accuracy and drop-off\\nrates close to movement onset are superior to those achieved before movement onset with noninvasive methods like EEG and fMRI [7, 12?14]. Importantly, our subjects played a matching pennies game?a 2-choice version of rock-paper-scissors [15]?to keep their task realistic, with minor\\nthough real consequences, unlike the Libet-type paradigms whose outcome bears no consequences\\nfor the subjects. It was suggested that accurate online, real-time prediction before movement onset\\nis key to investigating the relation between the neural correlates of decisions, their awareness, and\\nvoluntary action [16, 17]. Such prediction capabilities would facilitate many types of experiments\\nthat are currently infeasible. For example, it would make it possible to study decision reversals on\\na single-trial basis, or to test whether subjects can guess above chance which of their action contents are predictable from their current brain activity, potentially before having consciously made up\\ntheir mind [16, 18]. Accurately decoding these preparatory motor signals may also result in earlier\\nand improved classification for brain-computer interfaces [13, 19, 20]. The work we present here\\nsuggests that such ORT analysis might well be possible.\\nAcknowledgements\\nWe thank Ueli Rutishauser, Regan Blythe Towel, Liad Mudrik and Ralph Adolphs for meaningful\\ndiscussions. This research was supported by the Ralph Schlaeger Charitable Foundation, Florida\\nState University?s ?Big Questions in Free Will? initiative and the G. Harold & Leila Y. Mathers\\nCharitable Foundation.\\n8\\n\\n\\fReferences\\n[1] B. Libet, C. Gleason, E. Wright, and D. Pearl. Time of conscious intention to act in relation to\\nonset of cerebral activity (readiness-potential): The unconscious initiation of a freely voluntary\\nact. Brain, 106:623, 1983.\\n[2] B. Libet. Unconscious cerebral initiative and the role of conscious will in voluntary action.\\nBehavioral and brain sciences, 8:529?539, 1985.\\n[3] P. Haggard and M. Eimer. On the relation between brain potentials and the awareness of\\nvoluntary movements. Experimental Brain Research, 126:128?133, 1999.\\n[4] A. Sirigu, E. Daprati, S. Ciancia, P. Giraux, N. Nighoghossian, A. Posada, and P. Haggard.\\nAltered awareness of voluntary action after damage to the parietal cortex. Nature Neuroscience,\\n7:80?84, 2003.\\n[5] H. Kornhuber and L. Deecke. Hirnpotenti?alanderungen bei Willk?urbewegungen und passiven\\nBewegungen des Menschen: Bereitschaftspotential und reafferente Potentiale. Pfl?ugers Archiv\\nEuropean Journal of Physiology, 284:1?17, 1965.\\n[6] H. Shibasaki and M. Hallett. What is the Bereitschaftspotential? Clinical Neurophysiology,\\n117:2341?2356, 2006.\\n[7] C. Soon, M. Brass, H. Heinze, and J. Haynes. Unconscious determinants of free decisions in\\nthe human brain. Nature Neuroscience, 11:543?545, 2008.\\n[8] I. Fried, R. Mukamel, and G. Kreiman. Internally generated preactivation of single neurons in\\nhuman medial frontal cortex predicts volition. Neuron, 69:548?562, 2011.\\n[9] M. Cerf, N. Thiruvengadam, F. Mormann, A. Kraskov, R. Quian Quiorga, C. Koch, and\\nI. Fried. On-line, voluntary control of human temporal lobe neurons. Nature, 467:1104?1108,\\n2010.\\n[10] T. Ball, M. Kern, I. Mutschler, A. Aertsen, and A. Schulze-Bonhage. Signal quality of simultaneously recorded invasive and non-invasive EEG. Neuroimage, 46:708?716, 2009.\\n[11] G. Schalk, J. Kubanek, K. Miller, N. Anderson, E. Leuthardt, J. Ojemann, D. Limbrick,\\nD. Moran, L. Gerhardt, and J. Wolpaw. Decoding two-dimensional movement trajectories\\nusing electrocorticographic signals in humans. Journal of Neural engineering, 4:264, 2007.\\n[12] O. Bai, V. Rathi, P. Lin, D. Huang, H. Battapady, D. Y. Fei, L. Schneider, E. Houdayer, X. Chen,\\nand M. Hallett. Prediction of human voluntary movement before it occurs. Clinical Neurophysiology, 122:364?372, 2011.\\n[13] O. Bai, P. Lin, S. Vorbach, J. Li, S. Furlani, and M. Hallett. Exploration of computational\\nmethods for classification of movement intention during human voluntary movement from\\nsingle trial EEG. Clinical Neurophysiology, 118:2637?2655, 2007.\\n[14] U. Maoz, A. Arieli, S. Ullman, and C. Koch. Using single-trial EEG data to predict laterality\\nof voluntary motor decisions. Society for Neuroscience, 38:289.6, 2008.\\n[15] C. Camerer. Behavioral game theory: Experiments in strategic interaction. Princeton University Press, 2003.\\n[16] J. D. Haynes. Decoding and predicting intentions. Annals of the New York Academy of Sciences, 1224:9?21, 2011.\\n[17] P. Haggard. Decision time for free will. Neuron, 69:404?406, 2011.\\n[18] J. D. Haynes. Beyond libet. In W. Sinnott-Armstrong and L. Nadel, editors, Conscious will\\nand responsibility, pages 85?96. Oxford University Press, 2011.\\n[19] A. Muralidharan, J. Chae, and D. M. Taylor. Extracting attempted hand movements from EEGs\\nin people with complete hand paralysis following stroke. Frontiers in neuroscience, 5, 2011.\\n[20] E. Lew, R. Chavarriaga, S. Silvoni, and J. R. Milln. Detection of self-paced reaching movement\\nintention from EEG signals. Frontiers in Neuroengineering, 5:13, 2012.\\n\\n9\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(\"/content/NIPS Papers.zip\", \"r\") as zip_ref:\n",
        "    # Extract the file to a temporary directory\n",
        "    zip_ref.extractall(\"temp\")\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "papers = pd.read_csv(\"temp/NIPS Papers/papers.csv\")\n",
        "\n",
        "# Print head\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7QtApaZw_AN"
      },
      "source": [
        "** **\n",
        "#### Step 2: Data Cleaning <a class=\"anchor\\\" id=\"clean_data\"></a>\n",
        "** **\n",
        "\n",
        "Since the goal of this analysis is to perform topic modeling, let's focus only on the text data from each paper, and drop other metadata columns. Also, for the demonstration, we'll only look at 100 papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ipntFuhHw_AN",
        "outputId": "f5ad631d-6ba3-4267-9a5a-7a5c93831aa8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      year                                              title  \\\n",
              "4545  2013  Density estimation from unweighted k-nearest n...   \n",
              "4685  2014  Delay-Tolerant Algorithms for Asynchronous Dis...   \n",
              "4562  2013  One-shot learning by inverting a compositional...   \n",
              "4459  2013                      Memory Limited, Streaming PCA   \n",
              "1447  2002                       Learning Semantic Similarity   \n",
              "\n",
              "                                               abstract  \\\n",
              "4545  Consider an unweighted k-nearest neighbor grap...   \n",
              "4685  We analyze new online gradient descent algorit...   \n",
              "4562  People can learn a new visual class from just ...   \n",
              "4459  We consider streaming, one-pass principal comp...   \n",
              "1447                                   Abstract Missing   \n",
              "\n",
              "                                             paper_text  \n",
              "4545  Density estimation from unweighted k-nearest\\n...  \n",
              "4685  Delay-Tolerant Algorithms for\\nAsynchronous Di...  \n",
              "4562  One-shot learning by inverting a compositional...  \n",
              "4459  Memory Limited, Streaming PCA\\n\\nConstantine C...  \n",
              "1447  Learning Semantic Similarity\\n\\nJaz Kandola\\nJ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4152e0fb-e8a0-4f2c-89cb-6b2d88fc0c89\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4545</th>\n",
              "      <td>2013</td>\n",
              "      <td>Density estimation from unweighted k-nearest n...</td>\n",
              "      <td>Consider an unweighted k-nearest neighbor grap...</td>\n",
              "      <td>Density estimation from unweighted k-nearest\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4685</th>\n",
              "      <td>2014</td>\n",
              "      <td>Delay-Tolerant Algorithms for Asynchronous Dis...</td>\n",
              "      <td>We analyze new online gradient descent algorit...</td>\n",
              "      <td>Delay-Tolerant Algorithms for\\nAsynchronous Di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4562</th>\n",
              "      <td>2013</td>\n",
              "      <td>One-shot learning by inverting a compositional...</td>\n",
              "      <td>People can learn a new visual class from just ...</td>\n",
              "      <td>One-shot learning by inverting a compositional...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4459</th>\n",
              "      <td>2013</td>\n",
              "      <td>Memory Limited, Streaming PCA</td>\n",
              "      <td>We consider streaming, one-pass principal comp...</td>\n",
              "      <td>Memory Limited, Streaming PCA\\n\\nConstantine C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1447</th>\n",
              "      <td>2002</td>\n",
              "      <td>Learning Semantic Similarity</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Learning Semantic Similarity\\n\\nJaz Kandola\\nJ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4152e0fb-e8a0-4f2c-89cb-6b2d88fc0c89')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4152e0fb-e8a0-4f2c-89cb-6b2d88fc0c89 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4152e0fb-e8a0-4f2c-89cb-6b2d88fc0c89');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-404a8381-99a9-4737-85cc-daf35b31d9a3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-404a8381-99a9-4737-85cc-daf35b31d9a3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-404a8381-99a9-4737-85cc-daf35b31d9a3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "papers",
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 1988,\n        \"max\": 2016,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          2016,\n          2000,\n          2013\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Stochastic Neighbor Embedding\",\n          \"Sufficient Conditions for Agnostic Active Learnable\",\n          \"Viewpoint Invariant Face Recognition using Independent Component Analysis and Attractor Networks\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 51,\n        \"samples\": [\n          \"We propose a discriminative latent model for annotating images with unaligned object-level textual annotations. Instead of using the bag-of-words image representation currently popular in the computer vision community, our model explicitly captures more intricate relationships underlying visual and textual information. In particular, we model the mapping that translates image regions to annotations. This mapping allows us to relate image regions to their corresponding annotation terms. We also model the overall scene label as latent information. This allows us to cluster test images. Our training data consist of images and their associated annotations. But we do not have access to the ground-truth region-to-annotation mapping or the overall scene label. We develop a novel variant of the latent SVM framework to model them as latent variables. Our experimental results demonstrate the effectiveness of the proposed model compared with other baseline methods.\",\n          \"We study the problem of efficiently estimating the coefficients of generalized linear models (GLMs) in the large-scale setting where the number of observations $n$ is much larger than the number of predictors $p$, i.e. $n\\\\gg p \\\\gg 1$. We show that in GLMs with random (not necessarily Gaussian) design, the GLM coefficients are approximately proportional to the corresponding ordinary least squares (OLS) coefficients. Using this relation, we design an algorithm that achieves the same accuracy as the maximum likelihood estimator (MLE)  through iterations that  attain up to a cubic convergence rate, and that are cheaper than  any batch optimization algorithm by at least a factor of $\\\\mathcal{O}(p)$. We provide theoretical guarantees for our algorithm, and analyze the convergence behavior in terms of data dimensions. % Finally, we demonstrate the performance of  our algorithm through extensive numerical studies  on large-scale real and synthetic datasets, and show that it achieves the highest performance compared to several other widely used optimization algorithms.\",\n          \"We propose efficient algorithms for simultaneous clustering and completion of incomplete high-dimensional data that lie in a union of low-dimensional subspaces. We cast the problem as finding a completion of the data matrix so that each point can be reconstructed as a linear or affine combination of a few data points. Since the problem is NP-hard, we propose a lifting framework and reformulate the problem as a group-sparse recovery of each incomplete data point in a dictionary built using incomplete data, subject to rank-one constraints. To solve the problem efficiently, we propose a rank pursuit algorithm and a convex relaxation. The solution of our algorithms recover missing entries and provides a similarity matrix for clustering. Our algorithms can deal with both low-rank and high-rank matrices, does not suffer from initialization, does not need to know dimensions of subspaces and can work with a small number of data points. By extensive experiments on synthetic data and real problems of video motion segmentation and completion of motion capture data, we show that when the data matrix is low-rank, our algorithm performs on par with or better than low-rank matrix completion methods, while for high-rank data matrices, our method significantly outperforms existing algorithms.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Stochastic Neighbor Embedding\\n\\nGeoffrey Hinton and Sam Roweis\\nDepartment of Computer Science, University of Toronto\\n10 King?s College Road, Toronto, M5S 3G5 Canada\\nhinton,roweis @cs.toronto.edu\\n\\u0001\\n\\nAbstract\\nWe describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a\\nlow-dimensional space in a way that preserves neighbor identities. A\\nGaussian is centered on each object in the high-dimensional space and\\nthe densities under this Gaussian (or the given dissimilarities) are used\\nto define a probability distribution over all the potential neighbors of\\nthe object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the\\nlow-dimensional ?images? of the objects. A natural cost function is a\\nsum of Kullback-Leibler divergences, one per object, which leads to a\\nsimple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic\\nframework makes it easy to represent each object by a mixture of widely\\nseparated low-dimensional images. This allows ambiguous objects, like\\nthe document count vector for the word ?bank?, to have versions close to\\nthe images of both ?river? and ?finance? without forcing the images of\\noutdoor concepts to be located close to those of corporate concepts.\\n\\n1 Introduction\\nAutomatic dimensionality reduction is an important ?toolkit? operation in machine learning, both as a preprocessing step for other algorithms (e.g. to reduce classifier input size)\\nand as a goal in itself for visualization, interpolation, compression, etc. There are many\\nways to ?embed? objects, described by high-dimensional vectors or by pairwise dissimilarities, into a lower-dimensional space. Multidimensional scaling methods[1] preserve\\ndissimilarities between items, as measured either by Euclidean distance, some nonlinear\\nsquashing of distances, or shortest graph paths as with Isomap[2, 3]. Principal components analysis (PCA) finds a linear projection of the original data which captures as much\\nvariance as possible. Other methods attempt to preserve local geometry (e.g. LLE[4]) or\\nassociate high-dimensional points with a fixed grid of points in the low-dimensional space\\n(e.g. self-organizing maps[5] or their probabilistic extension GTM[6]). All of these methods, however, require each high-dimensional object to be associated with only a single\\nlocation in the low-dimensional space. This makes it difficult to unfold ?many-to-one?\\nmappings in which a single ambiguous object really belongs in several disparate locations\\nin the low-dimensional space. In this paper we define a new notion of embedding based on\\nprobable neighbors. Our algorithm, Stochastic Neighbor Embedding (SNE) tries to place\\nthe objects in a low-dimensional space so as to optimally preserve neighborhood identity,\\nand can be naturally extended to allow multiple different low-d images of each object.\\n\\n\\f2 The basic SNE algorithm\\nFor each object, , and each potential neighbor, \\u0001 , we start by computing the asymmetric\\nprobability, \\u0002\\u0004\\u0003\\u0006\\u0005 , that would pick \\u0001 as its neighbor:\\n\\n\\t\\u000b\\n\\n\\f\\u000f\\u000e\\u0011\\u0010\\u0013\\u0012\\u0015\\u0014\\u0003\\u0016\\u0005\\u0018\\u0017\\n\\u0019\\u001b\\u001a\\u001d\\u001c \\t\\u000b\\n\\n\\f\\u001f\\u000e\\u0011\\u0010\\u0013\\u0012 \\u0014 \\u001a\\n\\u001e \\u0003\\n\\u0003 \\u0017\\n\\n\\u0002 \\u0003\\u0006\\u0005\\b\\u0007\\n\\n(1)\\n\\nThe dissimilarities, \\u0012 \\u0014\\u0003\\u0016\\u0005 , may be given as part of the problem definition (and need not be\\nsymmetric), or they may be computed using the scaled squared Euclidean distance (?affinity?) between two high-dimensional points, !\\\"\\u0003$#$!\\u0004\\u0005 :\\n\\n%&%\\n\\n\\u0012 \\u0014\\u0003\\u0006\\u0005 \\u0007\\n)\\n\\n'% %\\n! \\u0003 \\u0010 ! \\u0005 \\u0014\\n(\\u001d) \\u0014\\n\\u0003\\n\\n(2)\\n\\nwhere \\u0003 is either\\n) set by hand or (as in some of our experiments) found by a binary search\\nfor the value of \\u0003 that makes the entropy of the distribution over neighbors equal to *'+\\u001d,.- .\\nHere, - is the effective number of local neighbors or ?perplexity? and is chosen by hand.\\nIn the low-dimensional space we also use Gaussian neighborhoods but with a fixed variance\\n(which we set without loss of generality to be \\u0014/ ) so the induced probability 01\\u0003\\u0016\\u0005 that point\\npicks point \\u0001 as its neighbor is a function of the low-dimensional images 23\\u0003 of all the\\nobjects and is given by the expression:\\n\\n\\t4\\n\\n\\f\\u001f\\u000e\\u0011\\u0010 %'% 2 \\u0003 \\u0010 2 \\u0005 &% % \\u0014\\n\\u0019 \\u001a\\u001d\\u001c \\t4\\n\\n\\f\\u001f\\u000e\\u0011\\u0010 '% % \\u0010 \\u001a \\u0017 &% % \\u0014\\n2 \\u0003 2\\n\\u000f\\n\\u001e \\u0003\\n\\u0017\\n\\n0\\u000b\\u0003\\u0006\\u0005 \\u0007\\n\\n(3)\\n\\nThe aim of the embedding is to match these two distributions as well as possible. This is\\nachieved by minimizing a cost function which is a sum of Kullback-Leibler divergences\\nbetween the original (\\u00025\\u0003\\u0016\\u0005 ) and induced ( 06\\u0003\\u0016\\u0005 ) distributions over neighbors for each object:\\n\\n7\\n\\n\\u000798\\n\\n8\\n\\u0003\\n\\n\\u0005\\n\\n\\u0002 \\u0003\\u0006\\u0005 *'+\\u001d,\\n\\n\\u0002:\\u0003\\u0006\\u0005\\n@\\u000e ? \\u0003 %&% A \\u0003 \\u0017\\n\\u0007;8\\n0\\u000b\\u0003\\u0006\\u0005\\n\\u00039<>=\\n\\n(4)\\n\\nThe dimensionality of the 2 space is chosen by hand (much less than the number of objects).\\nNotice that making 0 \\u0003\\u0006\\u0005 large when \\u0002 \\u0003\\u0006\\u0005 is small wastes some of the probability mass in the 0\\ndistribution so there is a cost for modeling a big distance in the high-dimensional space with\\na small distance in the low-dimensional space, though it is less than the cost of modeling\\na small distance with a big one. In this respect, SNE is an improvement over methods\\nlike LLE [4] or SOM [5] in which widely separated data-points can be ?collapsed? as near\\nneighbors in the low-dimensional space. The intuition is that while SNE emphasizes local\\ndistances, its cost function cleanly enforces both keeping the images of nearby objects\\nnearby and keeping the images of widely separated objects relatively far apart.\\nDifferentiating C is tedious because 2\\nthe result is simple: B\\n\\nB\\n\\n7\\n\\n2 \\u0003\\n\\n\\u0007\\n\\n(\\n\\n8\\n\\u0005\\n\\n\\u001a\\n\\naffects 0 \\u0003\\u0006\\u0005 via the normalization term in Eq. 3, but\\n\\n\\u000e 2\\u000f\\u0003 \\u0010 2\\u0004\\u0005 \\u0017 \\u000e \\u0004\\n\\u0002 \\u0003\\u0016\\u0005 \\u0010 0\\u000b\\u0003\\u0006\\u0005DCE\\u0002F\\u0005G\\u0003 \\u0010 0H\\u0005G\\u0003 \\u0017\\n\\n(5)\\n\\nwhich has the nice interpretation of a sum of forces pulling 2\\\"\\u0003 toward 2\\u0004\\u0005 or pushing it away\\ndepending on whether \\u0001 is observed to be a neighbor more or less often than desired.\\n\\n7\\n\\nGiven the gradient, there are many possible ways to minimize and we have only just begun the search for the best method. Steepest descent in which all of the points are adjusted\\nin parallel is inefficient and can get stuck in poor local optima. Adding random jitter that\\ndecreases with time finds much better local optima and is the method we used for the examples in this paper, even though it is still quite slow. We initialize the embedding by putting\\nall the low-dimensional images in random locations very close to the origin. Several other\\nminimization methods, including annealing the perplexity, are discussed in sections 5&6.\\n\\n\\f3 Application of SNE to image and document collections\\nAs a graphic illustration of the ability of SNE to model high-dimensional, near-neighbor\\nrelationships using only two dimensions, we ran the algorithm on a collection of bitmaps of\\nhandwritten digits and on a set of word-author counts taken from the scanned proceedings\\nof NIPS conference papers. Both of these datasets are likely to have intrinsic structure in\\nmany fewer dimensions than their raw dimensionalities: 256 for the handwritten digits and\\n13679 for the author-word counts.\\nTo begin, we used a set of \\u0002\\u0001\\u0003\\u0001\\u0002\\u0001 digit bitmaps from the UPS database[7] with \\u0004\\u0003\\u0001\\u0003\\u0001 examples\\nfrom each\\n(\\u0002\\u0005 of the five classes 0,1,2,3,4. The variance of the Gaussian around each point\\nin the \\u0004 -dimensional raw pixel image space was set to achieve a perplexity of 15 in the\\ndistribution over high-dimensional neighbors. SNE was initialized by putting all the 2\\\"\\u0003\\nin random locations very close to the origin and then was trained using gradient descent\\nwith annealed noise. Although SNE was given no information about class labels, it quite\\ncleanly separates the digit groups as shown in figure 1. Furthermore, within each region of\\nthe low-dimensional space, SNE has arranged the data so that properties like orientation,\\nskew and stroke-thickness tend to vary smoothly. For the embedding shown, the SNE\\ncost function in Eq. 4 has a value of \\u0004\\u0007\\u0006\\u0007\\b\\n\\t nats;\\n( with\\u0005 a uniform\\n\\u0005\\u000f\\u000e distribution across lowdimensional neighbors, the cost is \\u0003\\u0001\\u0002\\u0001\\u0003\\u0001 *'+\\u001d,\\f\\u000b \\u000e \\t\\u0003\\t\\u0002\\t\\f\\n\\u0007\\b \\u0017 \\u0007 \\b \\t\\u000f\\u0010 nats. We also applied\\nprincipal component analysis (PCA)[8] to the same data; the projection onto the first two\\nprincipal components does not separate classes nearly as cleanly as SNE because PCA is\\nmuch more interested in getting the large separations right which causes it to jumble up\\nsome of the boundaries between similar classes. In this experiment, we used digit classes\\nthat do not have very similar pairs like 3 and 5 or 7 and 9. When there are more classes and\\nonly two available dimensions, SNE does not as cleanly separate very similar pairs.\\nWe have also applied SNE to word-document and word-author matrices calculated from\\nthe OCRed text of NIPS volume 0-12 papers[9]. Figure 2 shows a map locating NIPS authors into two dimensions. Each of the 676 authors who published more than one paper\\nin NIPS vols. 0-12 is shown by a dot at the position 2 \\u0003 found by SNE; larger red dots\\nand corresponding last names are authors who published six or more papers in that period.\\nDistances \\u0012 \\u0003\\u0006\\u0005 were computed as the norm of the difference between log aggregate author\\nword counts, summed across all NIPS papers. Co-authored papers gave fractional counts\\nevenly to all authors. All words occurring in six or more documents were included, except for stopwords giving a vocabulary size of) 13649. (The bow toolkit[10] was used for\\npart of\\n(\\u0003\\u0005 the pre-processing of the data.) The \\u0003 were set to achieve a local perplexity of\\n- \\u0007\\nneighbors. SNE seems to have grouped authors by broad NIPS field: generative\\nmodels, support vector machines, neuroscience, reinforcement learning and VLSI all have\\ndistinguishable localized regions.\\n\\n4 A full mixture version of SNE\\nThe clean probabilistic formulation of SNE makes it easy to modify the cost function so\\nthat instead of a single image, each high-dimensional object can have several different\\nversions of its low-dimensional image. These alternative versions have mixing proportions\\nthat sum to \\b . Image-version \\u0011 of object has location 2 \\u0003\\u0013\\u0012 and mixing proportion 5\\n\\u0014 \\u0003\\u0013\\u0012 . The\\nlow-dimensional neighborhood distribution for is a mixture of the distributions induced\\nby each of its image-versions across all image-versions of a potential neighbor \\u0001 :\\n\\n0\\u000b\\u0003\\u0006\\u0005 \\u0007\\u001b8\\u0007\\u0015 \\u0014:\\u0003\\u0013\\u0012 8\\u0007\\u0016\\n\\n%&%\\n%'%\\n\\n\\u0014 \\u0005\\u0018\\u0017 4\\t \\n\\n\\u001e\\u001a \\f\\u001f\\u001d \\u000e$\\u0010 2\\u000f\\u0003\\u0019\\u0012 \\u0010 \\u0004\\n2 \\u0005\\u001a\\u0017 \\u0014 \\u001e\\u001a \\u0017 \\u001d\\n\\u0019\\u001b\\u001a \\u0019\\u001c\\u001b\\n\\t4\\n\\n\\f\\u001f\\u000e$\\u0010 %&% 2 \\u0003 \\u0012 \\u0010 2 '% % \\u0014 \\u0017\\n\\u0014\\n\\n(6)\\n\\nIn this multiple-image model, the derivatives with respect to the image locations 23\\u0003\\u0019\\u0012 are\\nstraightforward; the derivatives w.r.t the mixing proportions \\u0014 \\u0003\\u0013\\u0012 are most easily expressed\\n\\n\\f(\\u0002\\u0005\\nFigure 1: The result of running the SNE algorithm on \\u0002\\u0001\\u0003\\u0001\\u0003\\u0001\\n\\u0004 -dimensional grayscale\\nimages of handwritten digits. Pictures of the original data vectors ! \\u0003 (scans of handwritten\\ndigit) are shown at the location corresponding to their low-dimensional images 23\\u0003 as found\\nby SNE. The classes are quite well separated even though SNE had no information about\\nclass labels. Furthermore, within each class, properties like orientation, skew and strokethickness tend to vary smoothly across the space. Not all points are shown: to produce this\\ndisplay, digits are chosen in random order and are only displayed if a \\b \\u0004 x \\b \\u0004 region of the\\ndisplay centered on the 2-D location of the digit in the embedding does not overlap any of\\nthe \\b \\u0004 x \\b \\u0004 regions for digits that have already been displayed.\\n(SNE was initialized by putting all the \\u0001\\u0003\\u0002 in random locations very close to the origin and then was\\ntrained using batch gradient descent (see Eq. 5) with annealed noise. The learning rate was 0.2. For\\nthe first 3500 iterations, each 2-D point was jittered by adding Gaussian noise with a standard deviation of \\u0004\\u0006\\u0005 \\u0007 after each position update. The jitter was then reduced to \\u0004 for a further \\b\\t\\u0004\\n\\u0004 iterations.)\\n\\n\\fTouretzky\\n\\nWiles\\n\\nMaass\\nKailath\\nChauvin Munro Shavlik\\nSanger\\nMovellan Baluja Lewicki Schmidhuber\\nHertz\\nBaldi Buhmann Pearlmutter Yang\\nTenenbaum\\nCottrell\\nKrogh\\nOmohundro Abu?Mostafa\\nSchraudolph\\nMacKay\\nCoolen\\nLippmann\\nRobinson Smyth\\nCohn\\nAhmad Tesauro\\nPentland\\nGoodman\\nAtkeson\\nNeuneier\\nWarmuth\\nSollich Moore\\nThrun\\nPomerleau\\n\\nBarber\\n\\nRuppin\\nHorn\\nMeilijson MeadLazzaro\\nKoch\\nObermayer Ruderman\\nEeckman HarrisMurray\\nBialek Cowan\\nBaird Andreou\\nMel\\nCauwenberghs\\nBrown Li\\nJabri\\nGiles Chen\\nSpence Principe\\nDoya Touretzky\\nSun\\nStork Alspector Mjolsness\\nBell\\nLee\\nMaass\\nLee\\nGold\\nPomerleau Kailath Meir\\nSeung Movellan\\nRangarajan\\nYang Amari\\nTenenbaum\\nCottrell Baldi\\nAbu?Mostafa\\nMacKay\\nNowlan Lippmann\\nSmyth Cohn Kowalczyk\\nWaibel\\nPouget\\nAtkeson\\nKawato\\nViola Bourlard Warmuth\\nDayan\\nSollich\\nMorgan Thrun MooreSutton\\nBarber Barto Singh\\nTishby WolpertOpper\\nSejnowski\\nWilliamson\\nKearns\\nSinger\\nMoody\\nShawe?Taylor\\nSaad\\nZemel\\nSaul\\nTresp\\nBartlett\\nPlatt\\nLeen\\nMozer\\nBishop Jaakkola\\nSolla\\nGhahramani\\nSmola\\nWilliams\\nVapnik\\nScholkopf\\nHinton\\nBengio\\nJordan\\nMuller\\nGraf\\nLeCun Simard\\nDenker\\nGuyon\\nBower\\n\\nFigure 2: Embedding of NIPS authors into two dimensions. Each of the 676 authors\\nwho published more than one paper in NIPS vols. 0-12 is show by a dot at the location 2 \\u0003 found by the SNE algorithm. Larger red dots and corresponding last names\\nare authors who published six or more papers in that period. The inset in upper left\\nshows a blowup of the crowded boxed central portion of the space. Dissimilarities between authors were computed based on squared Euclidean distance between vectors of\\nlog aggregate author word counts. Co-authored papers gave fractional counts evenly\\nto all authors. All words occurring in six or more documents were included, except\\nfor stopwords giving a vocabulary size of 13649. The NIPS text data is available at\\nhttp://www.cs.toronto.edu/ roweis/data.html.\\n\\n\\fin terms of \\u0003\\u0013\\u0012 \\u0005\\u0018\\u0017 , the probability that version \\u0011 of picks version \\u0001 of \\u0001 :\\n\\n\\u0003\\u0013\\u0012@\\u0005 \\u0017 \\u0007\\n\\n&% %\\n%&%\\n\\u0014 \\u0005 \\u0017 4\\t \\n\\n\\u001a \\f\\u001f\\u001d \\u000e$\\u0010 2 \\u0003 \\u0012 \\u0010 2 \\u0005 \\u0017 \\u0014 \\u001a \\u0017 \\u001d\\n\\u00199\\u001a \\u0019 \\u001b\\n\\t\\u000b\\n \\f \\u0011\\u000e \\u0010 %'% 2\\u000f\\u0003\\u0013\\u0012 \\u0010 2 '% % \\u0014 \\u0017\\n\\u0014\\n\\n(7)\\n\\nThe effect on 06\\u0003\\u0016\\u0005 of changing the mixing proportion for version \\u0002 of object \\u0003\\n\\nB\\nB \\u000b0 \\u0003\\u0006\\u0005 \\u0007\\t\\b \\u0004 \\u0003 8\\u0007\\u0016\\n\\u0014\\u0005\\u0004\\u0007\\u0006\\nwhere \\b \\u0004 \\u0003 \\u0007\\n\\n\\b if \\u0003\\n\\n\\u0007\\n\\n\\u0004 \\u0006$\\u0005\\u0018\\u0017 C 8 \\u0015\\n\\n\\u0014:\\u0003\\u0013\\u0012\\n\\n\\u0014\\u0005\\u0004\\u0007\\u0006\\n\\n\\n\\n\\n\\u0003\\u0013\\u0012 \\u0004 \\u0006\\n\\n\\b \\u0004 \\u0005 \\u0010 8\\u0007\\u0016\\n\\nis given by\\n\\n\\u0003\\u0013\\u0012@\\u0005\\u001a\\u0017\\f\\u000b\\n\\n(8)\\n\\nand \\u0001 otherwise. The effect of changing \\u0014 \\u0004 \\u0006 on the cost, C, is\\n\\nB 7\\n\\nB\\n\\u0002\\n\\u0003\\n\\u0006\\n\\u0005\\nB\\nB 0 \\u0003\\u0006\\u0005\\n\\u0007 \\u0010 8 8\\n\\u0014 \\u0004 \\u0006\\n\\u0003 \\u0005 0\\u000b\\u0003\\u0006\\u0005 \\u0014 \\u0004 \\u0006\\n\\n(9)\\n\\nRather than optimizing the mixing proportions directly, it is easier\\u0019\\t\\nto\\u000f perform unconstrained\\n\\t4\\n\\n\\f\\u001f\\u000e\\u000e\\n \\u0003\\u0011\\u0010 \\u0017 .\\noptimization on ?softmax weights? defined by \\u0014 \\u0003 \\u0012 \\u0007 \\t4\\n\\n\\f\\u001f\\u000e\\u000e\\n \\u0003 \\u0012 \\u0017 \\nAs a ?proof-of-concept?, we recently implemented a simplified mixture version in which\\nevery object is represented in the low-dimensional\\nspace by exactly two components that\\n\\u0005\\nare constrained to have mixing proportions of \\u0001\\u0013\\u0012 . The two components are pulled together\\nby a force which increases linearly up to a threshold separation. Beyond this threshold\\nthe force remains constant.1 We ran two experiments with this simplified mixture version\\nof SNE. We took a dataset containing \\u0002\\u0001\\u0003\\u0001 pictures of each of the digits 2,3,4 and added\\n\\b\\n\\u0001\\u0002\\u0001 hybrid digit-pictures that were each constructed by picking new examples of two of\\nthe classes and taking each pixel at random from one of these two ?parents?. After mini\\u0004 \\u0014 of the hybrids and only \\b\\n\\u0016\\n\\t \\u0014 of the non-hybrids had significantly different\\nmization, \\u0004\\u0003\\u0015\\nlocations for their two mixture components. Moreover, the mixture components of each\\nhybrid always lay in the regions of the space devoted to the classes of its two parents and\\nnever in the region devoted to the third class. For this example we used a perplexity of \\b \\u0001\\nin defining the local neighborhoods, a step\\u0005 size of for each position update of \\u0013\\n\\u0001 \\u0012 \\u0006 times the\\ngradient, and used a constant jitter of \\u0013\\n\\u0001 \\u0012 \\u0001 . Our very simple mixture version of SNE also\\nmakes it possible to map a circle onto a line without losing any near neighbor relationships\\nor introducing any new ones. Points near one ?cut point? on the circle can mapped to a\\nmixture of two points, one near one end of the line and one near the other end. Obviously,\\nthe location of the cut on the two-dimensional circle gets decided by which pairs of mixture\\ncomponents split first during the stochastic optimization. For certain optimization parameters that control the ease with which two mixture components can be pulled apart, only\\na single cut in the circle is made. For other parameter settings, however, the circle may\\nfragment into two or more smaller line-segments, each of which is topologically correct\\nbut which may not be linked to each other.\\nThe example with hybrid digits demonstrates that even the most primitive mixture version\\nof SNE can deal with ambiguous high-dimensional objects that need to be mapped to two\\nwidely separated regions of the low-dimensional space. More work needs to be done before\\nSNE is efficient enough to cope with large matrices of document-word counts, but it is\\nthe only dimensionality reduction method we know of that promises to treat homonyms\\nsensibly without going back to the original documents to disambiguate each occurrence of\\nthe homonym.\\n1\\nWe used a threshold of \\u0004 \\u0005 \\u0004 \\b . At threshold the force was \\u0004 \\u0005 \\u0004\\u0018\\u0017\\n\\b nats per unit length. The low-d\\nspace has a natural scale because the variance of the Gaussian used to determine \\u0019 \\u0002 \\u001a is fixed at 0.5.\\n\\n\\f5 Practical optimization strategies\\nOur current method of reducing the SNE cost is to use steepest descent with added jitter\\nthat is slowly reduced. This produces quite good embeddings, which demonstrates that the\\nSNE cost function is worth minimizing, but it takes several hours to find a good embedding\\nfor just \\u0002\\u0001\\u0003\\u0001\\u0002\\u0001 datapoints so we clearly need a better search algorithm.\\nThe time per iteration could be reduced considerably by ignoring pairs of points for which\\nall four of \\u0002 \\u0003\\u0006\\u0005 # \\u0002 \\u0005G\\u0003 #G0 \\u0003\\u0016\\u0005 #G0 \\u0005G\\u0003 are small. Since the matrix \\u0002 \\u0003\\u0006\\u0005 is fixed during the learning, it is\\nnatural to sparsify it by replacing all entries below a certain threshold with zero and renormalizing. Then pairs H# \\u0001 for which both \\u00025\\u0003\\u0016\\u0005 and \\u0002F\\u0005G\\u0003 are zero can be ignored from gradient\\ncalculations if both 0 \\u0003\\u0006\\u0005 and 0 \\u0005G\\u0003 are small. This can in turn be determined in logarithmic\\ntime in the size of the training set by using sophisticated geometric data structures such\\nas K-D trees, ball-trees and AD-trees, since the 0\\u0018\\u0003\\u0006\\u0005 depend only on 42\\u001f\\u0003 \\u0010 2\\u0004\\u0005\\u0001 \\u0014 . Computational physics has attacked exactly this same complexity when performing multibody\\ngravitational or electrostatic simulations using, for example, the fast multipole method.\\nIn the mixture version of SNE there appears to be an interesting way of avoiding local\\noptima that does not involve annealing the jitter. Consider two components in the mixture\\nfor an object that are far apart in the low-dimensional space. By raising the mixing proportion of one and lowering the mixing proportion of the other, we can move probability mass\\nfrom one part of the space to another without it ever appearing at intermediate locations.\\nThis type of ?probability wormhole? seems like a good way to avoid local optima that arise\\nbecause a cluster of low-dimensional points must move through a bad region of the space\\nin order to reach a better one.\\nYet another search method, which we have used with some success on toy problems, is\\nto provide extra dimensions in the low-dimensional space but to penalize non-zero values\\non these dimensions. During the search, SNE will use the extra dimensions to go around\\nlower-dimensional barriers but as the penalty on using these dimensions is increased, they\\nwill cease to be used, effectively constraining the embedding to the original dimensionality.\\n\\n6 Discussion and Conclusions\\nPreliminary\\nexperiments show that we can find good optima by first annealing the perplex)\\nities \\u0003 \\u0014 (using high jitter) and only reducing the jitter after the final perplexity\\nhas been\\n)\\nreached. This raises the question of what SNE is doing when the variance, \\u0003 \\u0014 , of the Gaussian centered on each high-dimensional point is very big so that the distribution across\\nneighbors is almost uniform. It is clear that in the high variance limit, the contribution of\\n\\u0002:\\u0003\\u0006\\u0005\\u000f*&+ , \\u000e \\u0002:\\u0003\\u0006\\u0005 \\n 06\\u0003\\u0016\\u0005 \\u0017 to) the SNE cost function is just as important for distant neighbors as for\\nclose ones. When \\u0003 \\u0014 is very large, it can be shown that SNE is equivalent to minimizing the\\nmismatch between squared distances in the two spaces, provided all the squared distances\\nfrom an object are first normalized by subtracting off their ?antigeometric? mean, \\u0002 \\u0003 \\u0014 :\\n\\n\\u0011\\n\\n\\u000e@\\u0012 \\u0014\\u0003\\u0006\\u0005 \\u0010 \\u0002 \\u0003 \\u0014 \\u0017 \\u0010 \\u0013\\n\\u000e \\u0012 \\u0012 \\u0014\\u0006\\u0003 \\u0005 \\u0010 \\u0002 \\u0012 \\u0003 \\u0014 \\u0017\\u0015\\u0014 \\u0014\\n\\u0006\\u0003 \\u0005\\n\\t4\\n\\n\\f\\u001f\\u000e\\u0011\\u0010\\u0013\\u0012 \\u0014 \\u001a \\u0017\\n\\u0003 #\\n\\u0014\\u0002 \\u0007 \\u0010 *&+ , 8\\n\\u0012 \\u0014\\u0003\\u0016\\u0005 \\u0007 %&% !\\u000f\\u0003 \\u0010 !\\u0004\\u0005 %&% \\u0014 \\n ) \\u0014 #\\n\\u0003\\n\\u001a\\u001d\\u001c\\n\\u0016 \\u0010 \\b\\n\\u001e \\u0003\\n\\t4\\n\\n\\f\\u001f\\u000e$\\u0010 \\u0012 \\u0012 \\u0014 \\u001a \\u0017\\n\\u0003\\n\\u0012 \\u0012 \\u0014\\u0003\\u0006\\u0005 \\u0007 '% % 2 \\u0003 \\u0010 2 \\u0005 '% % \\u0014 \\n ) \\u0014 #\\n\\u0002 \\u0012 \\u0003 \\u0014 \\u0007 \\u0010 *'+\\u001d, 8 \\u001a\\u001d\\u001c\\n\\u0016 \\u0010 \\b\\n\\u001e \\u0003\\n\\u0002\\u0004\\u0003\\u0006\\u0005\\b\\u0007\\n\\t\\f\\u000b\\u000e\\n\\u0010\\u000f\\n\\nwhere \\u0016 is the number of objects.\\n\\n\\u0007;8\\n\\n(10)\\n(11)\\n\\n(12)\\n\\n\\fThis mismatch is very similar to ?stress? functions used in nonmetric versions of MDS,\\nand enables us to understand the large-variance limit of SNE as a particular variant of such\\nprocedures. We are still investigating the relationship to metric MDS and to PCA.\\nSNE can also be seen as an interesting special case of Linear Relational Embedding (LRE)\\n[11]. In LRE the data consists of triples (e.g. Colin has-mother Victoria) and the task\\nis to predict the third term from the other two. LRE learns an N-dimensional vector for\\neach object and an NxN-dimensional matrix for each relation. To predict the third term in\\na triple, LRE multiplies the vector representing the first term by the matrix representing\\nthe relationship and uses the resulting vector as the mean of a Gaussian. Its predictive\\ndistribution for the third term is then determined by the relative densities of all known\\nobjects under this Gaussian. SNE is just a degenerate version of LRE in which the only\\nrelationship is ?near? and the matrix representing this relationship is the identity.\\nIn summary, we have presented a new criterion, Stochastic Neighbor Embedding, for mapping high-dimensional points into a low-dimensional space based on stochastic selection\\nof similar neighbors. Unlike self-organizing maps, in which the low-dimensional coordinates are fixed to a grid and the high-dimensional ends are free to move, in SNE the\\nhigh-dimensional coordinates are fixed to the data and the low-dimensional points move.\\nOur method can also be applied to arbitrary pairwise dissimilarities between objects if such\\nare available instead of (or in addition to) high-dimensional observations. The gradient of\\nthe SNE cost function has an appealing ?push-pull? property in which the forces acting on\\n2\\u000f\\u0003 to bring it closer to points it is under-selecting and further from points it is over-selecting\\nas its neighbor. We have shown results of applying this algorithm to image and document\\ncollections for which it sensibly placed similar objects nearby in a low-dimensional space\\nwhile keeping dissimilar objects well separated.\\nMost importantly, because of its probabilistic formulation, SNE has the ability to be extended to mixtures in which ambiguous high-dimensional objects (such as the word ?bank?)\\ncan have several widely-separated images in the low-dimensional space.\\nAcknowledgments We thank the anonymous referees and several visitors to our poster for helpful\\nsuggestions. Yann LeCun provided digit and NIPS text data. This research was funded by NSERC.\\n\\nReferences\\n[1] T. Cox and M. Cox. Multidimensional Scaling. Chapman & Hall, London, 1994.\\n[2] J. Tenenbaum. Mapping a manifold of perceptual observations. In Advances in Neural Information Processing Systems, volume 10, pages 682?688. MIT Press, 1998.\\n[3] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear\\ndimensionality reduction. Science, 290:2319?2323, 2000.\\n[4] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding.\\nScience, 290:2323?2326, 2000.\\n[5] T. Kohonen. Self-organization and Associative Memory. Springer-Verlag, Berlin, 1988.\\n[6] C. Bishop, M. Svensen, and C. Williams. GTM: The generative topographic mapping. Neural\\nComputation, 10:215, 1998.\\n[7] J. J. Hull. A database for handwritten text recognition research. IEEE Transaction on Pattern\\nAnalysis and Machine Intelligence, 16(5):550?554, May 1994.\\n[8] I. T. Jolliffe. Principal Component Analysis. Springer-Verlag, New York, 1986.\\n[9] Yann LeCun. Nips online web site. http://nips.djvuzone.org, 2001.\\n[10] Andrew Kachites McCallum. Bow: A toolkit for statistical language modeling, text retrieval,\\nclassification and clustering. http://www.cs.cmu.edu/ mccallum/bow, 1996.\\n[11] A. Paccanaro and G.E. Hinton. Learning distributed representations of concepts from relational\\ndata using linear relational embedding. IEEE Transactions on Knowledge and Data Engineering, 13:232?245, 2000.\\n\\n\\f\",\n          \"Sufficient Conditions for Agnostic Active Learnable\\n\\nLiwei Wang\\nKey Laboratory of Machine Perception, MOE,\\nSchool of Electronics Engineering and Computer Science,\\nPeking University,\\nwanglw@cis.pku.edu.cn\\n\\nAbstract\\nWe study pool-based active learning in the presence of noise, i.e. the agnostic setting. Previous works have shown that the effectiveness of agnostic active learning\\ndepends on the learning problem and the hypothesis space. Although there are\\nmany cases on which active learning is very useful, it is also easy to construct\\nexamples that no active learning algorithm can have advantage. In this paper, we\\npropose intuitively reasonable sufficient conditions under which agnostic active\\nlearning algorithm is strictly superior to passive supervised learning. We show\\nthat under some noise condition, if the Bayesian classification boundary and the\\nunderlying distribution are smooth to a finite order, active learning achieves polynomial improvement in the label complexity; if the boundary and the distribution\\nare infinitely smooth, the improvement is exponential.\\n\\n1 Introduction\\nActive learning addresses the problem that the algorithm is given a pool of unlabeled data drawn\\ni.i.d. from some underlying distribution. The algorithm can then pay for the label of any example\\nin the pool. The goal is to learn an accurate classifier by requesting as few labels as possible. This\\nis in contrast with the standard passive supervised learning, where the labeled examples are chosen\\nrandomly.\\nThe simplest example that demonstrates the potential of active learning is to learn the optimal threshold on an interval. If there exists a perfect threshold separating the two classes (i.e. there is no noise),\\nthen binary search only needs O(ln 1? ) labels to learn an ?-accurate classifier, while passive learning requires O( 1? ) labels. Another encouraging example is to learn a homogeneous linear separator\\nfor data uniformly distributed on the unit sphere of Rd . In this case active learning can still give\\nexponential savings in the label complexity [Das05].\\nHowever, there are also very simple problems that active learning does not help at all. Suppose the\\ninstances are uniformly distributed on [0, 1], and the positive class could be any interval on [0, 1].\\nAny active learning algorithms needs O( 1? ) label requests to learn an ?-accurate classifier [Han07].\\nThere is no improvement over passive learning. All above are noise-free (realizable) problems. Of\\nmore interest and more realistic is the agnostic setting, where the class labels can be noisy so that\\nthe best classifier in the hypothesis space has a non-zero error ?. For agnostic active learning, there\\n2\\nis no active learning algorithm that can always reduce label requests due to a lower bound ?( ??2 ) for\\nthe label complexity [Kaa06].\\nIt is known that whether active learning helps or not depends on the distribution of the instance-label\\npairs and the hypothesis space. Thus a natural question would be that under what conditions is active\\nlearning guaranteed to require fewer labels than passive learning.\\n1\\n\\n\\fIn this paper we propose intuitively reasonable sufficient conditions under which active learning\\nachieves lower label complexity than that of passive learning. Specifically, we focus on the A2 algorithm [BAL06] which works in the agnostic setting. Earlier work has discovered that the label\\ncomplexity of A2 can be upper bounded by a parameter of the hypothesis space and the data distribution called disagreement coefficient [Han07]. This parameter often characterizes the intrinsic\\ndifficulty of the learning problem. By an analysis of the disagreement coefficient we show that, under some noise condition, if the Bayesian classification boundary and the underlying distribution are\\nsmooth to a finite order, then A2 gives polynomial savings in the label complexity; if the boundary\\nand the distribution are infinitely smooth, A2 gives exponential savings.\\n1.1 Related Works\\nOur work is closely related to [CN07], in which the authors proved sample complexity bounds for\\nproblems with smooth classification boundary under Tsybakov?s noise condition [Tsy04]. They also\\nassumed that the distribution of the instances is bounded from above and below. The main difference\\nto our work is that their analysis is for the membership-query setting [Ang88], in which the learning\\nalgorithm can choose any point in the instance space and ask for its label; while the pool-based\\nmodel analyzed here assumes the algorithm can only request labels of the instances it observes.\\nAnother related work is due to Friedman [Fri09]. He introduced a different notion of smoothness\\nand showed that this guarantees exponential improvement for active learning. But his work focused\\non the realizable case and does not apply to the agnostic setting studied here.\\nSoon after A2 , Dasgupta, Hsu and Monteleoni [DHM07] proposed an elegant agnostic active learning algorithm. It reduces active learning to a series of supervised learning problems. If the hypothesis space has a finite VC dimension, it has a better label complexity than A2 . However, this\\nalgorithm relies on the normalized uniform convergence bound for the VC class. It is not known\\nwhether it holds for more general hypothesis space such as the smooth boundary class analyzed in\\nthis paper. (For recent advances on this topic, see [GKW03].) It is left as an open problem whether\\nour results apply to this algorithm by refined analysis of the normalized bounds.\\n\\n2\\n\\nPreliminaries\\n\\nLet X be an instance space, D a distribution over X ? {?1, 1}. Let H be the hypothesis space, a\\nset of classifiers from X to {?1}. Denote DX the marginal of D over X . In our active learning\\nmodel, the algorithm has access to a pool of unlabeled examples from DX . For any unlabeled point\\nx, the algorithm can ask for its label y, which is generated from the conditional distribution at x.\\nThe error of a hypothesis h according to D is erD (h) = Pr(x,y)?D (h(x) 6= y). The empirical error\\nP\\n1\\non a finite sample S is erS (h) = |S|\\n(x,y)?S I[h(x) 6= y], where I is the indicator function. We\\nuse h? denote the best classifier in H. That is, h? = arg minh?H erD (h). Let ? = erD (h? ). Our\\n? ? H with error rate at most ? + ?, where ? is a predefined parameter.\\ngoal is to learn a h\\nA2 is the first rigorous agnostic active learning algorithm. A description of the algorithm is given\\nin Fig.1. It was shown that A2 is never much worse than passive learning in terms of the label\\ncomplexity. The key observation that A2 can be superior to passive learning is that, since our goal is\\n? such that erD (h)\\n? ? erD (h? ) + ?, we only need to compare the errors of hypotheses.\\nto choose an h\\nTherefore we can just request labels of those x on which the hypotheses under consideration have\\ndisagreement.\\nTo do this, the algorithm keeps track of two spaces. One is the current version space Vi , consisting\\nof hypotheses that with statistical confidence are not too bad compared to h? . To achieve such a\\nstatistical guarantee, the algorithm must be provided with a uniform convergence bound over the\\nhypothesis space. That is, with probability at least 1 ? ? over the draw of sample S according to D,\\nLB(S, h, ?) ? erD (h) ? U B(S, h, ?),\\nhold simultaneously for all h ? H, where the lower bound LB(S, h, ?) and upper bound\\nU B(S, h, ?) can be computed from the empirical error erS (h). The other space is the region of\\ndisagreement DIS(Vi ), which is the set of all x ? X for which there are hypotheses in Vi that\\ndisagree on x. Formally, for any V ? H,\\nDIS(V ) = {x ? X : ?h, h0 ? V, h(x) 6= h0 (x)}.\\n2\\n\\n\\fInput: concept space H, accuracy parameter ? ? (0, 1), confidence parameter ? ? (0, 1);\\n? ? H;\\nOutput: classifier h\\n?\\nLet n\\n? = 2(2 log2 ? + ln 1? ) log2 2? (? depends on H and the problem, see Theorem 5) ;\\nLet ? 0 = ?/?\\nn;\\nV0 ? H, S0 ? ?, i ?0, j1 ?0, k ?1 ;\\nwhile ?(Vi )(minh?Vi U B(Si , h, ? 0 ) ? minh?Vi LB(Si , h, ? 0 )) > ? do\\nVi+1 ? {h ? Vi : LB(Si , h, ? 0 ) ? minh0 ?Vi U B(Si , h0 , ? 0 )};\\ni ?i+1;\\nif ?(Vi ) < 12 ?(Vjk ) then\\nk ? k + 1; jk ? i;\\nend\\nSi0 ? Rejection sample 2i?jk samples x from D satisfying x ? DIS(Vi );\\nSi ? {(x, y = label(x)) : x ? Si0 };\\nend\\n? argminh?V U B(Si , h, ? 0 ).\\nReturn h=\\ni\\nAlgorithm 1: The A2 algorithm (this is the version in [Han07])\\n\\nThe volume of DIS(V ) is denoted by ?(V ) = PrX?DX (X ? DIS(V )). Requesting labels of\\nthe instances from DIS(Vi ) allows A2 require fewer labels than passive learning. Hence the key\\nissue is how fast ?(Vi ) reduces. This process, and in turn the label complexity of A2 , are nicely\\ncharacterized by the disagreement coefficient ? introduced in [Han07].\\nDefinition 1 Let ?(?, ?) be the pseudo-metric on a hypothesis space H induced by DX . That is, for\\nh, h0 ? H, ?(h, h0 ) = PrX?DX (h(X) 6= h0 (X)). Let B(h, r) = {h0 ? H: ?(h, h0 ) ? r}. The\\ndisagreement coefficient ?(?) is\\nPrX?DX (X ? DIS(B(h? , r)))\\n,\\nr\\nr??\\n\\n?(?) = sup\\n\\n(1)\\n\\nwhere h? = arg minh?H erD (h).\\nNote that ? depends on H and D, and 1 ? ?(?) ? 1? .\\n\\n3 Main Results\\nAs mentioned earlier, whether active learning helps or not depends on the distribution and the hypothesis space. There are simple examples such as learning intervals for which active learning has\\nno advantage. However, these negative examples are more or less ?artificial?. It is important to\\nunderstand whether problems with practical interest are actively learnable or not. In this section we\\nprovide intuitively reasonable conditions under which the A2 algorithm is strictly superior to passive\\nlearning. Our main results (Theorem 11 and Theorem 12) show that if the learning problem has a\\nsmooth Bayes classification boundary, and the distribution DX has a density bounded by a smooth\\nfunction, then under some noise condition A2 saves label requests. It is a polynomial improvement\\nfor finite smoothness, and exponential for infinite smoothness.\\nIn Section 3.1 we formally define the smoothness and introduce the hypothesis space, which contains\\nsmooth classifiers. We show a uniform convergence bound of order O(n?1/2 ) for this hypothesis\\nspace. This bound determines U B(S, h, ?) and LB(S, h, ?) in A2 . Section 3.2 is the main technical\\npart, where we give upper bounds for the disagreement coefficient of smooth problems. In Section\\n3.3 we show that under some noise condition, there is a sharper bound for the label complexity in\\nterms of the disagreement coefficient. These lead to our main results.\\n3\\n\\n\\f3.1\\n\\nSmoothness\\n\\nLet f be a function defined on ? ? Rd . For any vector k = (k1 , ? ? ? , kd ) of d nonnegative integers,\\nPd\\nlet |k| = i=1 ki . Define the K-norm as\\nkf kK :=\\n\\nDk f (x) ? Dk f (x0 )\\n,\\nkx ? x0 k\\n|k|=K?1x,x0 ??\\n\\nmax sup|Dk f (x)| + max\\n\\n|k|?K?1x??\\n\\nwhere\\nDk =\\n\\n? k1 x1\\n\\nsup\\n\\n(2)\\n\\n? |k|\\n,\\n? ? ? ? kd xd\\n\\nis the differential operator.\\nDefinition 2 (Finite Smooth Functions) A function f is said to be Kth order smooth with respect\\nto a constant C, if kf kK ? C. The set of Kth order smooth functions is defined as\\nFCK := {f : kf kK ? C}.\\n\\n(3)\\n\\nThus Kth order smooth functions have uniformly bounded partial derivatives up to order K ? 1, and\\nthe K ? 1th order partial derivatives are Lipschitz.\\nDefinition 3 (Infinitely Smooth Functions) A function f is said to be infinitely smooth with respect\\nto a constant C, if kf kK ? C for all nonnegative integers K. The set of infinitely smooth functions\\nis denoted by FC? .\\nWith the definitions of smoothness, we introduce the hypothesis space we use in the A2 algorithm.\\nK\\nDefinition 4 (Hypotheses with Smooth Boundaries) A set of hypotheses HC\\ndefined on [0, 1]d+1\\nK\\nis said to have Kth order smooth boundaries, if for every h ? HC , the classification boundary is\\na Kth order smooth function on [0, 1]d . To be precise, let x = (x1 , x2 , . . . , xd+1 ) ? [0, 1]d+1 . The\\nclassification boundary is the graph of function xd+1 = f (x1 , . . . , xd ), where f ? FCK . Similarly,\\n?\\n?\\nthe\\nis said to have infinitely smooth boundaries, if for every h ? HC\\na hypothesis space HC\\nd\\nclassification boundary is the graph an infinitely smooth function on [0, 1] .\\n\\nPrevious results on the label complexity of A2 assumes the hypothesis space has finite VC dimension. The goal is to ensure a O(n?1/2 ) uniform convergence bound so that U B(S, h, ?) ?\\nK\\n?\\nLB(S, h, ?) = O(n?1/2 ). The hypothesis space HC\\nand HC\\ndo not have finite VC dimensions.\\nK\\n?\\nCompared with the VC class, HC and HC are exponentially larger in terms of the covering num?\\nK\\nunder a broad class of\\nand HC\\nbers [vdVW96]. But uniform convergence bound still holds for HC\\ndistributions. The following theorem is a consequence of some known results in empirical processes.\\nTheorem 5 For any distribution D over [0, 1]d+1 ? {?1, 1}, whose marginal distribution DX on\\n[0, 1]d+1 has a density upper bounded by a constant M , and any 0 < ? ? ?0 (?0 is a constant), with\\nprobability at least 1 ? ? over the draw of the training set S of n examples,\\ns\\nlog 1?\\n|erD (h) ? erS (h)| ? ?\\n,\\n(4)\\nn\\nholds simultaneously for all h ? HK\\nC provided K > d (or K = ?). Here ? is a constant depending\\nonly on d, K, C and M .\\nK\\nProof It can be seen, from Corollary 2.7.3 in [vdVW96] that the bracketing numbers N[ ] of HC\\n2d\\n1\\nK\\nsatisfies log N[ ] (?, HC\\n, L2 (DX )) = O(( ? ) K ). Since K > d, then there exist constants c1 , c2 such\\nthat\\n?\\n!\\n?\\n?\\nnt2\\nPD sup |er(h) ? erS (h)| ? t ? c1 exp ?\\nc2\\nh?HK\\nC\\n\\nfor all nt2 ?\\nt0 is some constant (see Theorem 5.11 and Lemma 5.10 of [vdG00]). Let\\n? t0 , 2where\\n?\\nnt\\n? = c1 exp ? c2 , the theorem follows.\\n4\\n\\n\\fNow we can\\nU B(S, h, ?) and LB(S,q\\nh, ?) for A2 by simply letting U B(S, h, ?) =\\nq determine\\nln ?1\\nln ?1\\nerS (h) + ?\\nn and LB(S, h, ?) = erS (h) ? ?\\nn , where S is of size n.\\n3.2\\n\\nDisagreement Coefficient\\n\\nThe disagreement coefficient ? plays an important role for the label complexity of active learning\\nalgorithms. In fact previous negative examples for which active learning does not work are all the\\nresults of large ?. For instance the interval learning problem, ?(?) = 1? , which leads to the same\\nlabel complexity as passive learning. In the following two theorems we show that the disagreement\\ncoefficient ?(?) for smooth problems is small.\\n1\\nd+1\\nTheorem 6 Let the hypothesis space be HK\\n)\\nC . If the distribution DX has a density p(x , . . . , x\\n1\\nd+1\\nsuch that there exists a Kth order smooth function g(x , . . . , x ) and two constants 0 < ? ?\\n? such that ?g(x1 , . . . , x?d+1 ) ? ?\\np(x1 , . . . , xd+1 ) ? ?g(x1 , . . . , xd+1 ) for all (x1 , . . . , xd+1 ) ?\\nd\\n?\\n?\\n[0, 1]d+1 , then ?(?) = O 1? K+d .\\n1\\nd+1\\nTheorem 7 Let the hypothesis space be H?\\n)\\nC . If the distribution DX has a density p(x , . . . , x\\n1\\nd\\nsuch that there exist an infinitely smooth function g(x , . . . , x ) and two constants 0 < ? ? ? such\\nthat ?g(x1 , . . . , xd ) ? p(x1 , . . . , xd+1 ) ? ?g(x1 , . . . , xd ) for all (x1 , . . . , xd+1 ) ? [0, 1]d+1 , then\\n?(?) = O(logd ( 1? )).\\n\\nThe key points in the theorems are: the classification boundaries are smooth; and the density is\\nbounded from above and below by constants times a smooth function. These two conditions include\\na large class of learning problems. Note that the density itself is not necessarily smooth. We just\\nrequire the density does not change too rapidly.\\nThe intuition behind the two theorems above is as follows. Let fh? (x) and fh (x) be the classification\\nboundaries of h? and h, and suppose ?(h, h? ) is small, where ?(h, h? ) = Prx?DX (h(x) 6= h? (x))\\nis the pseudo metric. If the classification boundaries and the density are all smooth, then the two\\nboundaries have to be close to each other everywhere. That is, |fh (x) ? ff ? (x)| is small uniformly\\nfor all x. Hence only the points close to the classification boundary of h? can be in DIS(B(h? , ?)),\\nwhich leads to a small disagreement coefficient.\\nThe proofs of Theorem 6 and Theorem 7 rely on the following two lemmas.\\nR\\nLemma 8 Let ? be a function defined on [0, 1]d and [0,1]d |?(x)|dx ? r. If there exists a Kth\\n? and 0 < ? ? ? such that ?|?(x)|\\n?\\n?\\norder smooth function ?\\n? |?(x)| ? ?|?(x)|\\nfor all x ? [0, 1]d ,\\nK\\n\\nd\\n\\nthen k?k? = O(r K+d ) = O(r ? ( 1r ) K+d ), where k?k? = supx?[0,1]d |?(x)|.\\nR\\nLemma 9 Let ? be a function defined on [0, 1]d and [0,1]d |?(x)|dx ? r. If there exists an infinitely\\n? and 0 < ? ? ? such that ?|?(x)|\\n?\\n?\\nsmooth function ?\\n? |?(x)| ? ?|?(x)|\\nfor all x ? [0, 1]d , then\\nk?k? = O(r ? logd ( 1r ))\\nWe will briefly describe the ideas of the proofs of these two lemmas in the Appendix. The formal\\nproofs are given in the supplementary file.\\nProof of Theorem 6 First of all, since we focus on binary classification, DIS(B(h? , r)) can be\\nwritten equivalently as\\nDIS(B(h? , r)) = {x ? X , ?h ? B(h? , r), s.t. h(x) 6= h? (x)}.\\nConsider any h ? B(h? , r). Let fh , fh? ? FCK be the corresponding classification boundaries of h\\nand h? respectively. If r is sufficiently small, we must have\\n?\\n?Z\\nZ\\n?\\n? fh (x1 ,...,xd )\\n?\\n?\\n?(h, h? ) = Pr (h(X) 6= h? (X)) =\\np(x1 , . . . , xd+1 )dxd+1 ? .\\ndx1 . . . dxd ?\\nX?DX\\n?\\n? fh? (x1 ,...,xd )\\n[0,1]d\\n\\n5\\n\\n\\fDenote\\n\\nZ\\n?h (x1 , . . . , xd ) =\\n\\nfh (x1 ,...,xd )\\n\\np(x1 , . . . , xd+1 )dxd+1 .\\n\\nfh? (x1 ,...,xd )\\n\\n? h (x1 , . . . , xd ) and two constants 0 < u ? v\\nWe assert that there is a Kth order smooth function ?\\n? h | ? |?h | ? v|?\\n? h |. To see this, remember that fh and fh? are Kth order smooth\\nsuch that u|?\\nfunctions; and the density p is upper and lower bounded by constants times a Kth order smooth\\n1\\nd\\nR\\n? h (x1 , . . . , xd ) = fh (x 1,...,x d) g(x1 , . . . , xd+1 )dxd+1 is\\nfunction g(x1 , . . . , xd+1 ); and note that ?\\nfh? (x ,...,x )\\n\\na Kth order smooth function. The latter is easy to check by taking derivatives. By Lemma 8, we have\\nR\\nd\\nk?h k? = O(r ? ( 1r ) K+d ), since |?h | = ?(h, h? ) ? r. Because this holds for all h ? B(h? , r),\\nd\\nwe have suph?B(h? ,r) k?h k? = O(r ? ( 1r ) K+d ).\\nNow consider the region of disagreement of B(h? , r). Clearly DIS(B(h? , r)) = ?h?B(h? ,r) {x :\\nh(x) 6= h? (x)}. Hence\\n?\\n?\\nPr (x ? DIS(B(h? , r))) = Pr x ? ?h?B(h? ,r) {x : h(x) 6= h? (x)}\\nX?DX\\nX?DX\\n? ? ? d !\\nZ\\n1 K+d\\n1\\nd\\n.\\n? 2 sup\\nk?h k? dx . . . dx = O r ?\\nr\\nh?B(h? ,r) [0,1]d\\nThe theorem follows by the definition of ?(?).\\nTheorem 7 can be proved similarly by using Lemma 9.\\n3.3\\n\\nLabel Complexity\\n\\nIt was shown in [Han07] that the label complexity of A2 is\\n?\\n? ?\\n?\\n? ? 2\\n1\\n1\\n?\\n2\\n+ 1 polylog\\nln\\n,\\nO ?\\n?2\\n?\\n?\\n\\n(5)\\n\\nwhere ? = minh?H erD (h). When ? ? ?, our previous results on the disagreement coefficient\\nalready imply polynomial or exponential improvements for A2 . However, when ? < ?, the label\\ncomplexity becomes O( ?12 ), the same as passive learning whatever ? is. In fact, without any as2\\nsumption on the noise, the O( ?12 ) result is inevitable due to the ?( ??2 ) lower bound of agnostic\\nactive learning [Kaa06].\\nRecently, there has been considerable interest in how noise affects the learning rate. A remarkable\\nnotion is due to Tsybakov [Tsy04], which was first introduced for passive learning. Let ?(x) =\\nP (Y = 1|X = x). Tsybakov?s noise condition assumes that for some c > 0, 0 < ? ? ?\\nPr (|?(X) ? 1/2| ? t) ? ct?? ,\\n\\n(6)\\n\\nX?DX\\n\\nfor all 0 < t ? t0 , where t0 is some constant. (6) implies a connection between the pseudo distance\\n?(h, h? ) and the excess risk erD (h) ? erD (h? ):\\n1/?\\n\\n?(h, h? ) ? c0 (erD (h) ? erD (h? ))\\n?\\n\\n,\\n\\n(7)\\n1+?\\n?\\n\\n0\\n\\nwhere h is the Bayes classifier, c is some finite constant. Here ? =\\n? 1 is called the noise\\nexponent. ? = 1 is the optimal case, where the problem has bounded noise; ? > 1 correspond to\\nunbounded noise.\\nCastro and Nowak [CN07] noticed that Tsybakov?s noise condition is also important in active learning. They proved label complexity bounds in terms of ? for the membership-query setting. A notable\\n? 1 ) 2??2\\n?\\nfact is that O((\\n) (? > 1) is both an upper and a lower bound for membership-query in the\\n?\\nminimax sense. It is important to point out that the lower bound automatically applies to pool-based\\nmodel, since pool makes weaker assumptions than membership-query. Hence for large ?, active\\nlearning has very limited improvement over passive learning whatever other factors are.\\nRecently, Hanneke [Han09] obtained similar label complexity for pool-based model. He showed the\\nlabels requested by A2 is O(?2 ln 1? ln 1? ) for the bounded noise case, i.e. ? = 1. Here we slightly\\n6\\n\\n\\fgeneralize Hanneke?s result to unbounded noise by introducing the following noise condition. We\\nassume there exist c1 , c2 > 0 and T0 > 0 such that\\nPr (|?(X) ? 1/2| ?\\n\\nX?DX\\n\\n1\\n) ? c1 e?c2 T ,\\nT\\n\\nfor all T ? T0 . It is not difficult to show that (8) implies\\n?\\n?\\n?(h, h ) = O (er(h) ? er(h? )) ln\\n\\n1\\n(er(h) ? er(h? ))\\n\\n(8)\\n?\\n.\\n\\n(9)\\n\\nThis condition assumes unbounded noise. Under this noise condition, A2 has a better label complexity.\\nTheorem 10 Assume that the learning problem satisfies the noise condition (8) and DX has a density upper bounded by a constant M . For any hypothesis space H that has a O(n?1/2 ) uniform\\nconvergence bound, if the Bayes classifier h? is in H, then with probability at least 1 ? ?, A2 outputs\\n? ? H with erD (h)\\n? ? erD (h? ) + ?, and the number of labels requested by the algorithm is at most\\nh\\n1\\n2\\nO(? (?) ? ln ? ? polylog( 1? )).\\nProof As the proof of [Han07], one can show that with probability 1 ? ? we never remove h? from\\nVi , and for any h, h0 ? Vi we must have ?(Vi )(eri (h) ? eri (h0 )) = erD (h) ? erD (h0 ), where\\n? ? erD (h? ) + ?.\\neri (h) is the error rate of h conditioned on DIS(Vi ). These guarantees erD (h)\\nIf ?(Vi ) ? 2??(?), due to the O(n?1/2 ) uniform convergence bound, O(?2 (?) ln 1? ) labels suffices\\nto make ?(Vi )(U B(Si , h, ? 0 ) ? LB(Si , h, ? 0 )) ? ? for all h ? DIS(Vi ) and the algorithm stops.\\nHence we next consider ?(Vi ) > 2??(?). Note that there are at most O(ln 1? ) times ?(Vi ) <\\n1\\n1\\n2 ?(Vjk ) occurs. So below we bound the number of labels needed to make ?(Vi ) < 2 ?(Vjk )\\n?(V\\n\\n)\\n\\njk\\noccurs. By the definition of ?(?), if ?(h, h? ) ? 2?(?)\\nfor all h ? Vi , then ?(Vi ) < 21 ?(Vjk ). Let\\n?(h) = erD (h) ? erD (h? ). By the noise assumption (9) we have that if\\n\\n?(h) ln\\nthen ?(Vi ) <\\n\\n1\\n2 ?(Vjk ).\\n\\n1\\n?(Vjk )\\n?c\\n,\\n?(h)\\n2?(?)\\n\\n(10)\\n\\nHere and below, c is appropriate constant but may be different from\\n\\nline to line. Note that (10) holds if ?(h) ? c\\n\\n?(V\\n\\n?(Vjk )\\n?(?) ln\\n\\n?(?)\\n?(Vj )\\nk\\n\\n)\\n\\njk\\n, and in turn if ?(h) ? c ?(?) ln\\n1 since\\n?\\n\\n?(Vjk ) ? ?(Vi ) > 2??(?). But to have the last inequality, the algorithm only needs to label\\nO(?2 (?) ln2 1? ln 1? ) instances from DIS(Vi ). So the total number of labels requested by A2 is\\nO(?2 (?) ln 1? ln3 1? )\\nNow we give our main label complexity bounds for agnostic active learning.\\nK\\nTheorem 11 Let the instance space be [0, 1]d+1 . Let the Hypothesis space be HC\\n, where K > d.\\n?\\nK\\nAssume that the Bayes classifier h of the learning problem is in HC ; the noise condition (8) holds;\\nand DX has a density bounded by a Kth order smooth function as in Theorem 6. Then the A2\\n? with error rate erD (h)\\n? ? erD (h? ) + ? and the number of labels requested is at\\nalgorithm\\nh\\n?\\n?? outputs\\n2d\\n?\\n? ?\\n? 1 K+d ln 1 , where in O\\n? we hide the polylog 1 term.\\nmost O\\n?\\n\\n?\\n\\n?\\n\\nProof Note that the density DX is upper bounded by a smooth function implies that it is also upper\\nbounded by a constant M . Combining Theorem 5, 6 and 10 the theorem follows.\\nCombining Theorem 5, 7 and 10 we can show the following theorem.\\n?\\nTheorem 12 Let the instance space be [0, 1]d+1 . Let the Hypothesis space be HC\\n. Assume that\\n?\\nthe Bayes classifier h? of the learning problem is in HC\\n; the noise condition (8) holds; and DX\\nhas a density bounded by an infinitely smooth function as in Theorem 7. Then the A2 algorithm\\n? with error rate erD (h)\\n? ? erD (h? ) + ? and the number of labels requested is at most\\noutputs\\nh\\n?\\n?1? 1?\\nO polylog ? ln ? .\\n\\n7\\n\\n\\f4\\n\\nConclusion\\n\\nWe show that if the Bayesian classification boundary is smooth and the distribution is bounded by a\\nsmooth function, then under some noise condition active learning achieves polynomial or exponential improvement in the label complexity than passive supervised learning according to whether the\\nsmoothness is of finite order or infinite.\\nAlthough we assume that the classification boundary is the graph of a function, our results can be\\ngeneralized to the case that the boundaries are a finite number of functions. To be precise, consider\\nN functions f1 (x) ? ? ? ? ? fN (x), for all x ? [0, 1]d . Let f0 (x) ? 0, fN +1 (x) ? 1. The positive (or\\nnegative) set defined by these functions is {(x, xd+1 ) : f2i (x) ? x ? f2i+1 (x), i = 0, 1, . . . , N2 }.\\nOur theorems still hold in this case. In addition, by techniques in [Dud99] (page 259), our results\\nmay generalize to problems which have intrinsic smooth boundaries (not only graphs of functions).\\n\\nAppendix\\nIn this appendix we describe very briefly the ideas to prove Lemma 8 and Lemma 9. The formal\\nproofs can be found in the supplementary file.\\nIdeas to Prove Lemma 8 First consider the d = 1 case. Note that if f ? FCK , then |f (K?1) (x) ?\\nf (K?1) (x0 )| ? C|x ? x0 | for all x, x0 ? [0, 1]. It is not difficult to see that we only need to show for\\nR1\\nK\\nany f such that |f (K?1) (x)?f (K?1) (x0 )| ? C|x?x0 |, if 0 |f (x)|dx = r, then kf k? = O(r K+1 ).\\nR\\nTo show this, note that in order that kf k? achieves the maximum while |f | = r, the derivatives\\nof f must be as large as possible. Indeed, it can be shown that (one of) the optimal f is of the form\\n?\\nC\\n? K!\\n|x ? ?|K\\n0 ? x ? ?,\\n(11)\\nf (x) =\\n?\\n0\\n? < x ? 1.\\nThat is, |f (K?1) (x) ? f (K?1) (x0 )| = C|x ? x0 | (i.e. the K ? 1 order derivatives reaches the upper\\nR1\\nbound of the Lipschitz constant.) for all x, x0 ? [0, ?], where ? is determined by 0 f (x)dx = r. It\\nK\\nis then easy to check that kf k? = O(r K+1 ).\\nFor the general d > 1 case, we relax the constraint. Note that all K ? 1th order partial derivatives\\nare Lipschitz implies that all K ? 1th order directional derivatives are Lipschitz too. Under the latter\\nconstraint, (one of) the optimal f has the form\\n?\\nC\\n? K!\\n|kxk ? ?|K\\n0 ? kxk ? ?,\\nf (x) =\\n?\\n0\\n? < kxk.\\nR\\nK\\nwhere ? is determined by [0,1]d |f (x)|dx = r. This implies kf k? = O(r K+d ).\\nIdeas to Prove\\nR Lemma 9 Similar to the proof of Lemma 8, we only need to show that for any\\nf ? FC? , if [0,1]d |f (x)|dx = r, then kf k? = O(r ? logd ( 1r )).\\nSince f is infinitely smooth, we can choose K large and depending on r. For the d = 1 case, let\\nlog 1\\nK + 1 = log logr 1 . We know that the optimal f is of the form of Eq.(11). (Actually this choice of K\\nr\\nis approximately the largest K such that Eq.(11) is still the optimal form. If K is larger than this, ?\\nR1\\nC K\\nwill be out of [0, 1].) Since 0 |f (x)| = r, we have ? K+1 = (K+1)!\\n. Now, kf k? = K!\\n? . Note\\nC\\nthat ( 1r )K+1 = ( 1r )\\n\\nlog log 1\\nr\\nlog 1\\nr\\n\\n= log 1r . By Stirling?s formula we can show kf k? = O(r ? log 1r ).\\n\\nFor the d > 1 case, let K + d =\\n\\nlog r1\\nlog log\\n\\n1\\nr\\n\\n. By similar arguments we can show kf k? = O(r ? logd 1r ).\\n\\nAcknowledgement\\nThis work was supported by NSFC(60775005).\\n8\\n\\n\\fReferences\\n[Ang88]\\n\\nD. Angluin. Queries and concept learning. Machine Learning, 2:319?342, 1988.\\n\\n[BAL06]\\n\\nM.-F. Balcan, A.Beygelzimer, and J. Langford. Agnostic active learning. In 23th International\\nConference on Machine Learning, 2006.\\n\\n[CN07]\\n\\nR. Castro and R. Nowak. Minimax bounds for active learning. In 20th Annual Conference on\\nLearning Theory, 2007.\\n\\n[Das05]\\n\\nS. Dasgupta. Coarse sample complexity bounds for active learning. In Advances in Neural Information Processing Systems, 2005.\\n\\n[DHM07] S. Dasgupta, D. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. In Advances\\nin Neural Information Processing Systems, 2007.\\n[Dud99]\\n\\nR.M. Dudley. Uniform Central Limit Theorems. Cambridge University Press, 1999.\\n\\n[Fri09]\\n\\nE. Friedman. Active learning for smooth problems. In 22th Annual Conference on Learning\\nTheory, 2009.\\n\\n[GKW03] V.E. Gine, V.I. Koltchinskii, and J. Wellner. Ratio limit theorems for empirical processes. Stochastic Inequalities and Applications, 56:249?278, 2003.\\n[Han07]\\n\\nS. Hanneke. A bound on the label complexity of agnostic active learning. In 24th International\\nConference on Machine Learning, 2007.\\n\\n[Han09]\\n\\nS. Hanneke. Adaptive rates of convergence in active learning. In 22th Annual Conference on\\nLearning Theory, 2009.\\n\\n[Kaa06]\\n\\nM. Kaariainen. Active learning in the non-realizable case. In 17th International Conference on\\nAlgorithmic Learning Theory, 2006.\\n\\n[Tsy04]\\n\\nA. Tsybakov. Optimal aggregation of classifiers in statistical learning. The Annals of Statistics,\\n32:135?166, 2004.\\n\\n[vdG00]\\n\\nS. van de Geer. Applications of Empirical Process Theory. Cambridge University Press, 2000.\\n\\n[vdVW96] A. van der Vaart and J. Wellner. Weak Convergence and Empirical Processes with Application to\\nStatistics. Springer Verlag, 1996.\\n\\n9\\n\\n\\f\",\n          \"Viewpoint invariant face recognition using\\nindependent component analysis and\\nattractor networks\\nMarian Stewart Bartlett\\nUniversity of California San Diego\\nThe Salk Institute\\nLa Jolla, CA 92037\\nmarni@salk.edu\\n\\nTerrence J. Sejnowski\\nUniversity of California San Diego\\nHoward Hughes Medical Institute\\nThe Salk Institute, La Jolla, CA 92037\\nterry@salk.edu\\n\\nAbstract\\nWe have explored two approaches to recogmzmg faces across\\nchanges in pose. First, we developed a representation of face images\\nbased on independent component analysis (ICA) and compared it\\nto a principal component analysis (PCA) representation for face\\nrecognition. The ICA basis vectors for this data set were more\\nspatially local than the PCA basis vectors and the ICA representation had greater invariance to changes in pose. Second, we present\\na model for the development of viewpoint invariant responses to\\nfaces from visual experience in a biological system. The temporal\\ncontinuity of natural visual experience was incorporated into an\\nattractor network model by Hebbian learning following a lowpass\\ntemporal filter on unit activities. When combined with the temporal filter, a basic Hebbian update rule became a generalization\\nof Griniasty et al. (1993), which associates temporally proximal\\ninput patterns into basins of attraction. The system acquired representations of faces that were largely independent of pose.\\n\\n1\\n\\nIndependent component representations of faces\\n\\nImportant advances in face recognition have employed forms of principal component analysis, which considers only second-order moments of the input (Cottrell &\\nMetcalfe, 1991; Turk & Pentland 1991). Independent component analysis (ICA)\\nis a generalization of principal component analysis (PCA), which decorrelates the\\nhigher-order moments of the input (Comon, 1994). In a task such as face recognition, much of the important information is contained in the high-order statistics of\\nthe images. A representational basis in which the high-order statistics are decorrelated may be more powerful for face recognition than one in which only the second\\norder statistics are decorrelated, as in PCA representations. We compared an ICAbased representation to a PCA-based representation for recognizing faces across\\nchanges in pose.\\n\\n\\f818\\n\\nM. S. Bartlett and T. J. Sejnowski\\n\\n-30\\\"\\n\\n-IS\\\"\\n\\n0\\\"\\n\\nIS\\\"\\n\\n30\\\"\\n\\nFigure 1: Examples from image set (Beymer, 1994).\\nThe image set contained 200 images of faces, consisting of 40 subjects at each of\\nfive poses (Figure 1). The images were converted to vectors and comprised the rows\\nof a 200 x 3600 data matrix, X. We consider the face images in X to be a linear\\nmixture of an unknown set of statistically independent source images S, where A\\nis an unknown mixing matrix (Figure 2). The sources are recovered by a matrix of\\nlearned filters, W, which produce statistically independent outputs, U.\\n\\ns\\n\\nX\\n\\nX\\nSources\\n\\nUnknown\\nMixing\\nProcess\\n\\nU\\n\\n~\\n~\\n~\\n~\\n\\nX\\n\\nFace\\nImages\\n\\nLearned\\nWeights\\n\\nIII\\nSeparated\\nOutputs\\n\\nFigure 2: Image synthesis model.\\nThe weight matrix, W, was found through an unsupervised learning algorithm that\\nmaximizes the mutual information between the input and the output of a nonlinear\\ntransformation (Bell & Sejnowski, 1995). This algorithm has proven successful for\\nseparating randomly mixed auditory signals (the cocktail party problem), and has\\nrecently been applied to EEG signals (Makeig et al., 1996) and natural scenes (see\\nBell & Sejnowski, this volume). The independent component images contained in\\nthe rows of U are shown in Figure 3. In contrast to the principal components, all 200\\nindependent components were spatially local. We took as our face representation\\nthe rows of the matrix A = W- 1 which provide the linear combination of source\\nimages in U that comprise each face image in X.\\n1.1\\n\\nFace Recognition Performance: leA vs. Eigenfaces\\n\\nWe compared the performance of the leA representation to that of the peA representation for recognizing faces across changes in pose. The peA representation of a\\nface consisted of its component coefficients, which was equivalent to the \\\"Eigenface\\\"\\n\\n\\fViewpoint Invariant Face Recognition\\n\\n819\\n\\nFigure 3: Top: Four independent components of the image set. Bottom: First four\\nprincipal components.\\n\\nrepresentation (Turk & Pentland, 1991). A test image was recognized by assigning\\nit the label of the nearest of the other 199 images in Euclidean distance.\\nClassification error rates for the ICA and PCA representations and for the original\\ngraylevel images are presented in Table 1. For the PCA representation, the best\\nperformance was obtained with the 120 principal components corresponding to the\\nhighest eigenvalues. Dropping the first three principal components, or selecting\\nranges of intermediate components did not improve performance. The independent\\ncomponent sources were ordered by the magnitude of the weight vector, row of W,\\nused to extract the source from the image. 1 Best performance was obtained with\\nthe 130 independent components with the largest weight vectors. Performance with\\nthe ICA representation was significantly superior to Eigenfaces by a paired t-test\\n(0 < 0.05).\\n\\nGraylevel Images\\nPCA\\nICA\\n\\nMutual Information\\n.89\\n.10\\n.007\\n\\nPercent Correct Recognition\\n.83\\n.84\\n.87\\n\\nTable 1: Mean mutual information between all pairs of 10 basis images, and between the\\noriginal graylevel images. Face recognition performance is across all 200 images.\\nFor the task of recognizing faces across pose, a statistically independent basis set\\nprovided a more powerful representation for face images than a principal component\\nrepresentation in which only the second order statistics are decorrelated.\\nIThe magnitude of the weight vector for optimally projecting the source onto the sloping\\npart of the nonlinearity provides a measure of the variance of the original source (Tony\\nBell, personal communication).\\n\\n\\f820\\n\\n2\\n\\nM. S. Bartlett and T. 1. Sejnowski\\n\\nUnsupervised Learning of Viewpoint Invariant\\nRepresentations of Faces in an Attractor Network\\n\\nCells in the primate inferior temporal lobe have been reported that respond selectively to faces despite substantial changes in viewpoint (Hasselmo, Rolls, Baylis, &\\nNalwa, 1989). Some cells responded independently of viewing angle, whereas other\\ncells gave intermediate responses between a viewer-centered and an object centered\\nrepresentation. This section addresses how a system can acquire such invariance to\\nviewpoint from visual experience.\\nDuring natural visual experience, different views of an object or face tend to appear\\nin close temporal proximity as an animal manipulates the object or navigates around\\nit, or as a face changes pose. Capturing such temporal relationships in the input\\nis a way to automatically associate different views of an object without requiring\\nthree dimensional descriptions.\\nAttractor Network\\n\\nCoDlpetitive Hebbian Learning\\n\\nFigure 4: Model architecture.\\nHebbian learning can capture these temporal relationships in a feedforward system when the output unit activities are passed through a lowpass temporal filter\\n(Foldiak, 1991; Wallis & Rolls, 1996). Such lowpass temporal filters have been\\nrelated to the time course of the modifiable state of a neuron based on the open\\ntime of the NMDA channel for calcium influx (Rhodes, 1992). We show that 1)\\nthis lowpass temporal filter increases viewpoint invariance of face representations in\\na feedforward system trained with competitive Hebbian learning, and 2) when the\\ninput patterns to an attractor network are passed through a lowpass temporal filter, then a basic Hebbian weight update rule associates sequentially proximal input\\npatterns into the same basin of attraction.\\nThis simulation used a subset of 100 images from Section 1, consisting of twenty\\nfaces at five poses each. Images were presented to the model in sequential order as\\nthe subject changed pose from left to right (Figure 4). The first layer is an energy\\nmodel related to the output of V1 complex cells (Heeger, 1991). The images were\\nfiltered by a set of sine and cosine Gabor filters at 4 spatial scales and 4 orientations\\nat 255 spatial locations. Sine and cosine outputs were squared and summed. The\\nset of V1 model outputs projected to a second layer of 70 units, grouped into two\\n\\n\\fViewpoint Invariant Face Recognition\\n\\n821\\n\\ninhibitory pools. The third stage of the model was an attractor network produced\\nby lateral interconnections among all of the complex pattern units. The feedforward\\nand lateral connections were trained successively.\\n2.1\\n\\nCompetitive Hebbian learning of temporal relationships\\n\\nThe Competitive Learning Algorithm (Rumelhart & Zipser, 1985) was extended to\\ninclude a temporal lowpass filter on output unit activities (Bartlett & Sejnowski,\\n1996). This manipulation gives the winner in the previous time steps a competitive\\nadvantage for winning, and therefore learning, in the current time step.\\n\\nwinner = maxj [y/t)]\\ny/t) = AY} + (1 _ A)y/t-1)\\n\\n(1)\\n\\nThe output activity of unit j at time t, y/t), is determined by the trace, or running\\naverage, of its activation, where y} is the weighted sum of the feedforward inputs, a\\nis the learning rate, Xitl is the value of input unit i for pattern u, and 8t1 is the total\\namount of input activation for pattern u. The weight to each unit was constrained\\nto sum to one. This algorithm was used to train the feedforward connections. There\\nwas one face pattern per time step and A was set to 1 between individuals.\\n2.2\\n\\nLateral connections in the output layer form an attractor network\\n\\nHebbian learning of lateral interconnections, in combination with a lowpass temporal filter on the unit activities in (1), produces a learning rule that associates\\ntemporally proximal inputs into basins of attraction. We begin with a simple Hebbian learning rule\\nP\\n\\nWij\\n\\n=\\n\\n~ L(Y~ -\\n\\nyO)(y} _ yO)\\n\\n(2)\\n\\nt=1\\nwhere N is the number of units, P is the number of patterns, and yO is the mean\\nactivity over all of the units. Replacing y~ with the activity trace y/t) defined in\\n(1), substituting yO = Ayo + (1 - A)YO and multiplying out the terms leads to the\\nfollowing learning rule:\\n\\n+k2 [(y/t-1) _ yO)(y/t-1) _ yO)]\\nh\\nk 1= >'(1;>')\\nwere\\n>.\\nan d k 2=\\n\\n(3)\\n(1_;)2\\n\\n>.\\n\\nThe first term in this equation is basic Hebbian learning, the second term associates\\npattern t with pattern t - 1, and the third term is Hebbian association of the trace\\nactivity for pattern t - 1. This learning rule is a generalization of an attractor\\nnetwork learning rule that has been shown to associate random input patterns\\n\\n\\f822\\n\\nM. S. Bartlett and T. 1. Sejnowski\\n\\ninto basins of attraction based on serial position in the input sequence (Griniasty,\\nTsodyks & Amit, 1993). The following update rule was used for the activation V\\nof unit i at time t from the lateral inputs (Griniasty, Tsodyks, & Amit, 1993) :\\n\\nVi(t + lSt) = ?\\n\\n[I: W\\n\\nij Vj(t)\\n\\n- 0]\\n\\nWhere 0 is a neural threshold and ?(x) = 1 for x > 0, and 0 otherwise. In these\\nsimulations, 0 = 0.007, N = 70, P = 100, yO = 0.03, and A = 0.5 gave kl = k2 = 1.\\n\\n2.3\\n\\nResults\\n\\nTemporal association in the feedforward connections broadened the pose tuning of\\nthe output units (Figure 5 Left) . When the lateral connections in the output layer\\nwere added, the attractor network acquired responses that were largely invariant to\\npose (Figure 5 Right).\\n? Hebb plus trace\\nDTesiset\\n.. Griniasly 01. al.\\n? Hebbonly\\n\\n[J Same Face, with Trace\\no Same Face, no Trace\\n-- Different Faces\\n\\n80.8\\n\\n:c\\n\\n,.!g\\n~ 0.6\\n\\n...\\n<3~0.4\\nQ)\\n\\n~0.2\\n\\no,....-o_-u,\\n-600\\n\\n-450 _30?\\n\\n_15?\\n\\n0?\\n\\n15?\\n\\n30?\\n\\n45?\\n\\n60?\\n\\n-600 _45? _30?\\n\\n~Pose\\n\\n_15?\\n\\n0?\\n\\n15?\\n\\n30?\\n\\n45?\\n\\n60?\\n\\n~Pose\\n\\nFigure 5:\\nLeft: Correlation of the outputs of the feedforward system as a function\\nof change in pose. Correlations across different views of the same face (- ) are compared\\nto correlations across different faces (--) with the temporal trace parameter A = 0.5\\nand A = O. Right: Correlations in sustained activity patterns in the attractor network\\nas a function of change in pose. Results obtained with Equation 3 (Hebb plus trace) are\\ncompared to Hebb only, and to the learning rule in Griniasty et al. (1993). Test set\\nresults for Equation 3 (open squares) were obtained by alternately training on four poses\\nand testing on the fifth, and then averaging across all test cases.\\n\\nF\\n\\n5\\n10\\n20\\n\\n20\\n\\nN\\n70\\n70\\n70\\n160\\n\\nAttractor Network\\n% Correct\\nFIN\\n.07\\n1.00\\n_14\\n.90\\n.29\\n.61\\n.13\\n.73\\n\\nleA\\n% Correct\\n.96\\n.86\\n.89\\n.89\\n\\nTable 2: Face classification performance of the attractor network for four ratios of the\\nnumber of desired memories, F, to the number of units, N. Results are compared to ICA\\nfor the same subset of faces.\\n\\n\\fViewpoint Invariant Face Recognition\\n\\n823\\n\\nClassification accuracy of the attractor network was calculated by nearest neighbor\\non the activity states (Table 2). Performance of the attractor network depends both\\non the performance of the feedforward system, which comprises its input, and on\\nthe ratio of the number of patterns to be encoded in memory, F, to the number of\\nunits, N, where each individual in the face set comprises one memory pattern. The\\nattractor network performed well when this ratio was sufficiently high. The ICA\\nrepresentation also performed well, especially for N=20.\\nThe goal of this simulation was to begin with structured inputs similar to the responses of VI complex cells, and to explore the performance of unsupervised learning mechanisms that can transform these inputs into pose invariant responses. We\\nshowed that a lowpass temporal filter on unit activities, which has been related\\nto the time course of the modifiable state of a neuron (Rhodes, 1992), cooperates\\nwith Hebbian learning to (1) increase the viewpoint invariance of responses to faces\\nin a feedforward system, and (2) create basins of attraction in an attractor network which associate temporally proximal inputs. These simulations demonstrated\\nthat viewpoint invariant representations of complex objects such as faces can be\\ndeveloped from visual experience by accessing the temporal structure of the input.\\nAcknowledgments\\nThis project was supported by Lawrence Livermore National Laboratory ISCR Agreement\\nB291528, and by the McDonnell-Pew Center for Cognitive Neuroscience at San Diego.\\n\\nReferences\\nBartlett, M. Stewart, & Sejnowski, T., 1996. Unsupervised learning of invariant representations of faces through temporal association. Computational Neuroscience: Int. Rev.\\nNeurobio. Suppl. 1 J.M Bower, Ed., Academic Press, San Diego, CA:317-322.\\nBeymer, D. 1994. Face recognition under varying pose. In Proceedings of the 1994 IEEE\\nComputer Society Conference on Computer Vision and Pattern Recognition. Los Alamitos, CA: IEEE Comput. Soc. Press: 756-61.\\nBell, A. & Sejnowski, T., (1997). The independent components of natural scenes are edge\\nfilters. Advances in Neural Information Processing Systems 9.\\nBell, A., & Sejnowski, T., 1995. An information Maximization approach to blind separation and blind deconvolution. Neural Compo 7: 1129-1159.\\nComon, P. 1994. Independent component analysis - a new concept? Signal Processing\\n36:287-314.\\nCottrell & Metcalfe, 1991. Face, gender and emotion recognition using Holons. In Advances in Neural Information Processing Systems 3, D. Touretzky, (Ed.), Morgan Kaufman, San Mateo, CA: 564 - 571.\\nFoldiak, P. 1991. Learning invariance from transformation sequences. Neural Compo 3:194200.\\nGriniasty, M., Tsodyks, M., & Amit, D. 1993. Conversion of temporal correlations between\\nstimuli to spatial correlations between attractors. Neural Compo 5:1-17.\\nHasselmo M. Rolls E. Baylis G. & Nalwa V. 1989. Object-centered encoding by faceselective neurons in the cortex in the superior temporal sulcus of the monkey. Experimental Brain Research 75(2):417-29.\\nHeeger, D. (1991). Nonlinear model of neural responses in cat visual cortex. Computational\\nModels of Visual Processing, M. Landy & J. Movshon, Eds. MIT Press, Cambridge,\\nMA.\\nMakeig, S, Bell, AJ, Jung, T-P, and Sejnowski, TJ 1996. Independent component analysis of Electroencephalographic data, In: Advances in Neural Information Processing\\nSystems 8, 145-151.\\nRhodes, P. 1992. The long open time of the NMDA channel facilitates the self-organization\\nof invariant object responses in cortex. Soc. Neurosci. Abst. 18:740.\\nRumelhart, D. & Zipser, D. 1985. Feature discovery by competitive learning. Cognitive\\nScience 9: 75-112.\\nTurk, M., & Pentland, A. 1991. Eigenfaces for Recognition. J. Cog. Neurosci. 3(1):71-86.\\nWallis, G. & Rolls, E. 1996. A model of invariant object recognition in the visual system.\\nTechnical Report, Oxford University Department of Experimental Psychology.\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Remove the columns\n",
        "papers = papers.drop(columns=['id', 'event_type', 'pdf_name'], axis=1).sample(100)\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZwSLwLUw_AN"
      },
      "source": [
        "##### Remove punctuation/lower casing\n",
        "\n",
        "Next, let’s perform a simple preprocessing on the content of `paper_text` column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "Avtmskxbw_AN",
        "outputId": "007fac37-1f8d-4a82-d020-934d0b16dddd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4545    density estimation from unweighted k-nearest\\n...\n",
              "4685    delay-tolerant algorithms for\\nasynchronous di...\n",
              "4562    one-shot learning by inverting a compositional...\n",
              "4459    memory limited streaming pca\\n\\nconstantine ca...\n",
              "1447    learning semantic similarity\\n\\njaz kandola\\nj...\n",
              "Name: paper_text_processed, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_text_processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4545</th>\n",
              "      <td>density estimation from unweighted k-nearest\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4685</th>\n",
              "      <td>delay-tolerant algorithms for\\nasynchronous di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4562</th>\n",
              "      <td>one-shot learning by inverting a compositional...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4459</th>\n",
              "      <td>memory limited streaming pca\\n\\nconstantine ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1447</th>\n",
              "      <td>learning semantic similarity\\n\\njaz kandola\\nj...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Load the regular expression library\n",
        "import re\n",
        "\n",
        "# Remove punctuation\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "\n",
        "# Convert the titles to lowercase\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text_processed'].map(lambda x: x.lower())\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers['paper_text_processed'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-nC7pn_w_AN"
      },
      "source": [
        "** **\n",
        "#### Step 3: Exploratory Analysis <a class=\"anchor\\\" id=\"eda\"></a>\n",
        "** **\n",
        "\n",
        "To verify whether the preprocessing, we’ll make a simple word cloud using the `wordcloud` package to get a visual representation of most common words. It is key to understanding the data and ensuring we are on the right track, and if any more preprocessing is necessary before training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "4zDdCXRiw_AO",
        "outputId": "2fef7e3d-f08f-4f0e-9068-83e714fd62f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=400x200>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Join the different processed titles together.\n",
        "long_string = ','.join(list(papers['paper_text_processed'].values))\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD8f_Yv0w_AO"
      },
      "source": [
        "** **\n",
        "#### Step 4: Prepare text for LDA analysis <a class=\"anchor\\\" id=\"data_preparation\"></a>\n",
        "** **\n",
        "\n",
        "Next, let’s work to transform the textual data in a format that will serve as an input for training LDA model. We start by tokenizing the text and removing stopwords. Next, we convert the tokenized object into a corpus and dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB6JlQ47w_AO",
        "outputId": "28a36d25-f261-4f6f-b3df-0f2dfdd8c986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['density', 'estimation', 'unweighted', 'nearest', 'neighbor', 'graphs', 'roadmap', 'ulrike', 'von', 'luxburg', 'morteza', 'alamgir', 'department', 'computer', 'science', 'university', 'hamburg', 'germany', 'luxburgalamgir', 'informatikuni', 'hamburgde', 'abstract', 'consider', 'unweighted', 'nearest', 'neighbor', 'graph', 'points', 'sampled', 'iid']\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc))\n",
        "             if word not in stop_words] for doc in texts]\n",
        "\n",
        "\n",
        "data = papers.paper_text_processed.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "# remove stop words\n",
        "data_words = remove_stopwords(data_words)\n",
        "\n",
        "print(data_words[:1][0][:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjOmpTo5w_AO",
        "outputId": "224a7e18-406d-40a4-e304-bb344d2da158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 4), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 4), (12, 2), (13, 1), (14, 6), (15, 4), (16, 8), (17, 1), (18, 2), (19, 4), (20, 12), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 4), (27, 3), (28, 5), (29, 3)]\n"
          ]
        }
      ],
      "source": [
        "import gensim.corpora as corpora\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_words)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_words\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1][0][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P-NMl12w_AO"
      },
      "source": [
        "** **\n",
        "#### Step 5: LDA model tranining <a class=\"anchor\\\" id=\"train_model\"></a>\n",
        "** **\n",
        "\n",
        "To keep things simple, we'll keep all the parameters to default except for inputting the number of topics. For this tutorial, we will build a model with 10 topics where each topic is a combination of keywords, and each keyword contributes a certain weightage to the topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbDugS-Lw_AP",
        "outputId": "ed88cb77-aabb-4cef-9bd9-a01857f32eaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.006*\"data\" + 0.005*\"learning\" + 0.004*\"model\" + 0.004*\"algorithm\" + '\n",
            "  '0.004*\"using\" + 0.004*\"number\" + 0.004*\"two\" + 0.004*\"set\" + 0.004*\"time\" + '\n",
            "  '0.003*\"network\"'),\n",
            " (1,\n",
            "  '0.006*\"learning\" + 0.005*\"set\" + 0.005*\"model\" + 0.005*\"data\" + '\n",
            "  '0.005*\"function\" + 0.005*\"algorithm\" + 0.004*\"one\" + 0.003*\"using\" + '\n",
            "  '0.003*\"given\" + 0.003*\"models\"'),\n",
            " (2,\n",
            "  '0.007*\"model\" + 0.007*\"learning\" + 0.005*\"function\" + 0.005*\"algorithm\" + '\n",
            "  '0.004*\"data\" + 0.004*\"set\" + 0.004*\"using\" + 0.003*\"one\" + 0.003*\"also\" + '\n",
            "  '0.003*\"two\"'),\n",
            " (3,\n",
            "  '0.007*\"learning\" + 0.006*\"model\" + 0.005*\"algorithm\" + 0.005*\"data\" + '\n",
            "  '0.005*\"one\" + 0.005*\"time\" + 0.004*\"function\" + 0.004*\"set\" + 0.003*\"using\" '\n",
            "  '+ 0.003*\"problem\"'),\n",
            " (4,\n",
            "  '0.006*\"learning\" + 0.005*\"algorithm\" + 0.004*\"data\" + 0.004*\"function\" + '\n",
            "  '0.004*\"set\" + 0.004*\"model\" + 0.003*\"one\" + 0.003*\"time\" + 0.003*\"log\" + '\n",
            "  '0.003*\"linear\"'),\n",
            " (5,\n",
            "  '0.006*\"algorithm\" + 0.005*\"learning\" + 0.005*\"model\" + 0.004*\"set\" + '\n",
            "  '0.004*\"function\" + 0.004*\"using\" + 0.004*\"data\" + 0.004*\"one\" + '\n",
            "  '0.003*\"network\" + 0.003*\"time\"'),\n",
            " (6,\n",
            "  '0.006*\"algorithm\" + 0.005*\"learning\" + 0.004*\"model\" + 0.004*\"network\" + '\n",
            "  '0.004*\"data\" + 0.004*\"problem\" + 0.004*\"using\" + 0.003*\"function\" + '\n",
            "  '0.003*\"set\" + 0.003*\"number\"'),\n",
            " (7,\n",
            "  '0.006*\"learning\" + 0.005*\"algorithm\" + 0.004*\"set\" + 0.004*\"model\" + '\n",
            "  '0.004*\"function\" + 0.003*\"one\" + 0.003*\"two\" + 0.003*\"data\" + 0.003*\"using\" '\n",
            "  '+ 0.003*\"number\"'),\n",
            " (8,\n",
            "  '0.006*\"algorithm\" + 0.006*\"model\" + 0.005*\"function\" + 0.005*\"learning\" + '\n",
            "  '0.005*\"using\" + 0.004*\"data\" + 0.004*\"one\" + 0.003*\"set\" + 0.003*\"problem\" '\n",
            "  '+ 0.003*\"matrix\"'),\n",
            " (9,\n",
            "  '0.007*\"algorithm\" + 0.006*\"set\" + 0.005*\"model\" + 0.004*\"learning\" + '\n",
            "  '0.004*\"function\" + 0.004*\"data\" + 0.004*\"number\" + 0.003*\"distribution\" + '\n",
            "  '0.003*\"two\" + 0.003*\"models\"')]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# number of topics\n",
        "num_topics = 10\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=num_topics)\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MoGlnKgw_AP"
      },
      "source": [
        "** **\n",
        "#### Step 6: Analyzing our LDA model <a class=\"anchor\\\" id=\"results\"></a>\n",
        "** **\n",
        "\n",
        "Now that we have a trained model let’s visualize the topics for interpretability. To do so, we’ll use a popular visualization package, pyLDAvis which is designed to help interactively with:\n",
        "\n",
        "1. Better understanding and interpreting individual topics, and\n",
        "2. Better understanding the relationships between the topics.\n",
        "\n",
        "For (1), you can manually select each topic to view its top most frequent and/or “relevant” terms, using different values of the λ parameter. This can help when you’re trying to assign a human interpretable name or “meaning” to each topic.\n",
        "\n",
        "For (2), exploring the Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xxK2l_D2IND",
        "outputId": "1b770d0b-2aa4-4e26-ffd4-7be4e721ee5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.1)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.5.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (75.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MhsGMRi0w_AP",
        "outputId": "65c19830-38a9-4206-c807-2b7db50d1a8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.1)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.5.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (75.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.16.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "7      0.003858 -0.004192       1        1  19.408119\n",
              "0      0.006410  0.003220       2        1  17.717659\n",
              "5      0.010772  0.002848       3        1  17.004616\n",
              "3     -0.004673 -0.002122       4        1  12.253862\n",
              "4     -0.005089  0.007399       5        1  11.672877\n",
              "1     -0.005632  0.000660       6        1   7.273685\n",
              "2     -0.001321 -0.000120       7        1   6.259004\n",
              "9      0.001595 -0.004001       8        1   5.091388\n",
              "6     -0.004602  0.000716       9        1   2.803964\n",
              "8     -0.001319 -0.004408      10        1   0.514826, topic_info=          Term         Freq        Total Category  logprob  loglift\n",
              "457      model  1201.000000  1201.000000  Default  30.0000  30.0000\n",
              "147       data  1002.000000  1002.000000  Default  29.0000  29.0000\n",
              "853  algorithm  1307.000000  1307.000000  Default  28.0000  28.0000\n",
              "398   learning  1300.000000  1300.000000  Default  27.0000  27.0000\n",
              "502        one   819.000000   819.000000  Default  26.0000  26.0000\n",
              "..         ...          ...          ...      ...      ...      ...\n",
              "290      given     2.999778   645.992741  Topic10  -5.9833  -0.1032\n",
              "757        two     3.045148   710.773093  Topic10  -5.9683  -0.1837\n",
              "409        let     2.539651   491.519688  Topic10  -6.1498   0.0036\n",
              "774       used     2.454061   562.692580  Topic10  -6.1841  -0.1659\n",
              "257     figure     2.388084   614.743526  Topic10  -6.2114  -0.2816\n",
              "\n",
              "[878 rows x 6 columns], token_table=       Topic      Freq    Term\n",
              "term                          \n",
              "14298      5  0.293320  ackley\n",
              "1619       1  0.284971  action\n",
              "1619       2  0.116579  action\n",
              "1619       3  0.220205  action\n",
              "1619       4  0.103626  action\n",
              "...      ...       ...     ...\n",
              "15983      9  0.133276     zkm\n",
              "12231      1  0.184396     zlt\n",
              "12231      2  0.184396     zlt\n",
              "12231      3  0.184396     zlt\n",
              "12231      4  0.368791     zlt\n",
              "\n",
              "[3577 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[8, 1, 6, 4, 5, 2, 3, 10, 7, 9])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el4971389635014574562792230603\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el4971389635014574562792230603_data = {\"mdsDat\": {\"x\": [0.0038577093186581714, 0.006410432346583677, 0.01077226741563022, -0.004672841693070069, -0.00508859695959415, -0.005631803198908614, -0.0013208257986514482, 0.0015946930121921844, -0.004602297109382585, -0.001318737333457374], \"y\": [-0.004191783175752346, 0.0032202248170841614, 0.002848198622535182, -0.002122468567868469, 0.007399247463886831, 0.000660106262609234, -0.00012031039796861072, -0.004001118238415187, 0.0007156111799441606, -0.004407707966054961], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [19.408118979019637, 17.717659297883785, 17.00461563281672, 12.2538616511661, 11.672877019128492, 7.273685408388797, 6.259004216702518, 5.091387973211626, 2.803963728261147, 0.5148260934211696]}, \"tinfo\": {\"Term\": [\"model\", \"data\", \"algorithm\", \"learning\", \"one\", \"function\", \"network\", \"set\", \"time\", \"using\", \"number\", \"results\", \"error\", \"linear\", \"distribution\", \"problem\", \"also\", \"given\", \"xi\", \"neural\", \"training\", \"models\", \"method\", \"two\", \"figure\", \"information\", \"matrix\", \"space\", \"different\", \"used\", \"rkhss\", \"qn\", \"kpm\", \"translation\", \"charge\", \"mmd\", \"klin\", \"mle\", \"mv\", \"vt\", \"ina\", \"reproducing\", \"regained\", \"codeword\", \"mpi\", \"parzen\", \"kcon\", \"svmd\", \"injective\", \"kinetics\", \"ovie\", \"um\", \"kkcon\", \"implicit\", \"sriperumbudur\", \"ag\", \"qna\", \"pyi\", \"brains\", \"currents\", \"kernels\", \"glm\", \"voltage\", \"ilk\", \"yj\", \"rbfs\", \"ft\", \"pm\", \"shareboost\", \"characteristic\", \"annotation\", \"cm\", \"test\", \"norm\", \"day\", \"rf\", \"xi\", \"action\", \"scene\", \"field\", \"regression\", \"also\", \"sparse\", \"two\", \"data\", \"kernel\", \"size\", \"ij\", \"using\", \"algorithms\", \"different\", \"optimization\", \"function\", \"method\", \"mean\", \"yt\", \"matrix\", \"let\", \"number\", \"given\", \"neural\", \"rate\", \"image\", \"one\", \"model\", \"learning\", \"algorithm\", \"time\", \"log\", \"distribution\", \"features\", \"first\", \"linear\", \"results\", \"error\", \"training\", \"information\", \"set\", \"problem\", \"network\", \"figure\", \"epu\", \"eus\", \"denition\", \"sgd\", \"solla\", \"lattices\", \"bvs\", \"ko\", \"wref\", \"rnl\", \"tensor\", \"separability\", \"cruz\", \"norouzi\", \"glrt\", \"sgn\", \"pih\", \"ghm\", \"wv\", \"clm\", \"ewtron\", \"conductance\", \"elevated\", \"maxm\", \"rgg\", \"ssgd\", \"lmita\", \"rect\", \"prom\", \"csat\", \"boosting\", \"qt\", \"recommendation\", \"elicitation\", \"teaching\", \"greedy\", \"trees\", \"rank\", \"following\", \"loss\", \"sparse\", \"xt\", \"set\", \"vector\", \"tree\", \"data\", \"algorithm\", \"models\", \"bound\", \"case\", \"class\", \"sets\", \"sample\", \"optimal\", \"one\", \"function\", \"model\", \"machine\", \"approach\", \"non\", \"matrix\", \"optimization\", \"linear\", \"used\", \"learning\", \"problem\", \"using\", \"performance\", \"training\", \"figure\", \"first\", \"time\", \"two\", \"number\", \"log\", \"xi\", \"different\", \"also\", \"given\", \"rout\", \"shortest\", \"leftnk\", \"leftd\", \"rightd\", \"def\", \"knn\", \"badoiu\", \"demaine\", \"density\", \"rectifiable\", \"err\", \"covers\", \"shaw\", \"xd\", \"lnk\", \"leftdnk\", \"marchand\", \"valiant\", \"phi\", \"groenen\", \"ordinal\", \"rightdnk\", \"ouyang\", \"hajiaghayi\", \"xce\", \"rightnk\", \"logd\", \"linial\", \"directional\", \"wn\", \"passing\", \"junction\", \"undirected\", \"assignment\", \"bandit\", \"reward\", \"paths\", \"mwal\", \"regret\", \"rtd\", \"tables\", \"algorithm\", \"let\", \"blocking\", \"problem\", \"graph\", \"however\", \"lower\", \"estimate\", \"compute\", \"vcd\", \"lemma\", \"xi\", \"rn\", \"bound\", \"set\", \"method\", \"space\", \"model\", \"denote\", \"distributions\", \"learning\", \"distribution\", \"results\", \"ie\", \"log\", \"function\", \"algorithms\", \"matrix\", \"using\", \"two\", \"one\", \"used\", \"sample\", \"data\", \"given\", \"time\", \"number\", \"linear\", \"features\", \"figure\", \"also\", \"case\", \"manipulandum\", \"emanuel\", \"krug\", \"mussa\", \"brashers\", \"clockwise\", \"reza\", \"shadmehr\", \"plant\", \"controller\", \"limbs\", \"zlt\", \"todorov\", \"squire\", \"pio\", \"day\", \"pif\", \"engage\", \"sensorimotor\", \"catastrophic\", \"weinshall\", \"twelve\", \"edelman\", \"multiuser\", \"iio\", \"medline\", \"mpm\", \"alverez\", \"sio\", \"interference\", \"curl\", \"corr\", \"consolidation\", \"field\", \"frame\", \"objects\", \"motif\", \"object\", \"motifs\", \"semantic\", \"hmdm\", \"demodulator\", \"components\", \"learning\", \"equations\", \"gaze\", \"force\", \"face\", \"subjects\", \"layer\", \"may\", \"units\", \"noise\", \"given\", \"input\", \"parameters\", \"performance\", \"task\", \"one\", \"mean\", \"model\", \"time\", \"using\", \"show\", \"based\", \"gaussian\", \"example\", \"dimensional\", \"first\", \"different\", \"figure\", \"number\", \"data\", \"used\", \"also\", \"analysis\", \"algorithm\", \"two\", \"log\", \"function\", \"distribution\", \"neural\", \"error\", \"network\", \"linear\", \"set\", \"matrix\", \"results\", \"problem\", \"models\", \"ala\", \"satisfaction\", \"fti\", \"glasso\", \"externally\", \"ackley\", \"winning\", \"leinbach\", \"kaggle\", \"colder\", \"kludge\", \"unifonn\", \"fluorescence\", \"minsky\", \"subschemas\", \"schedule\", \"convolution\", \"prec\", \"netinput\", \"nit\", \"cold\", \"steven\", \"schema\", \"maxneggdnss\", \"freezes\", \"leaderboard\", \"scheduled\", \"dij\", \"baptiste\", \"predetennined\", \"annealing\", \"network\", \"subtree\", \"neurogenesys\", \"neuron\", \"competition\", \"units\", \"firing\", \"visible\", \"genetic\", \"neurons\", \"objects\", \"ca\", \"representing\", \"networks\", \"excitatory\", \"training\", \"neural\", \"case\", \"covariance\", \"maxima\", \"input\", \"time\", \"figure\", \"unit\", \"function\", \"set\", \"large\", \"given\", \"learning\", \"number\", \"class\", \"algorithm\", \"methods\", \"two\", \"image\", \"used\", \"information\", \"loss\", \"rate\", \"model\", \"models\", \"using\", \"problem\", \"data\", \"different\", \"one\", \"log\", \"matrix\", \"results\", \"also\", \"distribution\", \"parser\", \"roles\", \"sentences\", \"phrases\", \"hausser\", \"ate\", \"pat\", \"slot\", \"phra\", \"bone\", \"fido\", \"verb\", \"grammatically\", \"interclause\", \"rbs\", \"elman\", \"grammar\", \"equalizer\", \"clause\", \"infonnation\", \"supposed\", \"equalizers\", \"schreiber\", \"letter\", \"mog\", \"subordinate\", \"gibson\", \"parsing\", \"contingencies\", \"closes\", \"samad\", \"genetic\", \"hidden\", \"phrase\", \"strings\", \"word\", \"exponential\", \"units\", \"topic\", \"topics\", \"motif\", \"multinomial\", \"model\", \"recurrent\", \"crfs\", \"network\", \"training\", \"utility\", \"linear\", \"set\", \"nonlinear\", \"bayesian\", \"distribution\", \"data\", \"well\", \"parameter\", \"learning\", \"used\", \"structure\", \"log\", \"neural\", \"number\", \"models\", \"results\", \"figure\", \"given\", \"using\", \"section\", \"function\", \"two\", \"information\", \"problem\", \"algorithm\", \"matrix\", \"one\", \"error\", \"based\", \"algorithms\", \"time\", \"different\", \"also\", \"plefka\", \"tau\", \"xuanlong\", \"harik\", \"sauls\", \"pruned\", \"reassurance\", \"xali\", \"cdp\", \"exi\", \"mf\", \"glims\", \"strokes\", \"steerable\", \"pawelzik\", \"concavities\", \"ditioned\", \"laar\", \"tightened\", \"layered\", \"dcts\", \"imaginary\", \"offspring\", \"moralization\", \"pruning\", \"cleaned\", \"jutten\", \"dereverberated\", \"xil\", \"yedidia\", \"herb\", \"reverberation\", \"ica\", \"sh\", \"periodic\", \"mmse\", \"operator\", \"stein\", \"dereverberation\", \"li\", \"inference\", \"visual\", \"error\", \"nodes\", \"classification\", \"intensity\", \"gain\", \"smooth\", \"function\", \"layer\", \"also\", \"images\", \"figure\", \"frequency\", \"functions\", \"learning\", \"work\", \"set\", \"results\", \"neural\", \"optimal\", \"data\", \"network\", \"based\", \"image\", \"one\", \"number\", \"high\", \"time\", \"using\", \"two\", \"test\", \"output\", \"models\", \"let\", \"model\", \"algorithm\", \"given\", \"section\", \"first\", \"log\", \"space\", \"matrix\", \"algorithms\", \"problem\", \"xi\", \"linear\", \"spanner\", \"topics\", \"centrality\", \"mg\", \"krg\", \"discrepancy\", \"buchin\", \"subtracted\", \"unif\", \"stein\", \"ec\", \"erratum\", \"discrepancies\", \"riv\", \"gkk\", \"planning\", \"xnk\", \"bouts\", \"soho\", \"rodents\", \"senses\", \"ess\", \"langevin\", \"hiq\", \"module\", \"landauer\", \"complexities\", \"lever\", \"campus\", \"wasserstein\", \"topic\", \"modules\", \"lda\", \"ctm\", \"tp\", \"hln\", \"compression\", \"modularization\", \"ucrl\", \"dl\", \"cholinergic\", \"si\", \"mi\", \"low\", \"modularized\", \"demodulator\", \"learning\", \"space\", \"figure\", \"response\", \"value\", \"parameters\", \"model\", \"random\", \"xi\", \"neural\", \"variable\", \"network\", \"see\", \"number\", \"set\", \"mean\", \"distribution\", \"log\", \"one\", \"data\", \"problem\", \"algorithm\", \"input\", \"loss\", \"also\", \"results\", \"function\", \"given\", \"using\", \"method\", \"first\", \"models\", \"information\", \"training\", \"two\", \"used\", \"error\", \"winning\", \"lags\", \"kaggle\", \"initialisation\", \"glasso\", \"lag\", \"zkm\", \"stft\", \"bse\", \"xdt\", \"gdk\", \"fluorescence\", \"timme\", \"fsj\", \"modulate\", \"modulators\", \"cascade\", \"scattering\", \"sounds\", \"matyas\", \"auditory\", \"alexandre\", \"sinusoids\", \"realisable\", \"connectomics\", \"netinput\", \"gael\", \"rglasso\", \"rtt\", \"generalisation\", \"scores\", \"topic\", \"lda\", \"mcp\", \"modulator\", \"convolution\", \"auc\", \"time\", \"phoneme\", \"models\", \"topics\", \"frequency\", \"power\", \"network\", \"paper\", \"covariance\", \"model\", \"scale\", \"algorithm\", \"one\", \"function\", \"approach\", \"using\", \"networks\", \"linear\", \"show\", \"inference\", \"features\", \"well\", \"method\", \"variables\", \"also\", \"results\", \"second\", \"single\", \"data\", \"matrix\", \"problem\", \"since\", \"learning\", \"figure\", \"distribution\", \"space\", \"xi\", \"given\", \"log\", \"first\", \"two\", \"set\", \"number\", \"error\", \"equalization\", \"cfn\", \"proakis\", \"qam\", \"equalizer\", \"gelfand\", \"dispersive\", \"confused\", \"jxj\", \"fukunaga\", \"gibson\", \"krg\", \"equalizers\", \"timedispersive\", \"ofrbs\", \"fir\", \"researches\", \"distortions\", \"featurespace\", \"radial_basis\", \"rbs\", \"constellation\", \"taps\", \"bas\", \"push\", \"oufut\", \"intersymbol\", \"tsih\", \"herding\", \"voterra\", \"ilk\", \"lc\", \"gkkqg\", \"spanner\", \"roles\", \"tp\", \"hybrid\", \"clause\", \"stein\", \"impulse\", \"phrase\", \"oz\", \"step\", \"sentences\", \"computation\", \"characteristic\", \"bayes\", \"constant\", \"yt\", \"gmm\", \"zero\", \"processing\", \"one\", \"information\", \"testing\", \"decision\", \"data\", \"error\", \"signal\", \"xt\", \"gaussian\", \"results\", \"network\", \"noise\", \"linear\", \"distribution\", \"training\", \"rate\", \"time\", \"set\", \"function\", \"random\", \"neural\", \"different\", \"xi\", \"method\", \"number\", \"algorithm\", \"model\", \"learning\", \"space\", \"matrix\", \"also\", \"using\", \"problem\", \"given\", \"two\", \"let\", \"used\", \"figure\"], \"Freq\": [1201.0, 1002.0, 1307.0, 1300.0, 819.0, 982.0, 632.0, 966.0, 701.0, 845.0, 684.0, 541.0, 504.0, 528.0, 565.0, 684.0, 581.0, 645.0, 531.0, 497.0, 512.0, 564.0, 532.0, 710.0, 614.0, 465.0, 616.0, 448.0, 482.0, 562.0, 1.759536250540396, 13.334313252287963, 1.3859663837868694, 5.386965743859362, 6.859247612546815, 12.434563099200577, 1.305324791898028, 11.356445076318044, 7.669511527806312, 19.440139041383826, 2.2763095617802764, 5.2695308492634485, 1.3588400823983946, 1.3706594414055076, 1.6130489343736558, 4.37048617850048, 1.2531541128385195, 1.9666227354849366, 0.9127902315676957, 2.199058920795518, 2.07730592591961, 3.734606328869138, 0.9161441291428473, 8.30081276124147, 1.8949666574772916, 4.36555661667785, 1.5757471685845479, 0.9235181496765632, 0.9313430129173688, 8.64444892482548, 35.14651957323979, 15.943340324676173, 15.371513764939474, 9.56666547430424, 17.39752058118209, 10.276638596896, 36.11130337874854, 18.215938840494942, 24.729863138762425, 19.38818080685121, 25.02536784421679, 10.106640535445791, 81.94045895475195, 51.810590564956684, 10.117350416581766, 17.45885608285333, 139.08749149887967, 44.29578631604109, 21.26154096633718, 53.498744148617014, 58.66602184785406, 145.088075387056, 80.26853440270266, 170.18484417997152, 226.36896811084938, 77.63722179334357, 92.32558708707155, 59.507889151312334, 189.78715339980008, 116.25386560399278, 115.81396325953901, 81.90820982660051, 206.76433971173688, 123.55280336131227, 72.96364721924704, 58.555725024069524, 137.22059396229704, 112.86601477401993, 147.1859193039573, 138.68691073471808, 112.2121838107501, 83.2483768267531, 100.85524643117189, 162.49450835484456, 216.3923184883611, 227.55910406016076, 220.86601532714135, 139.973386645096, 125.88818556001057, 118.43269275426951, 93.42934927818528, 108.03963553601957, 109.75539988926339, 109.75684154125072, 104.98460782680363, 105.41476335021261, 100.20037079254894, 141.1960015511365, 115.30728526651251, 104.50504960594465, 103.18217720626329, 17.08061494293251, 38.47270642772381, 3.585802110670566, 10.12900522913452, 2.5605846467114692, 2.960273085126574, 3.1924047047786783, 4.045603303504662, 7.952631632424293, 4.737612112641267, 34.88681610133356, 4.23083008855226, 2.8329372700713917, 1.9464706160845389, 3.7142564176040773, 14.354318854945543, 0.9790112024861659, 1.8941427499148837, 2.2264956228485877, 3.6328884431578072, 17.07365901422476, 7.844069672254114, 2.500268936531983, 3.3692602352644943, 1.8253112425248044, 1.5057639541975523, 4.051502729758728, 1.8319162455233264, 0.5863406449501953, 1.206752152914323, 11.665887576477731, 5.52777819544227, 13.156975236364524, 5.222738965703605, 17.67008430292405, 37.75154053507418, 30.564458031240942, 72.7524386356321, 102.1523347249903, 99.21790795165377, 79.00996110641141, 86.03688677904863, 224.92727309350727, 100.55433318915586, 57.01751515708178, 223.27509280053317, 283.1601958259325, 129.86432553910984, 91.48364956489729, 101.50753980350319, 77.56427235534676, 52.52295761795156, 84.6539599362283, 83.59916696013983, 166.29756082234735, 192.8342989030415, 226.9050599920546, 63.180303481751835, 92.0624019070512, 78.08640451071346, 122.66453997164167, 73.3834337575098, 106.89396303708597, 111.24031816714208, 209.2890006648129, 128.7136873756858, 149.11649559309075, 81.73114902738676, 100.21899901233064, 112.65999997035955, 97.03065401881405, 117.26845592464885, 112.46095669624205, 108.56070607643431, 100.37520282560122, 94.2877841044648, 90.7365720927461, 95.13358688798893, 90.52638025902709, 3.246670049592425, 14.07001411545594, 3.871485961688837, 5.8496620876136625, 5.817901085465187, 1.624473003425965, 21.75339640357068, 1.6167986197070827, 1.205321450572246, 60.43687601304095, 1.165705649970512, 5.17193767757586, 2.3031825079468313, 1.8913949940053383, 20.335299693234198, 1.507761474669153, 1.4377054965557514, 5.78758854976916, 1.8973281574380276, 5.43824040870232, 1.0748331866886212, 2.538482078920425, 1.4485824384568688, 1.073921244909984, 1.0588378437343167, 1.4184914346066992, 3.2070228569699255, 3.2376443073122094, 1.0390131032947476, 2.052961501462795, 8.221587686427304, 10.819261816019647, 15.942727925277019, 8.731429571636058, 10.653612629670203, 30.46465899738361, 22.55714197148478, 17.13013404255885, 8.754824135448413, 55.93442902605184, 21.52796651252895, 14.295935981675042, 320.08166816376445, 127.83613897111651, 15.089280279036842, 167.6141580904339, 82.347125544441, 89.83795332464564, 41.789955039564816, 54.842834502499024, 52.20900534594728, 22.46723407988422, 54.784245979515624, 114.30252882393413, 34.244563779804665, 86.60101294228198, 186.22696573393674, 112.43072574025534, 96.86590339889912, 216.8579901419978, 53.47695123214986, 52.21468022241185, 221.70153381661373, 112.77252721092697, 108.88608806416681, 74.52037253661352, 115.37681955078328, 159.87379134723966, 93.69375729152367, 111.06141466146104, 136.9410480662695, 120.79023065876063, 132.79418700794497, 98.12170779537549, 74.3595264009492, 139.36195036605469, 105.09200295535426, 109.73246842977763, 106.48776628089469, 88.47581821867105, 76.31621922941936, 85.00524683347172, 81.72323694834752, 77.49780631006837, 1.736714107744543, 2.109494119528917, 2.5828175828247466, 1.960075201595748, 2.142499142537326, 1.7390191517558031, 2.1309071394836496, 3.301076346049809, 1.4627298529199406, 6.223541329146276, 1.150911896420222, 1.64493777972186, 1.717588199329553, 0.8381773741995816, 1.074371161095054, 9.281295679342335, 1.041256791234461, 0.546199893880751, 0.8344739065905038, 4.8714515275632015, 2.1685715132811616, 0.8294624506925541, 4.321518690861568, 2.6085560385817055, 1.3013633743081805, 1.3222114183499323, 4.950242272762904, 0.8082764207867708, 0.769176985497048, 6.183583247957429, 1.085228860358145, 1.9255370553327784, 5.164371647828781, 47.107835908591724, 14.959406252739669, 36.7672984058051, 20.94617217485824, 54.612635425790046, 11.095582773186292, 14.665585798731696, 7.223678550950501, 11.852710683726333, 38.04364845579079, 219.33946462229827, 18.980237383603892, 25.018731271837623, 10.327964985169633, 24.50630450849528, 13.826662732591513, 34.28345569853494, 52.133554923369765, 37.47173535589746, 46.416175123034286, 94.28112904838866, 67.21536863892827, 56.92734400859632, 59.58650774912426, 35.212962892522434, 111.67738421476393, 47.251602387830374, 143.5471144063841, 93.75846717233746, 108.17782983111647, 53.52506854216268, 59.73885213335912, 52.621463257379425, 51.585597956460845, 45.89864152159635, 68.15990118284377, 65.78245945554004, 77.56830964063482, 83.48251954627162, 107.34040072536202, 72.00323135149885, 73.49439509615316, 55.9844472358758, 120.69155202701221, 81.71064954953839, 74.02403538318934, 94.7816684381982, 69.02290900107441, 64.0902673460457, 64.33429406741806, 70.98150755639486, 65.14109863422794, 81.27919510931, 67.68465688389195, 63.33245016901424, 65.95290945239665, 60.73076927511603, 3.9996173259894414, 1.226574506023251, 2.9875978500867704, 2.821387748787875, 2.0555712155001107, 1.1409536522010753, 2.9893369017296996, 1.4366570298947101, 3.408475640006931, 0.8346098977311284, 0.8173248200206189, 1.0575262967902108, 4.377559566422082, 0.8049186883131946, 0.795720779968042, 4.970453066648492, 8.24371346537661, 1.0386355857038345, 1.0026324847154566, 0.7900867297698801, 0.7718945256519717, 0.7647226802107906, 0.7677107977932583, 0.7594750423004557, 0.5009256085941958, 0.9969081636588961, 1.2757411499996898, 1.4670748240032456, 0.7158271924180353, 0.4987679927263808, 9.174180902820162, 162.8107648592692, 3.1285368233790227, 4.440429316886048, 32.759399166191336, 4.92352517922988, 45.35849747332639, 17.187378232955528, 10.159882521088951, 8.514806817793842, 35.50651721663351, 30.234675524428567, 19.62346266576674, 14.080791512463074, 55.363782878260515, 7.36080301340563, 83.44385490139835, 80.39368594571032, 71.78984879433212, 33.304381862376744, 13.043475579699528, 66.42349316977531, 98.55532736490228, 87.46557116183645, 27.85773416259811, 121.57846261757004, 119.40810731781457, 55.34920602447582, 83.81203355574132, 143.79381397800685, 86.93690260394929, 48.75527825119533, 141.01792863228312, 56.968097582925665, 86.12041029820733, 59.8866664513204, 71.29730821457514, 61.92024436743835, 53.079929179036604, 49.171654769927265, 116.54492421067505, 69.00261612191218, 89.89092715592466, 75.3123580776574, 92.03213503123737, 58.56606004656654, 75.83256326397213, 63.673550853912346, 62.45969477062487, 59.334794306783735, 56.79513567301828, 56.32749512410129, 2.1257955923408978, 2.382699585786576, 7.944734898124714, 3.0612500416217574, 0.5412995064518704, 1.4504358908726898, 1.0712904870925282, 0.8949707515128021, 0.5231044740674654, 1.3794962658767462, 0.49157606111956365, 0.67381705694825, 0.8424241205012392, 0.8289780253988193, 4.546516284978654, 1.2635633215845008, 4.55420815757894, 8.241122320450563, 7.875416037847483, 1.1126490454419364, 0.3123371872123857, 1.9720327735712435, 1.5543252331530715, 5.918293271952026, 1.4362925255859307, 0.4595341776248772, 0.8650889496297156, 5.930202289439802, 0.5868748426400661, 0.2938785220977289, 2.1844176746436763, 5.953054741629813, 20.48786180838707, 5.893954027819554, 3.142078570398928, 13.786483344351698, 12.300766120489543, 28.33104265042345, 15.131015040268853, 10.249972036431108, 11.589248573731359, 4.894071275167658, 118.08836487472766, 7.027300582521729, 5.828609081489915, 62.16140667195174, 51.397632032010236, 9.515915741773604, 50.24346255974128, 82.31364110477182, 11.489200885516263, 19.683443745740224, 50.15720317836849, 78.28364603069012, 29.845556068457576, 23.77762633924441, 90.40060688554499, 46.13893017163566, 23.554349145588898, 48.89628224039009, 41.69644674497989, 52.75662838317801, 44.27975704357434, 42.43184399089557, 46.27016721156013, 47.829793362068294, 57.545488328990054, 34.79063389945775, 61.69683459795418, 48.63001042997054, 36.213878455433424, 45.82380075318456, 65.75202781013117, 41.80516754932543, 48.68912582572888, 36.22616430958865, 31.451949117590377, 34.445091314952556, 35.437637987546026, 32.748069536697685, 33.85746885528118, 0.4527561355810356, 0.4383667499716344, 0.5746533666539555, 0.27718441687013584, 0.40893828496394874, 1.7719123655991194, 0.38921564084483146, 0.6467383190985027, 0.6621598345181966, 0.5244780764903261, 1.8197349240984846, 0.38589157959115944, 3.4639805124693464, 1.6795129802188429, 0.5884758099439374, 0.6103345158492566, 0.24705801270348823, 0.37009731407269036, 0.3720649776713507, 2.4682651388843624, 1.6660551276906341, 1.137990417217828, 0.5879424525084355, 0.22851168177561915, 3.8548047534438723, 0.6272868808131503, 0.606963498927161, 2.3052586412336686, 0.689662011661954, 0.3537997490634165, 2.2296665425413043, 3.1430574695123674, 3.391093734081494, 3.6206134624630297, 5.024412759466542, 2.6248326036184952, 5.152610156804339, 8.52968457069013, 5.1485576509994955, 8.006136026927408, 32.83064533303999, 12.788386823339088, 45.9031000531484, 20.288633849101753, 25.27634159418147, 6.02244643757773, 6.459785216368899, 11.08894284638272, 74.3028451164357, 16.778450432848352, 46.25366644120107, 22.001841100994618, 46.49647820332658, 7.758738190998014, 28.285858827205455, 83.38227065020041, 26.13863801587454, 62.8752756767191, 39.27391310342763, 36.39529395564818, 28.34178113322612, 62.49742025446151, 43.15890129743584, 29.807626766117707, 32.188618583612545, 52.13866121405439, 44.69521664989091, 21.95938812321485, 45.27234266414261, 51.733611090055796, 44.56387553896762, 21.942374589543885, 19.360720776212137, 36.11812159354045, 32.64378282906769, 59.67468972253703, 60.31598202635254, 37.58807405762904, 28.533582615487855, 31.72164593508478, 34.662507663967425, 29.08697727703099, 32.221712433365, 29.475199163269878, 31.5259768218024, 29.924885387784627, 29.098745251904788, 2.6067602234882323, 11.234187090505278, 2.7421516809239157, 2.1865031390231153, 0.9727140256775448, 8.42481798847735, 0.336188997459823, 0.34833655494607596, 0.6683558432484775, 9.718237140764986, 2.3979630635590907, 0.21853309642137192, 1.3865840548997348, 0.44347148971892886, 2.2665040911747587, 10.284232713872159, 0.8923261453613035, 0.4256103186945552, 1.8983235296436862, 0.31490758443068967, 0.4239393679598343, 0.6200079358447957, 0.714447140720094, 0.9924079669955167, 4.274032485759863, 0.5234047851087236, 0.20786904190085717, 1.3776801681160369, 0.6301087906436555, 1.2841111234037306, 13.153310364136544, 2.8824877343116184, 2.771174890210525, 2.2859652589753825, 4.039744827210262, 1.0461111640239333, 3.992919496693322, 3.92118654340547, 2.3532517071047754, 3.2827865878802003, 3.672716314742972, 12.353169639887007, 6.978989219546439, 17.982702780040967, 2.3099388758669015, 4.696624499281996, 73.55709468942933, 29.524498275250377, 37.79970609547851, 15.677679724591933, 22.76740405087816, 22.999133427415174, 60.160932519673736, 24.067007202481705, 31.080007733133886, 29.25627741061631, 13.940569240930095, 33.58501389963112, 20.040334346792374, 35.44576326170524, 45.189556502457236, 18.388768045114354, 29.89903788726346, 31.489672680503503, 38.78282633061868, 44.790563167467916, 33.81577175932458, 52.80715018936096, 24.262952146903718, 22.23821923727079, 29.592079672604843, 28.13819992029688, 37.0019578513543, 29.318048890598856, 33.468278408888594, 26.253662783322657, 25.394666499884778, 26.776637562607025, 24.272727606934676, 25.16720823477424, 27.34293459369104, 25.29469755370894, 24.316751433662255, 0.8616882338764538, 0.3041539534004063, 0.9872678255221953, 0.37700108337277966, 0.7307711148142847, 0.3596019193483166, 0.6688367043707589, 0.5239917926787275, 0.8694150013216351, 0.8585796796427823, 0.5737678074104614, 1.103912469755988, 0.19321646909395476, 0.1285110293186618, 0.1942147056239223, 1.5091576133375977, 1.0594665775275498, 0.2511748420886788, 1.4713505794696489, 0.12380359162041984, 1.9647056996095893, 0.18584894605702926, 0.4447751167916113, 0.18703047776953313, 0.42743785689762853, 0.24208717423901926, 0.18290816970385368, 0.18328528966673951, 0.17887898903294247, 0.12069807953258906, 1.657617065938952, 7.325882616307919, 1.6244826737522837, 1.2429616455534083, 0.9644389028225206, 1.7916597658184903, 4.064804528861697, 34.077542294392266, 1.414820623651047, 25.887295582159386, 4.066267777428569, 4.019265350732662, 4.1356856779801205, 25.00654219671646, 12.003155528354274, 8.356817827713632, 38.30500097551329, 7.210629341102065, 37.513138242565994, 25.168252670619477, 28.90670711329849, 14.979004663015155, 25.56566162383574, 11.642561727966719, 16.849713998942253, 12.132751507150626, 12.10319901765744, 13.593873496167706, 10.874294349810759, 16.300113611236345, 12.62975191981366, 16.85275671076362, 15.678626138187337, 8.788675380119768, 8.67358284341043, 23.203893917130888, 16.589510822639074, 17.553362237576444, 11.47474696440025, 26.102090510517314, 15.907786409360625, 14.823844471505359, 12.898606298577194, 14.240753123821213, 15.858589864455238, 15.185715878815921, 13.795662944483615, 15.924032317121135, 18.0278411493474, 15.12939250592418, 13.609352688846245, 0.28519205442914475, 0.045664662009655084, 0.082112391438835, 0.07922645705729797, 0.6103040073171223, 0.06305565629416757, 0.0623207932889018, 0.08280177311031583, 0.030944492371108828, 0.050425566974419744, 0.0600037609413203, 0.08245628124194497, 0.12699344436023205, 0.018952745954564042, 0.01863300836773857, 0.018491312190271697, 0.01882079206584585, 0.018561528112713, 0.018776590219316575, 0.018513793062797535, 0.27133782704354814, 0.10934511942566504, 0.018109564935560894, 0.018050791872493182, 0.03687405631472814, 0.01809749532227781, 0.018080470131451233, 0.017604503037298918, 0.04294431189759668, 0.026501385195232124, 0.3001698751307977, 0.06472403406714382, 0.05989202338423214, 0.1735850007111546, 0.11344408566387736, 0.35629093132450185, 0.5727827036472749, 0.4156706748569858, 0.67691438230094, 0.14105164496288317, 0.3417403684382109, 0.07388481642517772, 2.2182539916466393, 0.341590796161529, 1.1870226302581117, 0.5319553607854229, 0.8937074468105077, 1.6385383116136925, 1.744869166900517, 0.6753963518578395, 1.5283518308120674, 1.7672663510010491, 5.189890031633009, 3.145993669143886, 0.6453229023690785, 1.1175485592905405, 5.7321494605055126, 3.166169784605964, 1.5556585049354528, 2.225652242423324, 2.215882034440046, 3.1948430463763287, 3.576481940856811, 1.7927174698402066, 3.0538189049770597, 3.2222831508864163, 2.9656667275046558, 2.1712956402892822, 3.7861377959692857, 4.835771050492044, 4.762884528559896, 2.275858369756062, 2.754539397099581, 2.6853185390413534, 2.873838632940662, 2.873596712426455, 3.4123520348818346, 5.178347125448834, 4.891779113455887, 5.035028757317468, 2.511824951308004, 3.023470766244647, 2.9093441401609437, 3.5813802077630257, 3.1571494470825314, 2.9997783513498373, 3.0451482853103613, 2.539650521379675, 2.4540605398687365, 2.388083578190015], \"Total\": [1201.0, 1002.0, 1307.0, 1300.0, 819.0, 982.0, 632.0, 966.0, 701.0, 845.0, 684.0, 541.0, 504.0, 528.0, 565.0, 684.0, 581.0, 645.0, 531.0, 497.0, 512.0, 564.0, 532.0, 710.0, 614.0, 465.0, 616.0, 448.0, 482.0, 562.0, 4.4707108229866, 34.16398324680882, 3.5855550082817973, 14.058030386444559, 17.935618337161163, 33.363725380661215, 3.562210191076511, 31.320986723772876, 21.219096866385847, 54.097888621467476, 6.377763044231628, 14.816984821059906, 3.8375534682381183, 3.9068473186291284, 4.601537717819476, 12.488958530276468, 3.5841598690212466, 5.63694159699075, 2.619282685669012, 6.3140176267239365, 5.966068614693144, 10.734772101809721, 2.6419398374112086, 24.02396830308158, 5.484638037214782, 12.689057637280627, 4.607721905264098, 2.7036345413246767, 2.730154133195706, 25.379313872996796, 103.88818313881174, 47.213284656485776, 45.70316478787494, 28.407914382983744, 52.53810506975517, 30.706808620146358, 111.50628937862214, 55.54598518681521, 77.10380579329833, 59.89845677236882, 78.90285066096699, 30.996515568845222, 278.54785348014474, 173.9260876791097, 31.056996784072343, 55.79462055939499, 531.8083391119767, 154.40182087340676, 69.6888731665533, 190.50817573471747, 211.5064003799884, 581.6997458125755, 303.3510053173974, 710.7730925477805, 1002.8862198642925, 299.7682303197394, 366.4439026187772, 223.64197015614954, 845.8078737057346, 483.7306135041473, 482.76841793559043, 325.08796307241437, 982.5037902253888, 532.9015670751979, 287.60464683460475, 221.8818506402176, 616.5521304494348, 491.5196883623716, 684.0931666470874, 645.9927410793306, 497.5576423272086, 345.1457014755655, 443.25422343955375, 819.0649597365275, 1201.3681744453804, 1300.1600086349017, 1307.3840053699932, 701.2522003817533, 611.6597240899912, 565.2727644947116, 405.8958540569046, 505.3895011400986, 528.523178171577, 541.7173399040921, 504.60534725770765, 512.1011120175363, 465.45136813062413, 966.2796282894926, 684.7764592816566, 632.119296674511, 614.7435263104817, 44.699187870533805, 102.94288360897833, 9.600389115700159, 27.16003312034318, 6.894177126284799, 7.980848920447236, 8.78481785288757, 11.196902756462931, 22.11011527695428, 13.246816139711369, 97.86939372867063, 11.948086029465303, 8.039281316421505, 5.633130084380222, 10.7513502383796, 42.0352640341491, 2.886519857008213, 5.632255693467205, 6.680459045334358, 10.915972546454544, 51.46942027014486, 24.11681466075779, 7.774168524001736, 10.489101796871077, 5.692694366069373, 4.725901074193145, 12.74585276217099, 5.779994456063246, 1.8556435114160108, 3.824379351307755, 37.07336419886907, 17.596610193618524, 42.84263051243876, 16.78844896512183, 59.2380666922051, 131.80973358495743, 106.14959925410535, 265.018514462107, 380.3421077803152, 379.87514939820414, 303.3510053173974, 338.3293572848694, 966.2796282894926, 405.42362133194507, 223.4436051932664, 1002.8862198642925, 1307.3840053699932, 564.1336405481602, 384.11753130262167, 442.5659981759836, 326.1200240476253, 210.91064924825486, 368.09011592537547, 366.08055968196186, 819.0649597365275, 982.5037902253888, 1201.3681744453804, 267.6010005032036, 425.4266701272935, 348.4112912358294, 616.5521304494348, 325.08796307241437, 528.523178171577, 562.6925801420424, 1300.1600086349017, 684.7764592816566, 845.8078737057346, 376.89506150228243, 512.1011120175363, 614.7435263104817, 505.3895011400986, 701.2522003817533, 710.7730925477805, 684.0931666470874, 611.6597240899912, 531.8083391119767, 482.76841793559043, 581.6997458125755, 645.9927410793306, 7.179629710749052, 32.18484211082876, 9.194718167561392, 14.250186631233653, 14.310569836388186, 4.003465550547467, 53.80260979269923, 4.005462883976031, 2.9892744058477794, 154.2919517741156, 2.987085097268009, 13.392488778455782, 5.984040636911832, 4.95656829314438, 54.469483787261545, 4.051697760540509, 3.86612585762158, 15.58387358213297, 5.119263046005624, 14.720680616865371, 2.910338713475195, 6.925352456372768, 3.9693249786408105, 2.95620486555128, 2.917991184520029, 3.911640166127515, 8.84453906383336, 9.001983464758432, 2.919299869971161, 5.769889341365485, 23.318114444157402, 31.379472317775345, 47.514255336563224, 25.572777410474057, 31.542119257263977, 94.49840636161667, 69.10385893593794, 52.687924700975444, 26.082856639871498, 189.12093019590614, 68.73753829444469, 44.5315646577404, 1307.3840053699932, 491.5196883623716, 48.18508144839604, 684.7764592816566, 312.3613259062674, 353.2498329560959, 152.72179370872144, 212.87539089470735, 203.4412914988566, 76.99339046118038, 220.52630059457158, 531.8083391119767, 126.84927558823111, 384.11753130262167, 966.2796282894926, 532.9015670751979, 448.3649670067919, 1201.3681744453804, 219.41163336749474, 215.04009925629504, 1300.1600086349017, 565.2727644947116, 541.7173399040921, 337.3217764129737, 611.6597240899912, 982.5037902253888, 483.7306135041473, 616.5521304494348, 845.8078737057346, 710.7730925477805, 819.0649597365275, 562.6925801420424, 368.09011592537547, 1002.8862198642925, 645.9927410793306, 701.2522003817533, 684.0931666470874, 528.523178171577, 405.8958540569046, 614.7435263104817, 581.6997458125755, 442.5659981759836, 4.855761973816758, 5.902929406508219, 7.486507310412329, 5.759695283398252, 6.755528763474532, 5.624909917246975, 6.901547511320327, 10.768418406738048, 4.781610194716162, 20.43562179110548, 3.793657573667923, 5.423118706469067, 5.6734520236121435, 2.7778375568100575, 3.5652190415589007, 31.056996784072343, 3.512732228336298, 1.845349339655782, 2.8204922752770836, 16.525653499148373, 7.411318412734383, 2.84718905684087, 15.008854391942993, 9.143642942220612, 4.584971667912089, 4.662219323808413, 17.479782991946824, 2.8560290424318415, 2.722792964869949, 21.91964971638287, 3.8496609162928275, 6.884411424041085, 19.100883700611497, 190.50817573471747, 59.02986012970293, 152.75954403827694, 84.87779876832998, 236.3323078062086, 44.461155219284784, 60.93698896106301, 28.3333290722874, 48.89728585669278, 178.61989967474912, 1300.1600086349017, 83.2355264132163, 116.75329298917308, 43.11916630515899, 115.50688528897639, 61.30552226068605, 177.9978142500325, 298.3225278008414, 205.0135402725304, 271.24614712900217, 645.9927410793306, 431.4727655063024, 355.61946957492665, 376.89506150228243, 196.9110116520071, 819.0649597365275, 287.60464683460475, 1201.3681744453804, 701.2522003817533, 845.8078737057346, 345.55336362778485, 398.005696277654, 338.74366279672154, 333.3217097044715, 290.12436744711624, 505.3895011400986, 482.76841793559043, 614.7435263104817, 684.0931666470874, 1002.8862198642925, 562.6925801420424, 581.6997458125755, 389.3404206286453, 1307.3840053699932, 710.7730925477805, 611.6597240899912, 982.5037902253888, 565.2727644947116, 497.5576423272086, 504.60534725770765, 632.119296674511, 528.523178171577, 966.2796282894926, 616.5521304494348, 541.7173399040921, 684.7764592816566, 564.1336405481602, 11.083487527545243, 3.4893796573996765, 8.527040512752187, 8.169383319851365, 6.0517178073692595, 3.4092454829943484, 8.978685397339465, 4.340695986220672, 10.40078330213997, 2.575710343305288, 2.5303085005907873, 3.3629004093986152, 13.995817072794173, 2.589216666618732, 2.565687608753126, 16.055858062772284, 26.639336605952803, 3.379871040837429, 3.266813177583377, 2.5765406773101422, 2.542171680937593, 2.573266463068972, 2.5852185175310227, 2.6017073122702192, 1.7164618635428743, 3.421396101019067, 4.390920587448316, 5.103919066645051, 2.491507980553203, 1.7441802921375675, 32.29345910003911, 632.119296674511, 11.406792389141792, 16.671041088785138, 143.3792162731013, 18.827109712357938, 205.0135402725304, 72.70048726480327, 41.710673570757805, 35.01394233393502, 182.95104661465538, 152.75954403827694, 94.54975101761008, 65.234623597814, 316.36818690437957, 31.339732135775993, 512.1011120175363, 497.5576423272086, 442.5659981759836, 184.75733856185437, 61.28142387914832, 431.4727655063024, 701.2522003817533, 614.7435263104817, 157.63013404107943, 982.5037902253888, 966.2796282894926, 369.9710649464159, 645.9927410793306, 1300.1600086349017, 684.0931666470874, 326.1200240476253, 1307.3840053699932, 400.0561426701232, 710.7730925477805, 443.25422343955375, 562.6925801420424, 465.45136813062413, 379.87514939820414, 345.1457014755655, 1201.3681744453804, 564.1336405481602, 845.8078737057346, 684.7764592816566, 1002.8862198642925, 482.76841793559043, 819.0649597365275, 611.6597240899912, 616.5521304494348, 541.7173399040921, 581.6997458125755, 565.2727644947116, 9.61368077712953, 10.913705117813645, 36.790537569003625, 14.268977996932023, 2.5625542586179564, 6.8668007182862345, 5.121655050382322, 4.419289103004425, 2.5883587084822866, 7.023176882322902, 2.530182833007006, 3.5128612725987245, 4.451015084066743, 4.385205772438713, 24.19140434911983, 6.788093139389529, 24.54514341110568, 44.41635631656899, 42.808824353518716, 6.064278972140128, 1.711038333039631, 10.803973626639866, 8.63723883636945, 33.28583379124503, 8.082564853301376, 2.6012421920172244, 4.920299484812902, 33.821022268349104, 3.3531587569606107, 1.6827713443387948, 12.555354450164243, 35.01394233393502, 124.51670696175115, 35.38855910972033, 18.832849642512, 88.2042734077812, 81.75429811000974, 205.0135402725304, 109.20426086681283, 73.10861098305627, 84.87779876832998, 32.093285295756075, 1201.3681744453804, 48.8650346357196, 39.859827681104115, 632.119296674511, 512.1011120175363, 71.0740440715499, 528.523178171577, 966.2796282894926, 91.0596586759236, 176.32691517047897, 565.2727644947116, 1002.8862198642925, 305.3076294764147, 229.34344553446002, 1300.1600086349017, 562.6925801420424, 230.627082728955, 611.6597240899912, 497.5576423272086, 684.0931666470874, 564.1336405481602, 541.7173399040921, 614.7435263104817, 645.9927410793306, 845.8078737057346, 417.0918643013587, 982.5037902253888, 710.7730925477805, 465.45136813062413, 684.7764592816566, 1307.3840053699932, 616.5521304494348, 819.0649597365275, 504.60534725770765, 398.005696277654, 483.7306135041473, 701.2522003817533, 482.76841793559043, 581.6997458125755, 2.5062391540857436, 2.5148336828145017, 3.4429415111794475, 1.6701513441040752, 2.572619240555451, 11.434996963464162, 2.5174512842299843, 4.190532162592444, 4.326486612781541, 3.430483626985436, 11.973771019109982, 2.547760271399489, 23.31357546564252, 11.336792834754343, 4.04627761690063, 4.222136677220611, 1.7141588473859275, 2.5741562884272797, 2.6002336004691577, 17.333473744813848, 11.704610265452125, 8.190470159258517, 4.250293244992207, 1.6764274485405701, 28.331295799195107, 4.649849674848917, 4.501320011340278, 17.109051615078442, 5.121595863061008, 2.636746894234492, 16.74885461602152, 23.72216966462724, 25.86202085789555, 27.77047080365578, 39.385601454714674, 20.60681712491225, 42.03772742400264, 71.87048863786976, 42.47859017230866, 69.65749281280176, 346.1069586395243, 121.7792996687717, 504.60534725770765, 209.13797236033923, 269.6589543526603, 53.334462574696104, 58.498990856587646, 108.97941521441462, 982.5037902253888, 177.9978142500325, 581.6997458125755, 247.83904289854337, 614.7435263104817, 73.61255841016306, 353.38676554220194, 1300.1600086349017, 326.47964564815015, 966.2796282894926, 541.7173399040921, 497.5576423272086, 366.08055968196186, 1002.8862198642925, 632.119296674511, 398.005696277654, 443.25422343955375, 819.0649597365275, 684.0931666470874, 273.3836229990043, 701.2522003817533, 845.8078737057346, 710.7730925477805, 278.54785348014474, 235.89212066600678, 564.1336405481602, 491.5196883623716, 1201.3681744453804, 1307.3840053699932, 645.9927410793306, 417.0918643013587, 505.3895011400986, 611.6597240899912, 448.3649670067919, 616.5521304494348, 483.7306135041473, 684.7764592816566, 531.8083391119767, 528.523178171577, 16.69645839308896, 73.10861098305627, 18.481908154179468, 15.270773916099387, 6.937570987197975, 60.1340006321825, 2.4609310337203927, 2.556151598332569, 4.942610991015162, 71.87048863786976, 18.106601377500162, 1.6623204585773979, 10.763434046080876, 3.4438819367537636, 17.624851763328802, 80.14808120473, 6.95743083116443, 3.329343645936627, 14.885959800130038, 2.495830871268951, 3.3629084951732584, 4.924843950374174, 5.6778842177073665, 7.9250015250425125, 34.25218875123147, 4.205827621850885, 1.6821446067789338, 11.178327034742198, 5.120517920342827, 10.52432521912324, 109.20426086681283, 23.85491662990649, 23.514474927030463, 19.288874389134683, 35.7452661160992, 8.622146413463094, 35.90040420683022, 35.35747939859874, 20.8787359712381, 30.53096599950699, 34.69017303173722, 135.25567610244966, 73.51336722163622, 227.70303490217435, 21.14013497895301, 48.89728585669278, 1300.1600086349017, 448.3649670067919, 614.7435263104817, 210.40405294571323, 346.77047267090734, 355.61946957492665, 1201.3681744453804, 382.4210229710486, 531.8083391119767, 497.5576423272086, 197.4162703268291, 632.119296674511, 318.92762789104427, 684.0931666470874, 966.2796282894926, 287.60464683460475, 565.2727644947116, 611.6597240899912, 819.0649597365275, 1002.8862198642925, 684.7764592816566, 1307.3840053699932, 431.4727655063024, 379.87514939820414, 581.6997458125755, 541.7173399040921, 982.5037902253888, 645.9927410793306, 845.8078737057346, 532.9015670751979, 505.3895011400986, 564.1336405481602, 465.45136813062413, 512.1011120175363, 710.7730925477805, 562.6925801420424, 504.60534725770765, 8.978685397339465, 3.1859060892312265, 10.40078330213997, 4.129346838342631, 8.169383319851365, 4.031107922638357, 7.503234831455379, 5.889131823812355, 10.13191196323261, 10.02648557565141, 6.755955694534931, 13.995817072794173, 2.4498641709510296, 1.6315951028462203, 2.4969778685742745, 19.440243401082107, 13.941978796709154, 3.3077471335263673, 19.47981677525772, 1.6442520781464867, 26.135096048783005, 2.475042445535877, 5.935850165001606, 2.499395200924586, 5.74039076836592, 3.266813177583377, 2.477119870875077, 2.498412261741679, 2.4564832484042025, 1.6579807601971432, 23.822572647243422, 109.20426086681283, 23.514474927030463, 17.96654873668662, 13.87013101090637, 26.639336605952803, 64.50449097161409, 701.2522003817533, 22.540208714115057, 564.1336405481602, 73.10861098305627, 73.61255841016306, 77.03881409778643, 632.119296674511, 269.43400849752766, 184.75733856185437, 1201.3681744453804, 166.1576228495671, 1307.3840053699932, 819.0649597365275, 982.5037902253888, 425.4266701272935, 845.8078737057346, 316.36818690437957, 528.523178171577, 345.55336362778485, 346.1069586395243, 405.8958540569046, 305.3076294764147, 532.9015670751979, 380.8448837615362, 581.6997458125755, 541.7173399040921, 238.6802664185345, 237.7948381376578, 1002.8862198642925, 616.5521304494348, 684.7764592816566, 363.22865237579447, 1300.1600086349017, 614.7435263104817, 565.2727644947116, 448.3649670067919, 531.8083391119767, 645.9927410793306, 611.6597240899912, 505.3895011400986, 710.7730925477805, 966.2796282894926, 684.0931666470874, 504.60534725770765, 18.03912948048567, 3.0957318184818714, 5.633094582206138, 5.5239520405180365, 44.41635631656899, 4.846280717198249, 4.893083652284062, 6.53652933556137, 2.483318298583205, 4.060401797251727, 4.920299484812902, 6.937570987197975, 10.803973626639866, 1.6459197198208577, 1.636110040686945, 1.6309354660690205, 1.6609728021146106, 1.6428756697236608, 1.6619710969627126, 1.6416680423341534, 24.19140434911983, 9.853398605503283, 1.6504877827007267, 1.646366222684529, 3.3820958387754803, 1.6637099001886597, 1.663484423313286, 1.6354282452633222, 3.9940854731074893, 2.4707815206684005, 28.407914382983744, 6.092321355112091, 5.631419264373472, 16.69645839308896, 10.913705117813645, 35.7452661160992, 58.75147559424054, 42.808824353518716, 71.87048863786976, 13.85208499669005, 35.38855910972033, 7.079962385999898, 273.6008410663631, 36.790537569003625, 142.7800424161113, 59.89845677236882, 105.4765165892671, 206.4717391226634, 221.8818506402176, 79.1178330981196, 198.86606218448463, 236.72644354257105, 819.0649597365275, 465.45136813062413, 76.0285953631979, 146.0197429455737, 1002.8862198642925, 504.60534725770765, 217.54249698385735, 338.3293572848694, 338.74366279672154, 541.7173399040921, 632.119296674511, 271.24614712900217, 528.523178171577, 565.2727644947116, 512.1011120175363, 345.1457014755655, 701.2522003817533, 966.2796282894926, 982.5037902253888, 382.4210229710486, 497.5576423272086, 482.76841793559043, 531.8083391119767, 532.9015670751979, 684.0931666470874, 1307.3840053699932, 1201.3681744453804, 1300.1600086349017, 448.3649670067919, 616.5521304494348, 581.6997458125755, 845.8078737057346, 684.7764592816566, 645.9927410793306, 710.7730925477805, 491.5196883623716, 562.6925801420424, 614.7435263104817], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -10.1464, -8.1211, -10.3851, -9.0275, -8.7859, -8.191, -10.445, -8.2817, -8.6742, -7.7442, -9.8889, -9.0495, -10.4049, -10.3962, -10.2334, -9.2366, -10.4858, -10.0352, -10.8027, -9.9235, -9.9804, -9.3938, -10.7991, -8.5951, -10.0723, -9.2377, -10.2568, -10.7911, -10.7826, -8.5546, -7.152, -7.9424, -7.979, -8.4532, -7.8552, -8.3816, -7.1249, -7.8092, -7.5035, -7.7468, -7.4916, -8.3983, -6.3055, -6.7639, -8.3972, -7.8516, -5.7764, -6.9206, -7.6546, -6.7318, -6.6396, -5.7341, -6.3261, -5.5746, -5.2893, -6.3594, -6.1862, -6.6254, -5.4656, -5.9557, -5.9595, -6.3059, -5.3799, -5.8948, -6.4215, -6.6415, -5.7899, -5.9853, -5.7198, -5.7793, -5.9911, -6.2897, -6.0978, -5.6208, -5.3344, -5.2841, -5.3139, -5.77, -5.8761, -5.9371, -6.1743, -6.029, -6.0132, -6.0132, -6.0577, -6.0536, -6.1043, -5.7613, -5.9639, -6.0623, -6.075, -7.7824, -6.9704, -9.3434, -8.305, -9.6801, -9.5351, -9.4596, -9.2227, -8.5469, -9.0648, -7.0683, -9.178, -9.579, -9.9543, -9.3082, -7.9563, -10.6416, -9.9816, -9.8199, -9.3303, -7.7828, -8.5606, -9.704, -9.4057, -10.0186, -10.2111, -9.2213, -10.015, -11.1542, -10.4324, -8.1637, -8.9106, -8.0434, -8.9673, -7.7485, -6.9893, -7.2005, -6.3333, -5.9939, -6.023, -6.2508, -6.1656, -5.2046, -6.0097, -6.577, -5.212, -4.9743, -5.7539, -6.1042, -6.0002, -6.2693, -6.6591, -6.1818, -6.1943, -5.5066, -5.3585, -5.1958, -6.4744, -6.0979, -6.2625, -5.8109, -6.3247, -5.9485, -5.9087, -5.2766, -5.7628, -5.6156, -6.2169, -6.013, -5.896, -6.0453, -5.8559, -5.8978, -5.9331, -6.0114, -6.074, -6.1124, -6.0651, -6.1147, -9.4017, -7.9352, -9.2256, -8.8129, -8.8183, -10.0941, -7.4995, -10.0988, -10.3925, -6.4777, -10.426, -8.936, -9.745, -9.942, -7.5669, -10.1687, -10.2162, -8.8236, -9.9388, -8.8858, -10.5071, -9.6477, -10.2087, -10.508, -10.5221, -10.2297, -9.4139, -9.4044, -10.541, -9.86, -8.4725, -8.198, -7.8103, -8.4124, -8.2134, -7.1627, -7.4632, -7.7384, -8.4097, -6.5551, -7.5099, -7.9193, -4.8107, -5.7285, -7.8653, -5.4576, -6.1683, -6.0813, -6.8466, -6.5748, -6.624, -7.4672, -6.5759, -5.8404, -7.0458, -6.118, -5.3523, -5.8569, -6.006, -5.2, -6.6, -6.6239, -5.178, -5.8539, -5.889, -6.2682, -5.8311, -5.5049, -6.0393, -5.8692, -5.6597, -5.7852, -5.6905, -5.9931, -6.2704, -5.6422, -5.9244, -5.8812, -5.9113, -6.0966, -6.2444, -6.1366, -6.1759, -6.229, -9.6996, -9.5052, -9.3028, -9.5787, -9.4897, -9.6983, -9.4951, -9.0574, -9.8713, -8.4233, -10.1111, -9.7539, -9.7107, -10.4282, -10.1799, -8.0236, -10.2112, -10.8564, -10.4326, -8.6682, -9.4776, -10.4386, -8.788, -9.2928, -9.9882, -9.9723, -8.6522, -10.4645, -10.5141, -8.4297, -10.1698, -9.5964, -8.6099, -6.3992, -7.5463, -6.647, -7.2097, -6.2514, -7.8451, -7.5661, -8.2743, -7.7791, -6.6129, -4.861, -7.3082, -7.032, -7.9168, -7.0527, -7.625, -6.717, -6.2978, -6.6281, -6.414, -5.7054, -6.0437, -6.2099, -6.1642, -6.6902, -5.536, -6.3962, -5.285, -5.7109, -5.5679, -6.2715, -6.1617, -6.2885, -6.3084, -6.4252, -6.0298, -6.0653, -5.9005, -5.827, -5.5756, -5.9749, -5.9544, -6.2266, -5.4584, -5.8485, -5.9473, -5.7001, -6.0172, -6.0913, -6.0875, -5.9892, -6.0751, -5.8538, -6.0368, -6.1032, -6.0627, -6.1452, -8.8169, -9.9988, -9.1086, -9.1658, -9.4825, -10.0712, -9.108, -9.8407, -8.9768, -10.3839, -10.4048, -10.1471, -8.7266, -10.4201, -10.4316, -8.5996, -8.0936, -10.1652, -10.2004, -10.4387, -10.462, -10.4713, -10.4674, -10.4782, -10.8944, -10.2062, -9.9595, -9.8198, -10.5374, -10.8987, -7.9867, -5.1105, -9.0625, -8.7123, -6.7139, -8.609, -6.3885, -7.3589, -7.8846, -8.0613, -6.6334, -6.7941, -7.2263, -7.5583, -6.1891, -8.2069, -5.7789, -5.8161, -5.9293, -6.6974, -7.6348, -6.007, -5.6124, -5.7318, -6.876, -5.4025, -5.4205, -6.1894, -5.7745, -5.2347, -5.7379, -6.3163, -5.2542, -6.1606, -5.7473, -6.1106, -5.9362, -6.0772, -6.2313, -6.3077, -5.4448, -5.9689, -5.7045, -5.8814, -5.6809, -6.1329, -5.8745, -6.0493, -6.0685, -6.1199, -6.1636, -6.1719, -8.9759, -8.8618, -7.6576, -8.6112, -10.3438, -9.3582, -9.6612, -9.841, -10.378, -9.4083, -10.4402, -10.1249, -9.9015, -9.9176, -8.2157, -9.4961, -8.214, -7.6209, -7.6663, -9.6233, -10.8937, -9.051, -9.289, -7.952, -9.368, -10.5076, -9.875, -7.95, -10.263, -10.9547, -8.9487, -7.9462, -6.7102, -7.9561, -8.5852, -7.1064, -7.2204, -6.3861, -7.0133, -7.4028, -7.28, -8.142, -4.9586, -7.7803, -7.9673, -5.6003, -5.7905, -7.4771, -5.8132, -5.3195, -7.2887, -6.7503, -5.8149, -5.3697, -6.334, -6.5613, -5.2258, -5.8984, -6.5708, -5.8404, -5.9996, -5.7644, -5.9395, -5.9822, -5.8956, -5.8624, -5.6775, -6.1807, -5.6078, -5.8458, -6.1406, -5.9053, -5.5442, -5.997, -5.8446, -6.1403, -6.2816, -6.1907, -6.1623, -6.2412, -6.2079, -10.3722, -10.4045, -10.1338, -10.8629, -10.474, -9.0078, -10.5234, -10.0156, -9.9921, -10.2252, -8.9811, -10.532, -8.3374, -9.0613, -10.11, -10.0736, -10.978, -10.5738, -10.5685, -8.6763, -9.0694, -9.4506, -10.1109, -11.056, -8.2305, -10.0462, -10.0791, -8.7446, -9.9514, -10.6188, -8.778, -8.4346, -8.3587, -8.2932, -7.9655, -8.6148, -7.9403, -7.4363, -7.9411, -7.4996, -6.0885, -7.0313, -5.7533, -6.5698, -6.35, -7.7843, -7.7142, -7.1739, -5.2717, -6.7597, -5.7457, -6.4887, -5.7404, -7.531, -6.2375, -5.1564, -6.3164, -5.4387, -5.9093, -5.9854, -6.2355, -5.4447, -5.8149, -6.1851, -6.1082, -5.6259, -5.78, -6.4906, -5.7671, -5.6337, -5.7829, -6.4914, -6.6166, -5.993, -6.0942, -5.4909, -5.4802, -5.9531, -6.2287, -6.1228, -6.0342, -6.2095, -6.1072, -6.1963, -6.129, -6.1811, -6.2091, -8.4152, -6.9544, -8.3646, -8.591, -9.401, -7.2422, -10.4634, -10.4279, -9.7763, -7.0993, -8.4987, -10.8942, -9.0465, -10.1865, -8.5551, -7.0427, -9.4873, -10.2276, -8.7324, -10.5288, -10.2315, -9.8514, -9.7096, -9.381, -7.9208, -10.0207, -10.9442, -9.0529, -9.8352, -9.1233, -6.7967, -8.3147, -8.3541, -8.5466, -7.9772, -9.3283, -7.9888, -8.007, -8.5176, -8.1847, -8.0724, -6.8594, -7.4304, -6.4839, -8.5361, -7.8265, -5.0753, -5.9881, -5.741, -6.6211, -6.248, -6.2379, -5.2763, -6.1925, -5.9368, -5.9973, -6.7385, -5.8593, -6.3756, -5.8053, -5.5625, -6.4616, -5.9755, -5.9237, -5.7154, -5.5714, -5.8524, -5.4067, -6.1844, -6.2715, -5.9858, -6.0362, -5.7624, -5.9951, -5.8628, -6.1055, -6.1388, -6.0858, -6.184, -6.1478, -6.0649, -6.1428, -6.1822, -8.9257, -9.9671, -8.7896, -9.7523, -9.0905, -9.7996, -9.179, -9.4231, -8.9168, -8.9293, -9.3324, -8.678, -10.4208, -10.8286, -10.4156, -8.3653, -8.7191, -10.1584, -8.3907, -10.8659, -8.1015, -10.4597, -9.587, -10.4533, -9.6268, -10.1953, -10.4756, -10.4735, -10.4979, -10.8913, -8.2715, -6.7854, -8.2916, -8.5593, -8.813, -8.1937, -7.3745, -5.2482, -8.4298, -5.5231, -7.3741, -7.3857, -7.3572, -5.5577, -6.2917, -6.6538, -5.1313, -6.8013, -5.1521, -5.5512, -5.4128, -6.0702, -5.5356, -6.3222, -5.9525, -6.2809, -6.2834, -6.1672, -6.3904, -5.9857, -6.2408, -5.9523, -6.0245, -6.6034, -6.6166, -5.6325, -5.9681, -5.9116, -6.3367, -5.5148, -6.01, -6.0806, -6.2197, -6.1207, -6.0131, -6.0565, -6.1525, -6.009, -5.8849, -6.0602, -6.1661, -8.3365, -10.1683, -9.5815, -9.6173, -7.5757, -9.8456, -9.8573, -9.5732, -10.5574, -10.0691, -9.8952, -9.5774, -9.1455, -11.0477, -11.0647, -11.0723, -11.0547, -11.0685, -11.057, -11.0711, -8.3863, -9.2951, -11.0932, -11.0964, -10.3821, -11.0939, -11.0948, -11.1215, -10.2297, -10.7124, -8.2853, -9.8195, -9.8971, -8.833, -9.2583, -8.1139, -7.6391, -7.9597, -7.4721, -9.0405, -8.1556, -9.6871, -6.2852, -8.156, -6.9104, -7.7131, -7.1942, -6.5881, -6.5252, -7.4743, -6.6577, -6.5124, -5.4352, -5.9357, -7.5199, -6.9707, -5.3358, -5.9294, -6.64, -6.2818, -6.2862, -5.9203, -5.8075, -6.4981, -5.9655, -5.9118, -5.9948, -6.3065, -5.7505, -5.5058, -5.521, -6.2595, -6.0686, -6.0941, -6.0262, -6.0263, -5.8545, -5.4374, -5.4943, -5.4655, -6.1609, -5.9755, -6.0139, -5.8061, -5.9322, -5.9833, -5.9683, -6.1498, -6.1841, -6.2114], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.707, 0.6986, 0.689, 0.6803, 0.6783, 0.6525, 0.6355, 0.625, 0.6218, 0.616, 0.6092, 0.6056, 0.6013, 0.592, 0.5912, 0.5895, 0.5886, 0.5865, 0.5853, 0.5847, 0.5845, 0.5836, 0.5804, 0.5768, 0.5767, 0.5725, 0.5665, 0.5653, 0.564, 0.5625, 0.5557, 0.5538, 0.5498, 0.5511, 0.5343, 0.5449, 0.512, 0.5246, 0.5023, 0.5115, 0.4912, 0.5188, 0.4159, 0.4284, 0.5179, 0.4776, 0.2983, 0.3908, 0.4523, 0.3694, 0.3571, 0.2509, 0.31, 0.21, 0.151, 0.2885, 0.261, 0.3155, 0.1451, 0.2137, 0.2119, 0.261, 0.081, 0.1778, 0.2679, 0.3073, 0.1369, 0.1682, 0.1031, 0.1009, 0.1502, 0.2173, 0.159, 0.022, -0.0746, -0.1034, -0.1387, 0.0281, 0.0587, 0.0765, 0.1706, 0.0966, 0.0676, 0.043, 0.0695, 0.0589, 0.1036, -0.2838, -0.142, -0.1604, -0.1452, 0.7686, 0.7464, 0.7458, 0.7443, 0.7402, 0.7388, 0.7184, 0.7126, 0.7081, 0.7024, 0.6991, 0.6924, 0.6876, 0.668, 0.6678, 0.6562, 0.6493, 0.6409, 0.6319, 0.6304, 0.6272, 0.6075, 0.5962, 0.595, 0.5932, 0.5869, 0.5845, 0.5816, 0.5785, 0.5771, 0.5744, 0.5727, 0.55, 0.5629, 0.5209, 0.4803, 0.4856, 0.4379, 0.416, 0.3881, 0.3853, 0.3614, 0.2729, 0.3364, 0.3648, 0.2284, 0.2008, 0.2618, 0.2958, 0.2582, 0.2944, 0.3404, 0.2609, 0.2538, 0.1362, 0.1023, 0.0639, 0.2871, 0.2, 0.235, 0.1159, 0.2422, 0.1324, 0.1096, -0.0959, 0.0591, -0.005, 0.2021, 0.0994, 0.0338, 0.0803, -0.0578, -0.1131, -0.1102, -0.0767, 0.0007, 0.059, -0.0801, -0.2345, 0.9781, 0.9442, 0.9067, 0.8813, 0.8716, 0.8697, 0.8661, 0.8645, 0.8634, 0.8344, 0.8307, 0.8202, 0.8169, 0.8083, 0.7864, 0.7832, 0.7825, 0.7812, 0.7791, 0.7759, 0.7756, 0.7681, 0.7637, 0.7591, 0.758, 0.7573, 0.7572, 0.7491, 0.7386, 0.7383, 0.7292, 0.7069, 0.6797, 0.6971, 0.6863, 0.6397, 0.6521, 0.6481, 0.68, 0.5535, 0.6107, 0.6355, 0.3645, 0.4249, 0.6106, 0.3643, 0.4385, 0.4025, 0.4757, 0.4154, 0.4116, 0.54, 0.3791, 0.2343, 0.4622, 0.282, 0.1252, 0.2157, 0.2394, 0.0597, 0.36, 0.3562, 0.0028, 0.1597, 0.1672, 0.2617, 0.1037, -0.044, 0.1302, 0.0576, -0.0491, -0.0006, -0.0477, 0.0252, 0.1723, -0.2019, -0.0443, -0.0831, -0.0884, -0.0157, 0.1005, -0.2068, -0.1909, 0.0293, 1.0712, 1.0703, 1.0351, 1.0214, 0.9509, 0.9254, 0.9241, 0.917, 0.9149, 0.9104, 0.9066, 0.9064, 0.9045, 0.9011, 0.8998, 0.8915, 0.8834, 0.8819, 0.8815, 0.8778, 0.8704, 0.866, 0.8543, 0.8451, 0.84, 0.8391, 0.8377, 0.837, 0.8352, 0.8338, 0.8331, 0.8253, 0.7914, 0.7021, 0.7266, 0.6751, 0.7001, 0.6344, 0.7113, 0.675, 0.7327, 0.6822, 0.5528, 0.3197, 0.6211, 0.5589, 0.6702, 0.5489, 0.6101, 0.4522, 0.355, 0.3998, 0.334, 0.1748, 0.24, 0.2672, 0.2548, 0.378, 0.1068, 0.2932, -0.0252, 0.0872, 0.0428, 0.2343, 0.2028, 0.2372, 0.2335, 0.2555, 0.0959, 0.1061, 0.0293, -0.0041, -0.1353, 0.0433, 0.0306, 0.1599, -0.2832, -0.0638, -0.0125, -0.2392, -0.0035, 0.0499, 0.0396, -0.0873, 0.0058, -0.3762, -0.11, -0.047, -0.2408, -0.1295, 1.1286, 1.1024, 1.0991, 1.0847, 1.0681, 1.0533, 1.0481, 1.0422, 1.0323, 1.021, 1.0178, 0.991, 0.9856, 0.9795, 0.9772, 0.9753, 0.975, 0.968, 0.9667, 0.9658, 0.956, 0.9345, 0.9338, 0.9166, 0.9163, 0.9148, 0.9119, 0.9012, 0.9007, 0.896, 0.8894, 0.7914, 0.8543, 0.825, 0.6716, 0.8066, 0.6394, 0.7057, 0.7356, 0.734, 0.5084, 0.528, 0.5755, 0.6147, 0.4049, 0.6992, 0.3336, 0.3251, 0.3291, 0.4345, 0.6007, 0.2767, 0.1857, 0.1979, 0.4148, 0.0584, 0.057, 0.2481, 0.1057, -0.054, 0.085, 0.2475, -0.079, 0.1988, 0.0373, 0.1462, 0.082, 0.1307, 0.1799, 0.1993, -0.185, 0.0468, -0.0938, -0.0595, -0.2406, 0.0385, -0.2317, -0.1145, -0.1417, -0.0636, -0.1786, -0.1582, 1.1119, 1.0991, 1.0882, 1.0816, 1.0661, 1.0661, 1.0563, 1.024, 1.0219, 0.9934, 0.9825, 0.9697, 0.9563, 0.9551, 0.9493, 0.9397, 0.9364, 0.9364, 0.9279, 0.9252, 0.9201, 0.9201, 0.9059, 0.8938, 0.8933, 0.8874, 0.8826, 0.8799, 0.8781, 0.8759, 0.8721, 0.8491, 0.8163, 0.8284, 0.8302, 0.7649, 0.7269, 0.6418, 0.6444, 0.6562, 0.6298, 0.7403, 0.3011, 0.6816, 0.6983, 0.3016, 0.322, 0.6102, 0.2677, 0.158, 0.5508, 0.4283, 0.1988, 0.0706, 0.2956, 0.3544, -0.0451, 0.1198, 0.3394, 0.0944, 0.1416, 0.0585, 0.0761, 0.0741, 0.0342, 0.0178, -0.0668, 0.1369, -0.147, -0.0612, 0.0673, -0.0834, -0.369, -0.0702, -0.2018, -0.0131, 0.0829, -0.0213, -0.3642, -0.0698, -0.2229, 1.06, 1.0242, 0.9808, 0.9752, 0.932, 0.9065, 0.9043, 0.9025, 0.8941, 0.8931, 0.8871, 0.8837, 0.8645, 0.8616, 0.8431, 0.8371, 0.8341, 0.8316, 0.8269, 0.822, 0.8216, 0.7974, 0.793, 0.7783, 0.7765, 0.768, 0.7675, 0.7667, 0.7661, 0.7626, 0.7547, 0.7499, 0.7395, 0.7338, 0.7121, 0.7105, 0.6721, 0.6398, 0.6609, 0.6078, 0.4158, 0.5175, 0.3739, 0.4382, 0.4039, 0.5901, 0.5677, 0.4859, 0.1892, 0.4095, 0.2393, 0.3495, 0.1893, 0.5212, 0.2459, 0.0243, 0.2462, 0.0388, 0.147, 0.1559, 0.2126, -0.0044, 0.087, 0.1794, 0.1486, 0.0169, 0.0429, 0.2495, 0.031, -0.023, 0.0017, 0.23, 0.271, 0.0227, 0.0593, -0.2312, -0.305, -0.073, 0.0889, 0.0028, -0.0994, 0.0358, -0.1804, -0.0268, -0.3071, -0.1064, -0.1282, 1.1205, 1.1046, 1.0696, 1.034, 1.013, 1.0122, 0.987, 0.9845, 0.9768, 0.9768, 0.956, 0.9486, 0.9283, 0.9279, 0.9265, 0.9244, 0.9239, 0.9206, 0.9182, 0.9075, 0.9066, 0.9053, 0.9048, 0.9, 0.8964, 0.8937, 0.8867, 0.884, 0.8825, 0.874, 0.8611, 0.8643, 0.8393, 0.8449, 0.7974, 0.8684, 0.7814, 0.7785, 0.7947, 0.7476, 0.7321, 0.5844, 0.6231, 0.439, 0.7637, 0.6347, 0.1054, 0.2572, 0.1887, 0.3808, 0.2543, 0.2392, -0.0166, 0.2119, 0.1379, 0.144, 0.3271, 0.0426, 0.2104, 0.0175, -0.085, 0.2278, 0.0381, 0.0111, -0.0726, -0.131, -0.0305, -0.2315, 0.0994, 0.1396, -0.0008, 0.02, -0.3015, -0.115, -0.2521, -0.0329, -0.0132, -0.0701, 0.024, -0.0354, -0.2803, -0.1245, -0.055, 1.2304, 1.2252, 1.2194, 1.1805, 1.1601, 1.1573, 1.1566, 1.1547, 1.1185, 1.1164, 1.1082, 1.0342, 1.0342, 1.0328, 1.0203, 1.0183, 0.997, 0.9963, 0.9909, 0.9878, 0.9862, 0.9851, 0.9829, 0.9816, 0.9767, 0.9719, 0.9683, 0.9618, 0.9544, 0.9541, 0.9089, 0.8723, 0.9017, 0.9031, 0.9082, 0.8749, 0.8098, 0.5499, 0.8058, 0.4926, 0.6849, 0.6664, 0.6495, 0.3442, 0.463, 0.4782, 0.1285, 0.4368, 0.023, 0.0916, 0.0481, 0.2277, 0.0751, 0.2719, 0.1284, 0.2249, 0.2209, 0.1777, 0.2392, 0.087, 0.1678, 0.0327, 0.0317, 0.2725, 0.263, -0.1922, -0.0412, -0.0897, 0.1193, -0.3341, -0.0803, -0.0669, 0.0256, -0.046, -0.1329, -0.1217, -0.0268, -0.2244, -0.4074, -0.2373, -0.0389, 1.122, 1.0526, 1.0408, 1.0246, 0.9817, 0.9271, 0.9058, 0.9004, 0.8839, 0.8806, 0.8624, 0.8367, 0.8256, 0.805, 0.794, 0.7895, 0.7889, 0.786, 0.7859, 0.7841, 0.7787, 0.768, 0.7567, 0.756, 0.7504, 0.7481, 0.7473, 0.7376, 0.7364, 0.734, 0.719, 0.7244, 0.7255, 0.7028, 0.7026, 0.6607, 0.6385, 0.6345, 0.604, 0.682, 0.629, 0.7066, 0.4541, 0.5897, 0.4792, 0.5452, 0.4982, 0.4327, 0.4236, 0.5057, 0.4007, 0.3716, 0.2076, 0.2722, 0.5, 0.3965, 0.1045, 0.1978, 0.3286, 0.2451, 0.2395, 0.1359, 0.0944, 0.2498, 0.1154, 0.1019, 0.1177, 0.2005, 0.0476, -0.0283, -0.0602, 0.1449, 0.0726, 0.0774, 0.0485, 0.0463, -0.0316, -0.2622, -0.2346, -0.2847, 0.0845, -0.0486, -0.0289, -0.1954, -0.1103, -0.1032, -0.1837, 0.0036, -0.1659, -0.2816]}, \"token.table\": {\"Topic\": [5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 9, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 1, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 5, 6, 7, 1, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 5, 6, 8, 9, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 9, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 7, 9, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 7, 8, 1, 2, 3, 4, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 3, 6, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 8, 1, 2, 3, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 9, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 7, 1, 1, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 7, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 5, 1, 2, 3, 4, 6, 8, 1, 2, 3, 4, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 3, 1, 2, 3, 4, 8, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 8, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 9, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 4, 5, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 1, 1, 2, 3, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 6, 8, 1, 3, 1, 2, 3, 4, 5, 8, 1, 4, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 3, 4, 5, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 1, 2, 3, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 1, 1, 2, 3, 4, 5, 6, 7, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 8, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 7, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 7, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 9, 1, 2, 3, 4], \"Freq\": [0.2933200337107135, 0.2849707325412656, 0.11657893603960866, 0.22020465696370525, 0.10362572092409658, 0.08419589825082847, 0.06476607557756037, 0.06476607557756037, 0.03238303778878018, 0.025906430231024145, 0.006476607557756036, 0.31523223507535697, 0.15761611753767849, 0.07880805876883924, 0.07880805876883924, 0.07880805876883924, 0.07880805876883924, 0.07880805876883924, 0.07880805876883924, 0.09022430868575884, 0.09022430868575884, 0.09022430868575884, 0.09022430868575884, 0.36089723474303537, 0.09022430868575884, 0.09022430868575884, 0.09022430868575884, 0.4040334749828857, 0.16903985293705379, 0.21646279810491503, 0.2447635879631548, 0.09255123169856791, 0.10784895594626509, 0.050482490017400676, 0.04589317274309153, 0.040538969256397515, 0.029065676070624633, 0.003824431061924294, 0.23980289186102022, 0.17365036996832497, 0.19432303305979223, 0.10129604914818957, 0.09302698391160268, 0.07028705451098868, 0.059950722965255054, 0.041345326182934516, 0.018605396782320534, 0.004134532618293452, 0.2492694917675264, 0.1633144946063104, 0.14096619534439425, 0.12549429585537536, 0.09798869676378624, 0.05844939806962688, 0.07907859738831872, 0.0515729982967296, 0.02922469903481344, 0.00515729982967296, 0.350136495512848, 0.350136495512848, 0.1900650332696423, 0.20290726524732086, 0.1464014445455353, 0.14383299814999959, 0.12328542698571393, 0.06421115988839267, 0.05650582070178555, 0.04366358872410702, 0.02568446395535707, 0.005136892791071414, 0.09289806925627137, 0.12386409234169517, 0.09289806925627137, 0.15483011542711894, 0.2786942077688141, 0.09289806925627137, 0.09289806925627137, 0.030966023085423792, 0.030966023085423792, 0.31684533309729235, 0.1901071998583754, 0.1774333865344837, 0.050695253295566775, 0.07604287994335017, 0.038021439971675085, 0.06336906661945847, 0.06336906661945847, 0.012673813323891694, 0.012673813323891694, 0.18804650864052058, 0.21625348493659868, 0.16689127641846202, 0.10107499839427982, 0.1269313933323514, 0.0634656966661757, 0.058764533950162685, 0.04231046444411713, 0.03525872037009761, 0.0047011627160130146, 0.1268145607901353, 0.15851820098766914, 0.3487400421728721, 0.09511092059260148, 0.09511092059260148, 0.06340728039506766, 0.06340728039506766, 0.06340728039506766, 0.03170364019753383, 0.14562822499523076, 0.14562822499523076, 0.14562822499523076, 0.14562822499523076, 0.14562822499523076, 0.14562822499523076, 0.1705307620339283, 0.13952516893685044, 0.07751398274269469, 0.1240223723883115, 0.2015363551310062, 0.1240223723883115, 0.06201118619415575, 0.046508389645616814, 0.06201118619415575, 0.19131362634624133, 0.19131362634624133, 0.07652545053849653, 0.1147881758077448, 0.1147881758077448, 0.07652545053849653, 0.1147881758077448, 0.03826272526924827, 0.07652545053849653, 0.4993180708279828, 0.12698626846763583, 0.21164378077939305, 0.3174656711690896, 0.08465751231175722, 0.07407532327278757, 0.06349313423381792, 0.06349313423381792, 0.03174656711690896, 0.021164378077939305, 0.4013633541635153, 0.18341446035253284, 0.18592698720667714, 0.16331424551937856, 0.15075161124865713, 0.07788833247847285, 0.07788833247847285, 0.07537580562432857, 0.05527559079117428, 0.025125268541442855, 0.005025053708288571, 0.18013488323649635, 0.18013488323649635, 0.1327309665953131, 0.10428861661060315, 0.12325018326707644, 0.0948078332823665, 0.08532704995412985, 0.056884699969419895, 0.0379231333129466, 0.00948078332823665, 0.15879594997127144, 0.23252264102936174, 0.11909696247845357, 0.13043953033354438, 0.10775439462336275, 0.11342567855090817, 0.06805540713054489, 0.03969898749281786, 0.034027703565272446, 0.005671283927545409, 0.2905463595613788, 0.10376655698620672, 0.31129967095862016, 0.062259934191724035, 0.062259934191724035, 0.020753311397241344, 0.062259934191724035, 0.062259934191724035, 0.020753311397241344, 0.1423857061776368, 0.1423857061776368, 0.1423857061776368, 0.1423857061776368, 0.1423857061776368, 0.1423857061776368, 0.18881480413945104, 0.32368252138191606, 0.13486771724246502, 0.10789417379397202, 0.08092063034547901, 0.05394708689698601, 0.05394708689698601, 0.05394708689698601, 0.026973543448493006, 0.15620219102348087, 0.23690665638561265, 0.22649317698404725, 0.08851457491330582, 0.11975501311800199, 0.04165391760626156, 0.06508424625978369, 0.04686065730704426, 0.015620219102348086, 0.005206739700782695, 0.30035950215606994, 0.30035950215606994, 0.3662796864986805, 0.2960538057085189, 0.14802690285425946, 0.2960538057085189, 0.19739610917048428, 0.2960941637557264, 0.09869805458524214, 0.09869805458524214, 0.09869805458524214, 0.09869805458524214, 0.09869805458524214, 0.09869805458524214, 0.11383275290919093, 0.3414982587275728, 0.11383275290919093, 0.11383275290919093, 0.11383275290919093, 0.15864663670247234, 0.16922307914930382, 0.0951879820214834, 0.11634086691514638, 0.2115288489366298, 0.08461153957465191, 0.07403509712782043, 0.05288221223415745, 0.03172932734049447, 0.010576442446831489, 0.19529274490519669, 0.19529274490519669, 0.19529274490519669, 0.19529274490519669, 0.19529274490519669, 0.19529274490519669, 0.14345165985133168, 0.14345165985133168, 0.14345165985133168, 0.14345165985133168, 0.07172582992566584, 0.07172582992566584, 0.07172582992566584, 0.07172582992566584, 0.07172582992566584, 0.14235164983223234, 0.23047409972837618, 0.17398534979495064, 0.11975614985886213, 0.16268759980826553, 0.04067189995206638, 0.058748299930762554, 0.04293144994940341, 0.027114599968044256, 0.0022595499973370216, 0.30255989575587244, 0.12102395830234897, 0.06051197915117448, 0.30255989575587244, 0.06051197915117448, 0.06051197915117448, 0.06051197915117448, 0.23113442603653178, 0.23113442603653178, 0.23113442603653178, 0.23113442603653178, 0.23113442603653178, 0.21642786916974516, 0.10821393458487258, 0.10821393458487258, 0.10821393458487258, 0.16232090187730885, 0.05410696729243629, 0.05410696729243629, 0.16232090187730885, 0.32302539710639216, 0.31720349778301316, 0.10016952561568836, 0.18364413029542864, 0.0834746046797403, 0.06677968374379224, 0.05008476280784418, 0.0834746046797403, 0.0834746046797403, 0.05008476280784418, 0.01669492093594806, 0.3902848437344678, 0.1115099553527051, 0.05575497767635255, 0.1115099553527051, 0.05575497767635255, 0.05575497767635255, 0.1115099553527051, 0.05575497767635255, 0.08647982231899999, 0.17295964463799998, 0.08647982231899999, 0.17295964463799998, 0.17295964463799998, 0.08647982231899999, 0.11530642975866666, 0.11530642975866666, 0.028826607439666665, 0.1441187186749879, 0.23917574588615015, 0.18091498727285715, 0.0705261814792494, 0.15025143010796613, 0.061327114329782086, 0.0674598257627603, 0.061327114329782086, 0.024530845731912835, 0.0030663557164891044, 0.2262116611200091, 0.16316906703738362, 0.12608518816525097, 0.11125163661639792, 0.13721035182689076, 0.07416775774426528, 0.0927096971803316, 0.04450065464655917, 0.01854193943606632, 0.007416775774426528, 0.1868773581338127, 0.09343867906690635, 0.14015801860035954, 0.14015801860035954, 0.1635176883670861, 0.1868773581338127, 0.023359669766726588, 0.023359669766726588, 0.023359669766726588, 0.21506071592142215, 0.21506071592142215, 0.21506071592142215, 0.21506071592142215, 0.21506071592142215, 0.1832177565021076, 0.3664355130042152, 0.0916088782510538, 0.0916088782510538, 0.1832177565021076, 0.0916088782510538, 0.0916088782510538, 0.17778062488322205, 0.17778062488322205, 0.17778062488322205, 0.3555612497664441, 0.32261690762593515, 0.12904676305037407, 0.06452338152518704, 0.12904676305037407, 0.09678507228778055, 0.06452338152518704, 0.09678507228778055, 0.03226169076259352, 0.03226169076259352, 0.2559608601113415, 0.2559608601113415, 0.2559608601113415, 0.3933644637372344, 0.38824241343719845, 0.15934469208679589, 0.10622979472453059, 0.10622979472453059, 0.10622979472453059, 0.2655744868113265, 0.10622979472453059, 0.053114897362265295, 0.053114897362265295, 0.053114897362265295, 0.16235593040196758, 0.1343635286085249, 0.1511589696845905, 0.2127422536301644, 0.11756808753245929, 0.07837872502163952, 0.06158328394557391, 0.05038632322819683, 0.02239392143475415, 0.0055984803586885375, 0.1671290374735801, 0.1392741978946501, 0.1671290374735801, 0.11141935831572007, 0.1392741978946501, 0.055709679157860036, 0.055709679157860036, 0.11141935831572007, 0.027854839578930018, 0.23812851869669593, 0.2031096188883583, 0.1540831591566856, 0.09104913950167785, 0.09104913950167785, 0.056030239693340214, 0.0770415795783428, 0.056030239693340214, 0.03501889980833764, 0.007003779961667527, 0.15237811248447677, 0.16712438143458744, 0.2556019951352514, 0.09830845966740437, 0.1278009975676257, 0.05898507580044262, 0.05898507580044262, 0.04423880685033197, 0.03440796088359153, 0.004915422983370219, 0.23684690393734237, 0.23684690393734237, 0.23684690393734237, 0.23684690393734237, 0.23684690393734237, 0.23684690393734237, 0.2073242287728761, 0.3317187660366017, 0.08292969150915043, 0.12439453726372565, 0.08292969150915043, 0.041464845754575215, 0.08292969150915043, 0.041464845754575215, 0.1529863859953316, 0.1529863859953316, 0.1529863859953316, 0.1529863859953316, 0.1529863859953316, 0.1529863859953316, 0.17420416838358613, 0.17420416838358613, 0.17420416838358613, 0.34840833676717226, 0.17420416838358613, 0.3141215921757544, 0.10470719739191815, 0.052353598695959076, 0.26176799347979535, 0.052353598695959076, 0.052353598695959076, 0.052353598695959076, 0.052353598695959076, 0.2131042252414986, 0.19373111385590783, 0.15014161323832856, 0.14045505754553317, 0.08233572338876083, 0.06780588984956774, 0.08233572338876083, 0.03874622277118157, 0.02421638923198848, 0.009686555692795392, 0.10148782567686683, 0.10148782567686683, 0.10148782567686683, 0.10148782567686683, 0.20297565135373366, 0.20297565135373366, 0.10148782567686683, 0.2982262614092348, 0.2982262614092348, 0.2982262614092348, 0.2936049639855576, 0.09786832132851919, 0.09786832132851919, 0.2936049639855576, 0.048934160664259596, 0.09786832132851919, 0.048934160664259596, 0.048934160664259596, 0.1501538893091716, 0.1126154169818787, 0.1126154169818787, 0.1126154169818787, 0.3003077786183432, 0.0750769446545858, 0.0750769446545858, 0.0375384723272929, 0.0750769446545858, 0.1452556999292482, 0.1452556999292482, 0.1452556999292482, 0.2905113998584964, 0.1452556999292482, 0.19485017634602733, 0.14072512736101975, 0.1190751077670167, 0.12448761266551746, 0.17861266165052506, 0.10283759307151442, 0.037887534289505316, 0.05953755388350835, 0.043300039188006074, 0.005412504898500759, 0.16711116462539055, 0.16711116462539055, 0.3342223292507811, 0.10035166313316085, 0.2759670736161923, 0.17561541048303148, 0.10035166313316085, 0.07526374734987064, 0.15052749469974128, 0.050175831566580426, 0.050175831566580426, 0.025087915783290213, 0.1243892284198765, 0.3731676852596295, 0.1243892284198765, 0.1243892284198765, 0.1243892284198765, 0.26148033658273145, 0.26148033658273145, 0.15553007083139508, 0.20737342777519346, 0.10368671388759673, 0.10368671388759673, 0.10368671388759673, 0.15553007083139508, 0.10368671388759673, 0.051843356943798366, 0.2597631380383981, 0.2597631380383981, 0.3546195159190597, 0.15760867374180432, 0.07880433687090216, 0.07880433687090216, 0.07880433687090216, 0.07880433687090216, 0.07880433687090216, 0.07880433687090216, 0.03940216843545108, 0.2253495915325087, 0.2223582252732276, 0.1385999700133571, 0.10669206324769216, 0.09173523195128672, 0.0777755227413083, 0.06182156935847583, 0.04487049388921633, 0.02293380798782168, 0.005982732518562178, 0.3219886349451704, 0.09659659048355111, 0.09659659048355111, 0.2897897714506533, 0.03219886349451704, 0.06439772698903408, 0.03219886349451704, 0.03219886349451704, 0.03219886349451704, 0.1708728402434119, 0.1708728402434119, 0.08543642012170595, 0.08543642012170595, 0.25630926036511786, 0.08543642012170595, 0.1708728402434119, 0.16436133577461218, 0.2191484476994829, 0.1986032807276564, 0.08218066788730609, 0.1369677798121768, 0.06163550091547957, 0.05478711192487073, 0.05478711192487073, 0.020545166971826522, 0.006848388990608841, 0.2497835905852248, 0.4995671811704496, 0.33452934198471246, 0.22496136150048465, 0.1840592957731238, 0.08180413145472169, 0.24541239436416507, 0.08180413145472169, 0.020451032863680423, 0.040902065727360847, 0.1022551643184021, 0.020451032863680423, 0.10416244466223074, 0.41664977864892294, 0.10416244466223074, 0.10416244466223074, 0.10416244466223074, 0.10416244466223074, 0.18686338263262772, 0.173190452196094, 0.24155510437876268, 0.08203758261920242, 0.11849873044995905, 0.059249365224979526, 0.054691721746134944, 0.054691721746134944, 0.027345860873067472, 0.004557643478844579, 0.11666195023802742, 0.1296243891533638, 0.3888731674600914, 0.09721829186502284, 0.08425585294968646, 0.05833097511901371, 0.038887316746009135, 0.05833097511901371, 0.02592487783067276, 0.00648121945766819, 0.11689718664694264, 0.17534577997041395, 0.11689718664694264, 0.23379437329388528, 0.05844859332347132, 0.05844859332347132, 0.11689718664694264, 0.05844859332347132, 0.05844859332347132, 0.11770635465344251, 0.23541270930688502, 0.094165083722754, 0.2118714383761965, 0.0706238127920655, 0.094165083722754, 0.11770635465344251, 0.0235412709306885, 0.047082541861377, 0.24028083795546953, 0.18849617460299767, 0.11599764590953703, 0.13671151125052577, 0.12221180551183365, 0.06835575562526289, 0.060070209488867383, 0.04764189028427413, 0.01864247880688988, 0.006214159602296626, 0.1959278716888683, 0.1959278716888683, 0.1959278716888683, 0.1959278716888683, 0.1959278716888683, 0.1378719076648782, 0.17923347996434164, 0.20336106380569532, 0.1585526938146099, 0.10340393074865864, 0.08616994229054886, 0.048255167682707364, 0.05170196537432932, 0.027574381532975638, 0.0034467976916219547, 0.17331354915783237, 0.17331354915783237, 0.34662709831566474, 0.17331354915783237, 0.17331354915783237, 0.18581430344976466, 0.18581430344976466, 0.18581430344976466, 0.09290715172488233, 0.09290715172488233, 0.09290715172488233, 0.09290715172488233, 0.09290715172488233, 0.14966574492606405, 0.1995543265680854, 0.16629527214007117, 0.06651810885602846, 0.06651810885602846, 0.04988858164202135, 0.0997771632840427, 0.13303621771205693, 0.03325905442801423, 0.20437010095529554, 0.20437010095529554, 0.20437010095529554, 0.20437010095529554, 0.20437010095529554, 0.20874878007872594, 0.15036988395501444, 0.1999034927872545, 0.12206496462230584, 0.09906721766448011, 0.08845287291471439, 0.04599549391565148, 0.05307172374882863, 0.026535861874414316, 0.005307172374882863, 0.18601181890418536, 0.12090768228772049, 0.24181536457544098, 0.11625738681511585, 0.09765620492469732, 0.08370531850688341, 0.06975443208906952, 0.05580354567125561, 0.01860118189041854, 0.004650295472604635, 0.16376815591359734, 0.09826089354815841, 0.09826089354815841, 0.19652178709631682, 0.13101452473087788, 0.06550726236543894, 0.06550726236543894, 0.09826089354815841, 0.03275363118271947, 0.16568542806315353, 0.11045695204210236, 0.16568542806315353, 0.16568542806315353, 0.05522847602105118, 0.05522847602105118, 0.05522847602105118, 0.11045695204210236, 0.1332546740591763, 0.1332546740591763, 0.19988201108876444, 0.2665093481183526, 0.1332546740591763, 0.06662733702958815, 0.12863111944545966, 0.38589335833637894, 0.12863111944545966, 0.12863111944545966, 0.12863111944545966, 0.12863111944545966, 0.05956476396822065, 0.29782381984110323, 0.17869429190466193, 0.1191295279364413, 0.1191295279364413, 0.1191295279364413, 0.05956476396822065, 0.05956476396822065, 0.14731677651817437, 0.14731677651817437, 0.14731677651817437, 0.14731677651817437, 0.14731677651817437, 0.14731677651817437, 0.33881482604127344, 0.16940741302063672, 0.16940741302063672, 0.33881482604127344, 0.5419028140148968, 0.06711531334057264, 0.3803201089299116, 0.13423062668114527, 0.08948708445409685, 0.08948708445409685, 0.13423062668114527, 0.06711531334057264, 0.044743542227048426, 0.022371771113524213, 0.1663051425649632, 0.1663051425649632, 0.1663051425649632, 0.1108700950433088, 0.1663051425649632, 0.1663051425649632, 0.0554350475216544, 0.0554350475216544, 0.0554350475216544, 0.13508537164183754, 0.15759960024881048, 0.13508537164183754, 0.04502845721394585, 0.13508537164183754, 0.1801138288557834, 0.0900569144278917, 0.04502845721394585, 0.022514228606972924, 0.022514228606972924, 0.18511707535720986, 0.18511707535720986, 0.18511707535720986, 0.09255853767860493, 0.09255853767860493, 0.18511707535720986, 0.09255853767860493, 0.16819741044825123, 0.14416920895564392, 0.16819741044825123, 0.22826791417976952, 0.10812690671673293, 0.07208460447782196, 0.03604230223891098, 0.0600705037315183, 0.02402820149260732, 0.2240061611868615, 0.0746687203956205, 0.3733436019781025, 0.0746687203956205, 0.0746687203956205, 0.0746687203956205, 0.20808340730161806, 0.1724119660499121, 0.15061275195164736, 0.1268317911171767, 0.09908733681029432, 0.0713428825034119, 0.09116034986547077, 0.04756192166894127, 0.027744454306882407, 0.005945240208617659, 0.2030521190268421, 0.2030521190268421, 0.2030521190268421, 0.2030521190268421, 0.2030521190268421, 0.2030521190268421, 0.16911301888251778, 0.18320577045606093, 0.2583671121816244, 0.11743959644619291, 0.07516134172556346, 0.06576617400986803, 0.04697583857847716, 0.05167342243632488, 0.0281855031470863, 0.0046975838578477164, 0.05828474771301918, 0.3691367355157882, 0.1262836200448749, 0.09714124618836531, 0.13599774466371142, 0.10685537080720184, 0.06799887233185571, 0.02914237385650959, 0.00971412461883653, 0.174861110786991, 0.3302932092643163, 0.233148147715988, 0.07771604923866267, 0.058287036928997, 0.038858024619331334, 0.058287036928997, 0.038858024619331334, 0.16200564928062228, 0.19200669544370047, 0.16200564928062228, 0.15600544004800662, 0.12900449850123624, 0.06900240617507986, 0.05700198770984857, 0.042001464628309476, 0.02400083693046256, 0.00600020923261564, 0.15954188690375662, 0.15954188690375662, 0.12763350952300528, 0.09572513214225396, 0.22335864166525926, 0.06381675476150264, 0.06381675476150264, 0.03190837738075132, 0.03190837738075132, 0.29150408768420727, 0.29150408768420727, 0.29150408768420727, 0.29150408768420727, 0.1467812736139283, 0.19570836481857104, 0.15901304641508898, 0.12231772801160691, 0.09785418240928552, 0.1467812736139283, 0.061158864005803455, 0.04892709120464276, 0.02446354560232138, 0.1652423380981655, 0.1652423380981655, 0.1652423380981655, 0.330484676196331, 0.1652423380981655, 0.12986238839765124, 0.12120489583780782, 0.19046483631655514, 0.2164373139960854, 0.103889910718121, 0.06060244791890391, 0.08657492559843416, 0.06060244791890391, 0.02597247767953025, 0.2291228133287655, 0.152748542219177, 0.1872401485267331, 0.12072062207644634, 0.1256479944060972, 0.06651952645028676, 0.05173740946133414, 0.02710054781307979, 0.03449160630755609, 0.0024636861648254355, 0.2782032833793048, 0.14172620096681565, 0.09973325253220361, 0.24670857205334576, 0.052491185543265054, 0.05774030409759156, 0.04199294843461204, 0.04724206698893855, 0.015747355662979516, 0.005249118554326505, 0.16754954805002523, 0.18381649446264903, 0.13826904450730237, 0.1268821820184657, 0.14152243378982712, 0.07482795349806952, 0.07482795349806952, 0.061814396367970474, 0.026027114260198093, 0.0032533892825247616, 0.17881585789994744, 0.1100405279384292, 0.1100405279384292, 0.09628546194612554, 0.23383612186916203, 0.08253039595382189, 0.08253039595382189, 0.06877532996151825, 0.041265197976910944, 0.01375506599230365, 0.21369656424671435, 0.19193117344380825, 0.15235773562034263, 0.1345496885997831, 0.1048696102321839, 0.05738148484402515, 0.06331750051754499, 0.04946679727933202, 0.027701406476425935, 0.003957343782346562, 0.14289983854445398, 0.07144991927222699, 0.07144991927222699, 0.07144991927222699, 0.28579967708890797, 0.14289983854445398, 0.07144991927222699, 0.07144991927222699, 0.18404483376432212, 0.26817961491372655, 0.16301113847697102, 0.12357295981318772, 0.09202241688216106, 0.06835950968389108, 0.047325814396539975, 0.028921331020107764, 0.0210336952873511, 0.005258423821837775, 0.2551069731300418, 0.1391492580709319, 0.11595771505910993, 0.23191543011821986, 0.04638308602364397, 0.06957462903546595, 0.04638308602364397, 0.04638308602364397, 0.023191543011821986, 0.15246520964516627, 0.11858405194624044, 0.15246520964516627, 0.2541086827419438, 0.10164347309677751, 0.0847028942473146, 0.05082173654838876, 0.03388115769892584, 0.05082173654838876, 0.5825937769080074, 0.14943102423786053, 0.176600301372017, 0.12226174710370406, 0.1358463856707823, 0.10867710853662584, 0.09509246996954761, 0.10867710853662584, 0.04075391570123469, 0.05433855426831292, 0.01358463856707823, 0.32285174406406064, 0.13452156002669194, 0.08968104001779463, 0.10761724802135354, 0.1883301840373687, 0.044840520008897315, 0.03587241600711785, 0.06277672801245623, 0.008968104001779463, 0.008968104001779463, 0.11727398251532876, 0.11727398251532876, 0.11727398251532876, 0.11727398251532876, 0.3518219475459863, 0.11727398251532876, 0.11727398251532876, 0.246281045554863, 0.246281045554863, 0.246281045554863, 0.246281045554863, 0.246281045554863, 0.2106862101290354, 0.1964369012314195, 0.16284924454418195, 0.09669173894810804, 0.12417254896493875, 0.06310408226087051, 0.07531777560168416, 0.03765888780084208, 0.02951642557363298, 0.005089038892005686, 0.19242373125000162, 0.17827492748161916, 0.19525349200367811, 0.1047011478860303, 0.0990416263786773, 0.06791425808823587, 0.07923330110294184, 0.05659521507352989, 0.022638086029411954, 0.0056595215073529884, 0.40369463414248746, 0.20513174371534762, 0.1538488077865107, 0.10256587185767381, 0.10256587185767381, 0.1538488077865107, 0.06837724790511587, 0.10256587185767381, 0.06837724790511587, 0.03418862395255794, 0.2184542712594067, 0.1741730000581756, 0.13874798309719075, 0.1564604915776832, 0.10037088138945713, 0.07380211866871848, 0.061993779681723524, 0.04132918645448235, 0.026568762720738652, 0.005904169493497478, 0.19699658494542735, 0.23125686058811035, 0.06852055128536604, 0.21412672276676883, 0.15417124039207358, 0.04282534455335377, 0.03426027564268302, 0.02569520673201226, 0.01713013782134151, 0.14801754854741367, 0.14801754854741367, 0.14801754854741367, 0.14801754854741367, 0.14801754854741367, 0.14801754854741367, 0.14801754854741367, 0.20634380432219862, 0.20634380432219862, 0.20634380432219862, 0.20634380432219862, 0.20634380432219862, 0.14280025803190036, 0.05712010321276014, 0.05712010321276014, 0.14280025803190036, 0.2570404644574206, 0.17136030963828042, 0.08568015481914021, 0.05712010321276014, 0.02856005160638007, 0.17754875744719645, 0.3550975148943929, 0.17754875744719645, 0.17754875744719645, 0.2032396611398596, 0.2032396611398596, 0.2032396611398596, 0.2032396611398596, 0.2032396611398596, 0.21517269647296272, 0.14086845596431372, 0.1625405261126697, 0.1455124709961043, 0.13003242089013575, 0.074304240508649, 0.05882419040268046, 0.04489214530730877, 0.024768080169549667, 0.004644015031790563, 0.17021419755949144, 0.17021419755949144, 0.2836903292658191, 0.056738065853163815, 0.056738065853163815, 0.056738065853163815, 0.11347613170632763, 0.11347613170632763, 0.056738065853163815, 0.1775751285872791, 0.1775751285872791, 0.1775751285872791, 0.1775751285872791, 0.1775751285872791, 0.1775751285872791, 0.12240826031140306, 0.12240826031140306, 0.12240826031140306, 0.12240826031140306, 0.3672247809342092, 0.12240826031140306, 0.12240826031140306, 0.338887669358587, 0.12708287600947013, 0.19062431401420518, 0.12708287600947013, 0.042360958669823374, 0.08472191733964675, 0.042360958669823374, 0.021180479334911687, 0.042360958669823374, 0.18602314645657309, 0.37204629291314617, 0.09301157322828654, 0.09301157322828654, 0.09301157322828654, 0.09301157322828654, 0.09301157322828654, 0.214869383226372, 0.16431188129075505, 0.17695125677465928, 0.08847562838732964, 0.05055750193561694, 0.11375437935513812, 0.05055750193561694, 0.08847562838732964, 0.037918126451712705, 0.012639375483904236, 0.12222377151166376, 0.08148251434110917, 0.08148251434110917, 0.20370628585277295, 0.20370628585277295, 0.20370628585277295, 0.04074125717055459, 0.04074125717055459, 0.04074125717055459, 0.22466785241409104, 0.22466785241409104, 0.22466785241409104, 0.22466785241409104, 0.22466785241409104, 0.14406392939151336, 0.15366819135094759, 0.2625164935578688, 0.1312582467789344, 0.1152511435132107, 0.05122273045031586, 0.05122273045031586, 0.05762557175660535, 0.02561136522515793, 0.0032014206531447414, 0.15932055568919373, 0.28829433886616007, 0.15932055568919373, 0.09862701066473899, 0.11380039692085267, 0.05310685189639791, 0.06828023815251161, 0.04552015876834107, 0.022760079384170535, 0.007586693128056845, 0.3436026175818944, 0.34270151510566915, 0.3902356395525934, 0.11941115054439912, 0.17911672581659868, 0.11941115054439912, 0.17911672581659868, 0.05970557527219956, 0.05970557527219956, 0.11941115054439912, 0.05970557527219956, 0.25037020532812415, 0.25037020532812415, 0.25037020532812415, 0.17668311776634057, 0.1365278637285359, 0.12046576211341403, 0.1365278637285359, 0.12046576211341403, 0.1606210161512187, 0.06424840646048749, 0.04818630484536562, 0.02409315242268281, 0.008031050807560936, 0.1938667701400425, 0.1792353157898506, 0.1792353157898506, 0.10242018045134321, 0.09876231686379525, 0.08778872610115132, 0.08047299892605538, 0.03657863587547972, 0.029262908700383776, 0.007315727175095944, 0.3785488230532431, 0.1261829410177477, 0.1261829410177477, 0.1261829410177477, 0.1261829410177477, 0.1261829410177477, 0.2319608023446563, 0.11598040117232815, 0.2319608023446563, 0.11598040117232815, 0.11598040117232815, 0.11598040117232815, 0.10588236886481073, 0.10588236886481073, 0.21176473772962146, 0.24705886068455837, 0.10588236886481073, 0.14117649181974765, 0.03529412295493691, 0.03529412295493691, 0.03529412295493691, 0.20948346777901292, 0.14154288363446818, 0.25477719054204273, 0.07643315716261281, 0.10474173388950646, 0.08209487250799154, 0.05661715345378727, 0.045293722763029814, 0.022646861381514907, 0.005661715345378727, 0.20425018909952913, 0.15318764182464684, 0.15318764182464684, 0.08510424545813713, 0.11914594364139199, 0.10212509454976457, 0.08510424545813713, 0.05106254727488228, 0.05106254727488228, 0.017020849091627425, 0.11600021577912054, 0.15466695437216071, 0.15466695437216071, 0.1933336929652009, 0.07733347718608036, 0.07733347718608036, 0.11600021577912054, 0.03866673859304018, 0.03866673859304018, 0.2282686899695768, 0.1600845358228201, 0.22233963308725013, 0.1096875523230434, 0.08300679635257338, 0.06225509726443004, 0.06521962570559338, 0.04150339817628669, 0.026680755970470017, 0.00592905688232667, 0.21810385590787773, 0.21810385590787773, 0.21810385590787773, 0.21810385590787773, 0.26828595705049135, 0.14308584376026204, 0.15202870899527843, 0.08942865235016378, 0.09837151758518016, 0.08942865235016378, 0.07154292188013102, 0.058128624027606456, 0.026828595705049134, 0.004471432617508189, 0.35201457823281707, 0.14080583129312682, 0.07040291564656341, 0.10560437346984512, 0.21120874693969025, 0.035201457823281705, 0.035201457823281705, 0.07040291564656341, 0.2278602090156357, 0.16920312550666017, 0.14438666709901668, 0.12633833371163958, 0.13536250040532813, 0.047376875141864845, 0.07219333354950834, 0.04963291681528698, 0.02481645840764349, 0.004512083346844271, 0.20174383912734947, 0.15736019451933259, 0.1936740855622555, 0.10894167312876872, 0.10490679634622173, 0.0766626588683928, 0.08876728921603377, 0.04438364460801689, 0.024209260695281936, 0.00403487678254699, 0.12209311316147081, 0.12209311316147081, 0.12209311316147081, 0.12209311316147081, 0.12209311316147081, 0.12209311316147081, 0.12209311316147081, 0.3330007723567397, 0.08325019308918492, 0.12487528963377739, 0.12487528963377739, 0.12487528963377739, 0.04162509654459246, 0.04162509654459246, 0.08325019308918492, 0.04162509654459246, 0.14438259658945923, 0.14438259658945923, 0.14438259658945923, 0.21657389488418885, 0.07219129829472962, 0.07219129829472962, 0.14438259658945923, 0.07219129829472962, 0.07219129829472962, 0.31358957460937675, 0.15679478730468838, 0.15679478730468838, 0.15679478730468838, 0.15679478730468838, 0.1675782545025565, 0.20802817800317358, 0.14735329275224796, 0.13290689150202759, 0.11268192975171903, 0.052007044500793395, 0.09534624825145456, 0.043339203750661165, 0.034671363000528935, 0.0028892802500440776, 0.16490006554680198, 0.16490006554680198, 0.16490006554680198, 0.16490006554680198, 0.16490006554680198, 0.16490006554680198, 0.2148452165940052, 0.16328236461144396, 0.16328236461144396, 0.10527415613106256, 0.13320403428828323, 0.07734427797384189, 0.06230511281226151, 0.051562851982561254, 0.01933606949346047, 0.006445356497820157, 0.24216904976704828, 0.24216904976704828, 0.24216904976704828, 0.3817839156771206, 0.2062702611034207, 0.16223503682291515, 0.12051745592559411, 0.15528210667336165, 0.15296446329017713, 0.05330579781324355, 0.06257637134598155, 0.05562344119642805, 0.025494077215029522, 0.0046352867663690045, 0.24374483912335687, 0.14999682407591192, 0.13124722106642292, 0.07499841203795596, 0.11249761805693394, 0.09374801504744495, 0.11249761805693394, 0.07499841203795596, 0.01874960300948899, 0.22803946995715943, 0.22803946995715943, 0.22803946995715943, 0.22803946995715943, 0.22803946995715943, 0.27372700191990595, 0.09124233397330199, 0.09124233397330199, 0.27372700191990595, 0.045621166986650995, 0.09124233397330199, 0.045621166986650995, 0.045621166986650995, 0.045621166986650995, 0.10523157659912634, 0.21046315319825268, 0.33674104511720426, 0.08418526127930107, 0.0631389459594758, 0.04209263063965053, 0.0631389459594758, 0.04209263063965053, 0.04209263063965053, 0.2221570555927322, 0.2221570555927322, 0.2221570555927322, 0.2221570555927322, 0.2221570555927322, 0.09614660463065787, 0.09614660463065787, 0.09614660463065787, 0.09614660463065787, 0.28843981389197365, 0.09614660463065787, 0.09614660463065787, 0.09614660463065787, 0.2790054117404862, 0.2790054117404862, 0.2602010223591856, 0.20349054312705536, 0.14678006389492518, 0.090069584662795, 0.090069584662795, 0.06004638977519666, 0.06671821086132963, 0.05003865814599722, 0.033359105430664816, 0.0033359105430664816, 0.3369006843948193, 0.0962573383985198, 0.1347602737579277, 0.12513453991807574, 0.07700587071881583, 0.057754403039111876, 0.07700587071881583, 0.057754403039111876, 0.038502935359407915, 0.009625733839851979, 0.3167555300344816, 0.1583777650172408, 0.1583777650172408, 0.1583777650172408, 0.37850975477923177, 0.28072459129588784, 0.28072459129588784, 0.3952087264325736, 0.09293229490660279, 0.14869167185056448, 0.40890209758905227, 0.11151875388792334, 0.05575937694396167, 0.05575937694396167, 0.03717291796264112, 0.09293229490660279, 0.01858645898132056, 0.17862082430300522, 0.35724164860601043, 0.08931041215150261, 0.17862082430300522, 0.08931041215150261, 0.08931041215150261, 0.2788968507498094, 0.2788968507498094, 0.14414266922029598, 0.14414266922029598, 0.14414266922029598, 0.14414266922029598, 0.14414266922029598, 0.14414266922029598, 0.14414266922029598, 0.14414266922029598, 0.2671472713608888, 0.1335736356804444, 0.1335736356804444, 0.4007209070413331, 0.2480707585088669, 0.2480707585088669, 0.3138824472510753, 0.23776533179929132, 0.23776533179929132, 0.23776533179929132, 0.23776533179929132, 0.23776533179929132, 0.23776533179929132, 0.1761219429028412, 0.1761219429028412, 0.1761219429028412, 0.1761219429028412, 0.1761219429028412, 0.1761219429028412, 0.21893604034069933, 0.2027185558710179, 0.12703696167917122, 0.12163113352261073, 0.14866027430541312, 0.06486993787872573, 0.04054371117420358, 0.03784079709592334, 0.029732054861082626, 0.005405828156560478, 0.1252999536725927, 0.37589986101777806, 0.1252999536725927, 0.1252999536725927, 0.1252999536725927, 0.17415944196064387, 0.12359702332690854, 0.11236093029718959, 0.1910135815052223, 0.1348331163566275, 0.08988874423775167, 0.09550679075261115, 0.03370827908915688, 0.03370827908915688, 0.0056180465148594795, 0.23076736140075757, 0.17307552105056817, 0.11538368070037879, 0.17307552105056817, 0.11538368070037879, 0.05769184035018939, 0.11538368070037879, 0.05769184035018939, 0.05769184035018939, 0.16414104603344604, 0.16414104603344604, 0.16414104603344604, 0.16414104603344604, 0.16414104603344604, 0.12758099040312512, 0.17010798720416684, 0.17010798720416684, 0.12758099040312512, 0.08505399360208342, 0.12758099040312512, 0.04252699680104171, 0.12758099040312512, 0.08505399360208342, 0.2922783479241555, 0.17536303107752696, 0.16074944515439973, 0.17074821447022362, 0.16844080616657195, 0.1107555985752802, 0.06922224910955012, 0.06383829640102956, 0.05691607149007454, 0.019997538631647813, 0.0038456805060861176, 0.14034903905162804, 0.14034903905162804, 0.4210471171548842, 0.07017451952581402, 0.07017451952581402, 0.07017451952581402, 0.07017451952581402, 0.2586568665447417, 0.10875809152345323, 0.10875809152345323, 0.4350323660938129, 0.10875809152345323, 0.10875809152345323, 0.23037780189500742, 0.23037780189500742, 0.23037780189500742, 0.1859188672256231, 0.19498808026101933, 0.24940335847339684, 0.09522673687166061, 0.09069213035396248, 0.06801909776547187, 0.06348449124777374, 0.03627685214158499, 0.013603819553094372, 0.004534606517698124, 0.2298992343043053, 0.14241545487877322, 0.26041683177832814, 0.08748377942553211, 0.0895182859238003, 0.056966181951509286, 0.06713871444285023, 0.04679364946016834, 0.01627605198614551, 0.006103519494804566, 0.15021405296192758, 0.09012843177715654, 0.12017124236954206, 0.15021405296192758, 0.21029967414669862, 0.18025686355431308, 0.030042810592385515, 0.030042810592385515, 0.030042810592385515, 0.17891764964327914, 0.08945882482163957, 0.08945882482163957, 0.17891764964327914, 0.17891764964327914, 0.08945882482163957, 0.08945882482163957, 0.08945882482163957, 0.1866274463098511, 0.14355957408450085, 0.11484765926760068, 0.15791553149295093, 0.11484765926760068, 0.07177978704225042, 0.11484765926760068, 0.04306787222535025, 0.02871191481690017, 0.26359785525743784, 0.26359785525743784, 0.20812710689537664, 0.20245091307095728, 0.16650168551630132, 0.1229841995290862, 0.07000639050117215, 0.09460323040698938, 0.05486987363605384, 0.04162542137907533, 0.03216509833837639, 0.005676193824419363, 0.34254788632244165, 0.15691378500274952, 0.31382757000549905, 0.07845689250137476, 0.07845689250137476, 0.15691378500274952, 0.07845689250137476, 0.07845689250137476, 0.24681011741275513, 0.49362023482551026, 0.20599688852729184, 0.16348959406927924, 0.18801303317967114, 0.12098229961126664, 0.10463334020433872, 0.08010990109394683, 0.057221357924247734, 0.05068177416147657, 0.024523439110391888, 0.003269791881385585, 0.1110866292872973, 0.1110866292872973, 0.33325988786189187, 0.1110866292872973, 0.1110866292872973, 0.1110866292872973, 0.1110866292872973, 0.21059550783128483, 0.260611940941215, 0.13162219239455303, 0.09476797852407817, 0.1395195239382262, 0.05001643310993015, 0.031589326174692726, 0.05791376465360333, 0.018427106935237424, 0.005264887695782121, 0.21080087940251535, 0.18884245446475334, 0.1493172895767817, 0.08783369975104806, 0.10979212468881008, 0.07905032977594326, 0.061483589825733646, 0.07905032977594326, 0.030741794912866823, 0.0043916849875524034, 0.17024420266820914, 0.20298347241209552, 0.27500986584864556, 0.1047656631804364, 0.09166995528288184, 0.0523828315902182, 0.04583497764144092, 0.03273926974388637, 0.0261914157951091, 0.006547853948777275, 0.20179296750930323, 0.23542512876085378, 0.18310843348066405, 0.11210720417183513, 0.08221194972601244, 0.05231669528018973, 0.06726432250310108, 0.04110597486300622, 0.022421440834367028, 0.0037369068057278377, 0.20594090183831096, 0.4118818036766219, 0.19250669509020676, 0.1283377967268045, 0.38501339018041353, 0.06416889836340225, 0.06416889836340225, 0.06416889836340225, 0.06416889836340225, 0.2222034329848703, 0.19949651282583247, 0.18003343840380004, 0.1102907550581838, 0.10055921784716758, 0.06812076047711352, 0.051901531792086494, 0.035682303107059464, 0.02757268876454595, 0.004865768605508109, 0.13054526957103044, 0.13054526957103044, 0.17949974566016685, 0.11422711087465165, 0.21213606305292448, 0.06527263478551522, 0.09790895217827283, 0.03263631739275761, 0.03263631739275761, 0.19067409571681385, 0.2860111435752208, 0.09533704785840692, 0.09533704785840692, 0.09533704785840692, 0.09533704785840692, 0.09533704785840692, 0.3843629893661681, 0.14749137560732314, 0.21118083325593998, 0.17095591263576093, 0.17430798935410918, 0.0938581481137511, 0.07374568780366157, 0.0502811507752238, 0.043576997338527296, 0.03016869046513428, 0.0033520767183482536, 0.2226359696913987, 0.2226359696913987, 0.11131798484569935, 0.11131798484569935, 0.11131798484569935, 0.055658992422849676, 0.11131798484569935, 0.055658992422849676, 0.25382065555422245, 0.13907981126258764, 0.14255680654415231, 0.16341877823354048, 0.07997089147598789, 0.06258591506816444, 0.06258591506816444, 0.06258591506816444, 0.024338966970952836, 0.006953990563129382, 0.21449012381149263, 0.21449012381149263, 0.21449012381149263, 0.2326883756048372, 0.16325716675500673, 0.2101701457075949, 0.09945551537948687, 0.09570247706327982, 0.060048613059312826, 0.052542536426898725, 0.04878949811069167, 0.030024306529656413, 0.005629557474310577, 0.1974722834468299, 0.17247579187128181, 0.14497965113817893, 0.12248280872018565, 0.1424800019806241, 0.07748912388419908, 0.05749193062376061, 0.04999298315109618, 0.0274961407331029, 0.004999298315109618, 0.1670317560614802, 0.1670317560614802, 0.0835158780307401, 0.1670317560614802, 0.0835158780307401, 0.0835158780307401, 0.1670317560614802, 0.0835158780307401, 0.19645369753246206, 0.13096913168830804, 0.13096913168830804, 0.13096913168830804, 0.13096913168830804, 0.06548456584415402, 0.06548456584415402, 0.13096913168830804, 0.27205936492749394, 0.20404452369562048, 0.09522077772462288, 0.08161780947824819, 0.08161780947824819, 0.08161780947824819, 0.054411872985498794, 0.09522077772462288, 0.027205936492749397, 0.3862171956840923, 0.35120221776572935, 0.12770989736935615, 0.1915648460540342, 0.06385494868467807, 0.06385494868467807, 0.06385494868467807, 0.06385494868467807, 0.031927474342339036, 0.031927474342339036, 0.35967206488744274, 0.08991801622186069, 0.20980870451767492, 0.05994534414790712, 0.05994534414790712, 0.02997267207395356, 0.05994534414790712, 0.05994534414790712, 0.02997267207395356, 0.19411052059875225, 0.09705526029937613, 0.09705526029937613, 0.19411052059875225, 0.1455828904490642, 0.04852763014968806, 0.1455828904490642, 0.04852763014968806, 0.04852763014968806, 0.17979500755438094, 0.18895123479094664, 0.18062739184861418, 0.11986333836958729, 0.09738896242528967, 0.09822134671952291, 0.049943057653994705, 0.049943057653994705, 0.03163060318086331, 0.004161921471166226, 0.1683998137528014, 0.23044185039857037, 0.1329472213837906, 0.10813040672548302, 0.12231144367308734, 0.07799570321182381, 0.06381466626421949, 0.04786099969816461, 0.04608837007971407, 0.0035452592369010823, 0.1979779135578714, 0.11313023631878365, 0.14141279539847956, 0.16969535447817546, 0.14141279539847956, 0.028282559079695912, 0.056565118159391824, 0.11313023631878365, 0.028282559079695912, 0.14191016296664055, 0.0946067753110937, 0.14191016296664055, 0.1892135506221874, 0.0946067753110937, 0.04730338765554685, 0.0946067753110937, 0.0946067753110937, 0.04730338765554685, 0.40048412626539637, 0.21629211704208404, 0.21629211704208404, 0.07209737234736134, 0.14419474469472268, 0.07209737234736134, 0.07209737234736134, 0.07209737234736134, 0.07209737234736134, 0.15431905548224825, 0.20575874064299765, 0.10287937032149883, 0.15431905548224825, 0.10287937032149883, 0.05143968516074941, 0.10287937032149883, 0.10287937032149883, 0.1751712874052255, 0.11678085827015033, 0.08758564370261275, 0.20436650197276307, 0.1459760728376879, 0.05839042913507517, 0.05839042913507517, 0.11678085827015033, 0.029195214567537583, 0.20960039716641005, 0.08384015886656401, 0.12576023829984603, 0.16768031773312803, 0.12576023829984603, 0.08384015886656401, 0.041920079433282006, 0.12576023829984603, 0.041920079433282006, 0.12372310252376677, 0.24744620504753354, 0.12372310252376677, 0.12372310252376677, 0.12372310252376677, 0.12372310252376677, 0.1178164389877091, 0.08247150729139636, 0.20028794627910548, 0.2474145218741891, 0.1178164389877091, 0.14137972678525093, 0.03534493169631273, 0.03534493169631273, 0.01178164389877091, 0.13494926009924124, 0.08996617339949417, 0.2024238901488619, 0.24740697684860896, 0.13494926009924124, 0.1124577167493677, 0.022491543349873543, 0.044983086699747085, 0.022491543349873543, 0.4346373153163541, 0.21731865765817704, 0.21731865765817704, 0.17162684464573394, 0.17162684464573394, 0.1144178964304893, 0.28604474107622324, 0.1144178964304893, 0.05720894821524465, 0.1144178964304893, 0.05720894821524465, 0.09347749762461095, 0.12463666349948126, 0.15579582937435157, 0.1869549952492219, 0.12463666349948126, 0.15579582937435157, 0.06231833174974063, 0.06231833174974063, 0.031159165874870314, 0.21873120075205854, 0.10936560037602927, 0.10936560037602927, 0.3280968011280878, 0.10936560037602927, 0.10936560037602927, 0.3472405920092337, 0.17362029600461684, 0.3472405920092337, 0.3770188736294979, 0.1413820776110617, 0.09425471840737447, 0.09425471840737447, 0.047127359203687234, 0.047127359203687234, 0.09425471840737447, 0.047127359203687234, 0.15335743531578552, 0.15335743531578552, 0.3450542294605174, 0.07667871765789276, 0.11501807648683914, 0.07667871765789276, 0.03833935882894638, 0.03833935882894638, 0.03833935882894638, 0.3061087199176016, 0.16610788588861303, 0.08542691274271527, 0.11390255032362036, 0.11232057045801452, 0.25786271809375166, 0.09808275166756197, 0.06802513422105105, 0.053787315430598504, 0.03954949664014596, 0.0063279194624233535, 0.1706872000259659, 0.15488282965319128, 0.11695234075853218, 0.13591758520586172, 0.17384807410052083, 0.07586097778931818, 0.07586097778931818, 0.05689573334198863, 0.03793048889465909, 0.003160874074554924, 0.2250995472125529, 0.12259886053540828, 0.1145595909921028, 0.12862831269288735, 0.1607853908661092, 0.08441233020470733, 0.07235342588974915, 0.05828470418896459, 0.02411780862991638, 0.006029452157479095, 0.11996851242514364, 0.05998425621257182, 0.11996851242514364, 0.11996851242514364, 0.23993702485028728, 0.17995276863771545, 0.05998425621257182, 0.05998425621257182, 0.16041376566175947, 0.15343925411124817, 0.13251571945971435, 0.10461767325766921, 0.23015888116687228, 0.06277060395460153, 0.07671962705562409, 0.04184706930306769, 0.027898046202045123, 0.006974511550511281, 0.1694442342562512, 0.14211451905363004, 0.14211451905363004, 0.12025074689153312, 0.19677394945887236, 0.06012537344576656, 0.07652320256733926, 0.05465943040524232, 0.03279565824314539, 0.005465943040524232, 0.3881172957238076, 0.17213516796449066, 0.19126129773832296, 0.13866444086028415, 0.14344597330374223, 0.11475677864299377, 0.06694145420841303, 0.09563064886916148, 0.043033791991122665, 0.03347072710420652, 0.004781532443458074, 0.14746753243641972, 0.23226136358736105, 0.14746753243641972, 0.16958766230188266, 0.09216720777276231, 0.05898701297456788, 0.06636038959638887, 0.0516136363527469, 0.02580681817637345, 0.007373376621820985, 0.20952248631514192, 0.22387334154220645, 0.1693400916793613, 0.12628752599816773, 0.0803647892715613, 0.06601393404449678, 0.048792907772019355, 0.048792907772019355, 0.022961368363303224, 0.005740342090825806, 0.15374535994940683, 0.16472717137436446, 0.16472717137436446, 0.1098181142495763, 0.12079992567453393, 0.12079992567453393, 0.07687267997470341, 0.05490905712478815, 0.03294543427487289, 0.010981811424957631, 0.29897757543962583, 0.1724870627536303, 0.15523835647826725, 0.07474439385990646, 0.10349223765217817, 0.06899482510145212, 0.05749568758454343, 0.04599655006763474, 0.02299827503381737, 0.005749568758454343, 0.17752119781022663, 0.35504239562045325, 0.17752119781022663, 0.17752119781022663, 0.2148830117986472, 0.15933502235409894, 0.15494965476637143, 0.12132850326046066, 0.1271756600440973, 0.07747482738318572, 0.0657805138159124, 0.05116262185682076, 0.021926837938637467, 0.004385367587727494, 0.17348453277754902, 0.16925320270980393, 0.14386522230333332, 0.23272315372598038, 0.10155192162588235, 0.055007290880686274, 0.055007290880686274, 0.03808197060970588, 0.029619310474215687, 0.004231330067745098, 0.11128600904792117, 0.13747095235331439, 0.1440171881796627, 0.24221072557488724, 0.19638707479044912, 0.06546235826348304, 0.045823650784438126, 0.03273117913174152, 0.03273117913174152, 0.006546235826348304, 0.23527788375030897, 0.23527788375030897, 0.23527788375030897, 0.23527788375030897, 0.1977865101836505, 0.2026701277190493, 0.16238028305200938, 0.1367412909911658, 0.09278873317257678, 0.05982431480863504, 0.06348702796018411, 0.04761527097013809, 0.030522609596242364, 0.006104521919248473, 0.14272893345262352, 0.14272893345262352, 0.14272893345262352, 0.14272893345262352, 0.11894077787718628, 0.09515262230174902, 0.11894077787718628, 0.04757631115087451, 0.023788155575437255, 0.12019212393639717, 0.22945769115130368, 0.17209326836347777, 0.11472884557565184, 0.12019212393639717, 0.07921753623080723, 0.07648589705043456, 0.05736442278782592, 0.02731639180372663, 0.005463278360745326, 0.25223942229363394, 0.22455460765164972, 0.13842407320992106, 0.08920662495750468, 0.12304362063104093, 0.058445719799744444, 0.049217448252416376, 0.03691308618931228, 0.024608724126208188, 0.006152181031552047, 0.14439698286832917, 0.14439698286832917, 0.4331909486049875, 0.14439698286832917, 0.14439698286832917, 0.17804749001967154, 0.19076516787821954, 0.10174142286838375, 0.15685136025542495, 0.13565523049117834, 0.0720668411984385, 0.08054529310413713, 0.055109937387041195, 0.02967458166994526, 0.004239225952849323, 0.33827154932766057, 0.3352291314710042, 0.3352291314710042, 0.1676145657355021, 0.1676145657355021, 0.28248737648025535, 0.14124368824012767, 0.14124368824012767, 0.28248737648025535, 0.14124368824012767, 0.14124368824012767, 0.20784310159017605, 0.18186271389140404, 0.1484593582786972, 0.14474787432172975, 0.10763303475205546, 0.06680671122541373, 0.04824929144057658, 0.04453780748360916, 0.04453780748360916, 0.0037114839569674296, 0.21365336988729203, 0.19185200561307855, 0.1526095499194943, 0.08284518424201119, 0.11772736708075275, 0.10464654851622467, 0.05668354711295503, 0.04796300140326964, 0.026161637129056167, 0.004360272854842694, 0.19121562742692538, 0.1743436603010202, 0.13778773152822563, 0.16028368769609921, 0.10685579179739947, 0.07873584658755751, 0.053427895898699736, 0.06467587398263652, 0.028119945209841967, 0.005623989041968393, 0.10401843198069885, 0.10401843198069885, 0.10401843198069885, 0.10401843198069885, 0.2080368639613977, 0.2080368639613977, 0.14783704526516264, 0.1182696362121301, 0.17740445431819515, 0.14783704526516264, 0.14783704526516264, 0.17740445431819515, 0.05913481810606505, 0.029567409053032526, 0.029567409053032526, 0.320282911525646, 0.0800707278814115, 0.160141455762823, 0.0800707278814115, 0.0800707278814115, 0.0800707278814115, 0.0800707278814115, 0.0800707278814115, 0.0800707278814115, 0.09560390211853906, 0.15933983686423175, 0.35054764110130987, 0.09560390211853906, 0.09560390211853906, 0.031867967372846355, 0.06373593474569271, 0.031867967372846355, 0.031867967372846355, 0.19524938524029492, 0.19524938524029492, 0.19524938524029492, 0.19524938524029492, 0.19524938524029492, 0.19524938524029492, 0.18979680935914456, 0.11387808561548673, 0.32265457591054575, 0.11387808561548673, 0.09489840467957228, 0.07591872374365782, 0.03795936187182891, 0.03795936187182891, 0.018979680935914456, 0.24714072900563372, 0.24714072900563372, 0.24714072900563372, 0.15919550593431336, 0.2175671914435616, 0.14858247220535914, 0.15919550593431336, 0.11674337101849647, 0.0663314608059639, 0.05306516864477112, 0.05306516864477112, 0.02653258432238556, 0.005306516864477112, 0.12694994656229813, 0.1777299251872174, 0.12694994656229813, 0.20311991449967703, 0.07616996793737889, 0.07616996793737889, 0.12694994656229813, 0.02538998931245963, 0.05077997862491926, 0.20379492484626852, 0.13586328323084568, 0.3396582080771142, 0.06793164161542284, 0.06793164161542284, 0.06793164161542284, 0.06793164161542284, 0.06793164161542284, 0.22182580753428943, 0.22182580753428943, 0.08873032301371578, 0.08873032301371578, 0.08873032301371578, 0.04436516150685789, 0.08873032301371578, 0.04436516150685789, 0.3863452143332797, 0.1412886007734242, 0.1412886007734242, 0.16954632092810903, 0.1412886007734242, 0.16954632092810903, 0.16954632092810903, 0.028257720154684838, 0.028257720154684838, 0.028257720154684838, 0.1401642080063492, 0.1401642080063492, 0.1401642080063492, 0.1401642080063492, 0.1401642080063492, 0.2102463120095238, 0.28467868741410457, 0.28467868741410457, 0.34643794241431913, 0.34643794241431913, 0.280487675046958, 0.21210738603429885, 0.08733833542588777, 0.16219976579093442, 0.1497228607300933, 0.11229214554756999, 0.04990762024336444, 0.07486143036504665, 0.12476905060841109, 0.02495381012168222, 0.20913457167734692, 0.20913457167734692, 0.3240558240071077, 0.10801860800236923, 0.14402481066982564, 0.12602170933609744, 0.09001550666864103, 0.054009304001184616, 0.07201240533491282, 0.054009304001184616, 0.03600620266745641, 0.15576563762739434, 0.19470704703424294, 0.18172657723196006, 0.11682422822054576, 0.07788281881369717, 0.07788281881369717, 0.07788281881369717, 0.05192187920913145, 0.05192187920913145, 0.2958692766432386, 0.1775223166248278, 0.1775223166248278, 0.1775223166248278, 0.1775223166248278, 0.1775223166248278, 0.16793801603611955, 0.1883826440752993, 0.24533553647015724, 0.09638181789899035, 0.10952479306703448, 0.06717520641444781, 0.04673057837526805, 0.0496512395237223, 0.026285950336088274, 0.004380991722681379, 0.15629855053918473, 0.1816442614374309, 0.15629855053918473, 0.12250426934152316, 0.13517712479064625, 0.08026141784444621, 0.08026141784444621, 0.050691421796492345, 0.029569996047953868, 0.00844857029941539, 0.5388966112553141, 0.1749016642846674, 0.1749016642846674, 0.0874508321423337, 0.0874508321423337, 0.1749016642846674, 0.0874508321423337, 0.1749016642846674, 0.17648327967201732, 0.17648327967201732, 0.07059331186880693, 0.07059331186880693, 0.2117799356064208, 0.07059331186880693, 0.14118662373761387, 0.03529665593440347, 0.03529665593440347, 0.2956746489957716, 0.2956746489957716, 0.36987247526066835, 0.18102981211006675, 0.18102981211006675, 0.18102981211006675, 0.18102981211006675, 0.18102981211006675, 0.18102981211006675, 0.3805176904017567, 0.0878117747080977, 0.1463529578468295, 0.0878117747080977, 0.0878117747080977, 0.029270591569365897, 0.0878117747080977, 0.0878117747080977, 0.029270591569365897, 0.4340539731174091, 0.21702698655870456, 0.21702698655870456, 0.17048738177356235, 0.3409747635471247, 0.11365825451570823, 0.11365825451570823, 0.05682912725785411, 0.05682912725785411, 0.05682912725785411, 0.05682912725785411, 0.20396368221081043, 0.16996973517567537, 0.16735481617297268, 0.1333608691378376, 0.10198184110540522, 0.06537297506756745, 0.06537297506756745, 0.06275805606486475, 0.02614919002702698, 0.005229838005405396, 0.23394591930996922, 0.2754524533810928, 0.16979945756368733, 0.07169310430466799, 0.06791978302547494, 0.052826497908702724, 0.04150653407112357, 0.06414646174628189, 0.018866606395965258, 0.003773321279193052, 0.24047815066262956, 0.1448663558208612, 0.16514764563578174, 0.11589308465668895, 0.14196902870444394, 0.052151888095510024, 0.06663852367759614, 0.03476792539700668, 0.03187059828058946, 0.005794654232834447, 0.3256606742727124, 0.29309460684544114, 0.032566067427271236, 0.06513213485454247, 0.032566067427271236, 0.06513213485454247, 0.06513213485454247, 0.032566067427271236, 0.032566067427271236, 0.12401098988323722, 0.16534798651098295, 0.16534798651098295, 0.04133699662774574, 0.16534798651098295, 0.2066849831387287, 0.04133699662774574, 0.04133699662774574, 0.04133699662774574, 0.4000967912677739, 0.07002371152557946, 0.303436083277511, 0.16338866022635207, 0.09336494870077261, 0.09336494870077261, 0.11670618587596576, 0.07002371152557946, 0.046682474350386305, 0.17301054656739237, 0.34602109313478474, 0.17301054656739237, 0.17301054656739237, 0.17301054656739237, 0.3347745268169966, 0.16371624536109755, 0.12278718402082316, 0.10232265335068597, 0.16371624536109755, 0.18418077603123473, 0.14325171469096035, 0.04092906134027439, 0.06139359201041158, 0.020464530670137194, 0.26058268849583377, 0.26058268849583377, 0.2789513692919066, 0.17966359378122798, 0.17020761516116334, 0.09928777551067862, 0.07091983965048473, 0.06619185034045241, 0.05200788241035547, 0.042551903790290835, 0.033095925170226206, 0.004727989310032315, 0.12690292912127143, 0.222080125962225, 0.2961068346163, 0.11632768502783215, 0.09517719684095358, 0.04758859842047679, 0.0370133543270375, 0.04230097637375715, 0.01586286614015893, 0.005287622046719643, 0.16862211189900403, 0.13796354609918512, 0.1073049802993662, 0.13796354609918512, 0.2146099605987324, 0.09197569739945674, 0.07664641449954729, 0.04598784869972837, 0.030658565799818915, 0.3374505717852476, 0.13498022871409904, 0.13498022871409904, 0.06749011435704952, 0.13498022871409904, 0.06749011435704952, 0.06749011435704952, 0.06749011435704952, 0.1996159266515484, 0.1235717641176252, 0.1948631664931782, 0.12832452427599542, 0.0855496828506636, 0.0998079633257742, 0.0617858820588126, 0.0760441625339232, 0.0285165609502212, 0.0047527601583702, 0.20305792688761792, 0.13291064305371356, 0.20121194573409412, 0.11629681267199936, 0.10891288805790415, 0.07753120844799957, 0.07199326498742817, 0.05168747229866638, 0.029535698456380786, 0.005537943460571398, 0.12646398042053378, 0.1686186405607117, 0.12646398042053378, 0.25292796084106756, 0.04215466014017793, 0.08430932028035586, 0.12646398042053378, 0.04215466014017793, 0.04215466014017793, 0.1447097188779342, 0.17365166265352106, 0.33283235341924866, 0.10129680321455393, 0.08682583132676053, 0.0723548594389671, 0.043412915663380264, 0.02894194377558684, 0.02894194377558684, 0.2897900792133187, 0.14489503960665934, 0.2897900792133187, 0.14489503960665934, 0.30468887196576605, 0.2509202475012191, 0.14338299857212522, 0.07169149928606261, 0.07169149928606261, 0.035845749643031305, 0.05376862446454695, 0.035845749643031305, 0.017922874821515652, 0.17566374298265175, 0.3513274859653035, 0.17566374298265175, 0.17566374298265175, 0.17566374298265175, 0.4002541995622795, 0.1397568386769968, 0.1397568386769968, 0.41927051603099036, 0.1397568386769968, 0.0698784193384984, 0.0698784193384984, 0.0698784193384984, 0.25193200490790335, 0.25193200490790335, 0.11306411705378173, 0.11306411705378173, 0.33919235116134516, 0.11306411705378173, 0.11306411705378173, 0.11306411705378173, 0.290370000587944, 0.290370000587944, 0.4473561541303014, 0.2236780770651507, 0.2207344099535228, 0.20496766638541405, 0.26803464065784915, 0.07095034605648948, 0.0551836024883807, 0.0551836024883807, 0.039416858920271934, 0.039416858920271934, 0.02365011535216316, 0.007883371784054386, 0.0754898376676487, 0.3774491883382435, 0.1509796753352974, 0.0754898376676487, 0.0754898376676487, 0.1509796753352974, 0.0754898376676487, 0.0754898376676487, 0.1832558217772941, 0.1832558217772941, 0.09162791088864705, 0.09162791088864705, 0.09162791088864705, 0.1832558217772941, 0.09162791088864705, 0.13928294916140874, 0.13928294916140874, 0.41784884748422624, 0.13928294916140874, 0.11638473239660015, 0.2327694647932003, 0.3200580140906504, 0.07274045774787509, 0.13093282394617517, 0.029096183099150037, 0.04364427464872506, 0.05819236619830007, 0.15929458685842493, 0.07964729342921247, 0.07964729342921247, 0.07964729342921247, 0.23894188028763738, 0.15929458685842493, 0.07964729342921247, 0.07964729342921247, 0.14398647968788417, 0.23092171270698403, 0.20103772635666844, 0.1032355892101811, 0.1222526714331092, 0.046184342541396806, 0.06791815079617178, 0.05705124666878429, 0.024450534286621837, 0.002716726031846871, 0.2865838911737139, 0.2865838911737139, 0.16249630644057053, 0.16249630644057053, 0.1986065967606973, 0.12036763440042261, 0.12638601612044376, 0.08425734408029582, 0.05416543548019018, 0.04212867204014791, 0.04212867204014791, 0.006018381720021131, 0.3023205703556628, 0.3023205703556628, 0.3013393536987022, 0.14349493033271535, 0.15784442336598686, 0.0860969581996292, 0.10044645123290073, 0.05739797213308614, 0.0860969581996292, 0.0430484790998146, 0.02869898606654307, 0.06228256354100673, 0.1868476906230202, 0.1868476906230202, 0.12456512708201346, 0.31141281770503365, 0.06228256354100673, 0.06228256354100673, 0.06228256354100673, 0.22774267493212108, 0.22774267493212108, 0.22774267493212108, 0.38681449680897234, 0.11577774088974213, 0.11577774088974213, 0.11577774088974213, 0.23155548177948426, 0.23155548177948426, 0.23155548177948426, 0.12593098337543063, 0.08395398891695376, 0.1679079778339075, 0.08395398891695376, 0.2098849722923844, 0.12593098337543063, 0.04197699445847688, 0.04197699445847688, 0.08395398891695376, 0.2011058589813632, 0.188536742795028, 0.125691161863352, 0.16339851042235762, 0.09636322409523654, 0.06703528632712107, 0.062845580931676, 0.0502764647453408, 0.0377073485590056, 0.004189705395445067, 0.21338224889396404, 0.15344341493498537, 0.1750213951602177, 0.12467277463467562, 0.10549234776780245, 0.08391436754257013, 0.06952904739241525, 0.04315596045046464, 0.028770640300309756, 0.004795106716718293, 0.20694350137187764, 0.18185944059952883, 0.17558842540644162, 0.11601378107211321, 0.10660725828248242, 0.06898116712395921, 0.05016812154469761, 0.06271015193087201, 0.025084060772348805, 0.0031355075965436006, 0.11487275822690877, 0.14769354629173986, 0.09846236419449324, 0.2461559104862331, 0.11487275822690877, 0.11487275822690877, 0.06564157612966216, 0.06564157612966216, 0.01641039403241554, 0.35454803715133754, 0.35454803715133754, 0.16308541262128926, 0.13590451051774105, 0.13590451051774105, 0.13590451051774105, 0.13590451051774105, 0.21744721682838566, 0.054361804207096416, 0.027180902103548208, 0.027180902103548208, 0.16739082687116402, 0.33478165374232804, 0.08369541343558201, 0.08369541343558201, 0.16739082687116402, 0.08369541343558201, 0.08369541343558201, 0.14592049327335824, 0.23285185096812483, 0.1924908634669832, 0.08382666634852494, 0.12315275673425269, 0.08486156346393883, 0.06519851827107495, 0.04657037019362497, 0.018628148077449988, 0.005174485577069441, 0.13749898406440925, 0.2512912467384031, 0.16120570545482463, 0.1280162955082431, 0.12327495123016001, 0.07112016417124616, 0.05215478705891385, 0.04741344278083077, 0.023706721390415384, 0.004741344278083077, 0.14727522541214993, 0.36818806353037487, 0.14727522541214993, 0.07363761270607497, 0.07363761270607497, 0.036818806353037484, 0.036818806353037484, 0.07363761270607497, 0.1427372977870592, 0.33305369483647146, 0.09515819852470614, 0.11894774815588267, 0.09515819852470614, 0.04757909926235307, 0.04757909926235307, 0.0713686488935296, 0.023789549631176535, 0.1800473616508434, 0.1800473616508434, 0.10802841699050604, 0.1440378893206747, 0.07201894466033736, 0.07201894466033736, 0.1440378893206747, 0.03600947233016868, 0.03600947233016868, 0.2785924438191249, 0.09286414793970829, 0.09286414793970829, 0.2785924438191249, 0.09286414793970829, 0.32423821032933936, 0.18157339778443005, 0.11672575571856217, 0.1556343409580829, 0.0518781136526943, 0.06484764206586788, 0.0518781136526943, 0.02593905682634715, 0.012969528413173574, 0.20175249100938214, 0.20175249100938214, 0.4035049820187643, 0.20175249100938214, 0.12428210727975511, 0.12428210727975511, 0.4349873754791429, 0.09321158045981633, 0.062141053639877555, 0.062141053639877555, 0.031070526819938778, 0.062141053639877555, 0.031070526819938778, 0.19967972319992758, 0.16205890578544846, 0.18521017804051254, 0.15627108772168244, 0.09839290708402229, 0.05209036257389415, 0.05209036257389415, 0.05498427160577716, 0.0347269083825961, 0.0028939090318830085, 0.2291955568394704, 0.1330812910680796, 0.1478681011867551, 0.17004831636476836, 0.07393405059337756, 0.0665406455340398, 0.059147240474702045, 0.08872086071205307, 0.022180215178013267, 0.007393405059337756, 0.20225933144118044, 0.17927531650468267, 0.13790408961898667, 0.15169449858088532, 0.11951687766978844, 0.06435524182219378, 0.06895204480949334, 0.03677442389839644, 0.03677442389839644, 0.00919360597459911, 0.20097533474444793, 0.1844568140805207, 0.17344446697123586, 0.09911112398356335, 0.11838273142481179, 0.08259260331963612, 0.05506173554642409, 0.052308648769102885, 0.03028395455053325, 0.005506173554642409, 0.1682122299763474, 0.18923875872339083, 0.15139100697871266, 0.12195386673285187, 0.147185701229304, 0.07149019773994765, 0.058874280491721595, 0.05046366899290422, 0.037847751744678165, 0.004205305749408686, 0.1684678642827113, 0.1684678642827113, 0.1684678642827113, 0.1684678642827113, 0.1684678642827113, 0.1684678642827113, 0.36726993675325725, 0.25106162046230146, 0.15827797811753788, 0.17465156206073146, 0.1036993649735593, 0.13371760220274753, 0.04912075182958072, 0.06003647445837644, 0.043662890515182864, 0.021831445257591432, 0.002728930657198929, 0.22628073807621152, 0.22628073807621152, 0.22628073807621152, 0.22628073807621152, 0.22628073807621152, 0.2018729863499036, 0.1284646276772114, 0.22022507601807667, 0.08258440350677876, 0.1009364931749518, 0.1009364931749518, 0.1009364931749518, 0.036704179336346114, 0.027528134502259584, 0.009176044834086529, 0.2015321847082909, 0.06717739490276363, 0.13435478980552726, 0.2015321847082909, 0.13435478980552726, 0.06717739490276363, 0.06717739490276363, 0.13435478980552726, 0.14504994311611047, 0.4351498293483314, 0.14504994311611047, 0.14504994311611047, 0.14504994311611047, 0.15400555531972193, 0.2566759255328699, 0.15400555531972193, 0.15400555531972193, 0.05133518510657398, 0.05133518510657398, 0.10267037021314795, 0.05133518510657398, 0.16504411683636075, 0.1561228132235845, 0.21634161260982424, 0.12935890238525571, 0.10036466564373289, 0.06690977709582192, 0.06467945119262786, 0.06690977709582192, 0.028994236741522836, 0.006690977709582193, 0.11978588230590537, 0.17967882345885805, 0.17967882345885805, 0.11978588230590537, 0.05989294115295268, 0.05989294115295268, 0.11978588230590537, 0.17967882345885805, 0.05989294115295268, 0.26372089954439304, 0.2604243883000881, 0.16482556221524566, 0.06922673613040319, 0.06922673613040319, 0.05274417990887861, 0.04285464617596387, 0.04615115742026878, 0.026372089954439307, 0.0032965112443049133, 0.3599922528041397, 0.3599922528041397, 0.364654875386388, 0.182327437693194, 0.182327437693194, 0.21159985879956877, 0.42319971759913755, 0.21159985879956877, 0.21159985879956877, 0.17641673700420388, 0.17641673700420388, 0.08820836850210194, 0.08820836850210194, 0.17641673700420388, 0.08820836850210194, 0.17641673700420388, 0.08820836850210194, 0.08820836850210194, 0.1391391680998094, 0.15305308490979033, 0.20870875214971407, 0.09739741766986657, 0.05565566723992375, 0.05565566723992375, 0.12522525128982845, 0.1391391680998094, 0.027827833619961877, 0.013913916809980938, 0.16812813813259622, 0.2119876524280561, 0.16447317860797456, 0.13888846193562296, 0.10964878573864971, 0.06944423096781148, 0.06213431191856817, 0.04385951429545988, 0.029239676196973256, 0.007309919049243314, 0.3886111346616484, 0.16980431580026098, 0.16980431580026098, 0.16980431580026098, 0.16980431580026098, 0.16980431580026098, 0.10619741770174469, 0.10619741770174469, 0.10619741770174469, 0.15929612655261705, 0.21239483540348938, 0.15929612655261705, 0.053098708850872346, 0.053098708850872346, 0.21446731786673207, 0.17157385429338565, 0.08578692714669282, 0.17157385429338565, 0.12868039072003923, 0.08578692714669282, 0.12868039072003923, 0.04289346357334641, 0.19512018912751175, 0.160432155504843, 0.13441613028784144, 0.11273610927367346, 0.12574412188217424, 0.10406410086800627, 0.0737120714481711, 0.052032050434003135, 0.03468803362266876, 0.004336004202833595, 0.19574093095517678, 0.1631174424626473, 0.09787046547758839, 0.22836441944770622, 0.08155872123132366, 0.09787046547758839, 0.048935232738794195, 0.048935232738794195, 0.03262348849252946, 0.3897590636476513, 0.3912131035781762, 0.17533412827814895, 0.17533412827814895, 0.08766706413907448, 0.08766706413907448, 0.2630011924172234, 0.08766706413907448, 0.08766706413907448, 0.3548023277494464, 0.1774011638747232, 0.1774011638747232, 0.1774011638747232, 0.17964785341557618, 0.17964785341557618, 0.31438374347725834, 0.06736794503084106, 0.08982392670778809, 0.06736794503084106, 0.044911963353894045, 0.044911963353894045, 0.17266682911612294, 0.1371177760628035, 0.14727464836375193, 0.17774526526659715, 0.14727464836375193, 0.08633341455806147, 0.05586279765521624, 0.05078436150474204, 0.02539218075237102, 0.005078436150474204, 0.13504829658886702, 0.3038586673249508, 0.20257244488330053, 0.06752414829443351, 0.1688103707360838, 0.033762074147216756, 0.050643111220825134, 0.050643111220825134, 0.2043539786855862, 0.35761946269977585, 0.13283008614563102, 0.08174159147423447, 0.08174159147423447, 0.04087079573711724, 0.030653096802837926, 0.04087079573711724, 0.030653096802837926, 0.01021769893427931, 0.29438388763546897, 0.1651421808686777, 0.1256516593566026, 0.11847156453622532, 0.0861611378445275, 0.05385071115282969, 0.07898104302415021, 0.0466706163324524, 0.02513033187132052, 0.007180094820377292, 0.19729418817147898, 0.14468240465908458, 0.1709882964152818, 0.14468240465908458, 0.13152945878098599, 0.0789176752685916, 0.05261178351239439, 0.05261178351239439, 0.026305891756197197, 0.013152945878098598, 0.19964286732189315, 0.16684439626186787, 0.15686225289577319, 0.13404592520184255, 0.14117602760619588, 0.04991071683047329, 0.06417092163917994, 0.03279847106002531, 0.048484696349602625, 0.005704081923482662, 0.40818589530692356, 0.17625953226327368, 0.17625953226327368, 0.17625953226327368, 0.35251906452654735, 0.14651442968433112, 0.18314303710541388, 0.1282001259737897, 0.10072867040797764, 0.09157151855270694, 0.1373572778290604, 0.027471455565812083, 0.11904297411851902, 0.06410006298689486, 0.15046107226068586, 0.13678279296425985, 0.12310451366783387, 0.0957479550749819, 0.10942623437140789, 0.13678279296425985, 0.027356558592851972, 0.15046107226068586, 0.054713117185703944, 0.16785439449554632, 0.13987866207962193, 0.22380585932739508, 0.11190292966369754, 0.05595146483184877, 0.08392719724777316, 0.08392719724777316, 0.11190292966369754, 0.027975732415924385, 0.20503763326412072, 0.19527393644201974, 0.12106984059405224, 0.08787327139890888, 0.16207736724687638, 0.09958970758543007, 0.05077122347492513, 0.048818484110504934, 0.02148013300862217, 0.005858218093260592, 0.35566860097423353, 0.0711337201948467, 0.1422674403896934, 0.0711337201948467, 0.0711337201948467, 0.0711337201948467, 0.0711337201948467, 0.0711337201948467, 0.1655898810261186, 0.25509792482402055, 0.15663907664632842, 0.08503264160800686, 0.14768827226653822, 0.044754021898950974, 0.06265563065853137, 0.04922942408884607, 0.026852413139370587, 0.004475402189895098, 0.15073066796699375, 0.2920406691860504, 0.13188933447111953, 0.06594466723555976, 0.16015133471493084, 0.07536533398349687, 0.05652400048762265, 0.03768266699174844, 0.028262000243811324, 0.00942066674793711, 0.35122360336322767, 0.35122360336322767, 0.23917618967627427, 0.1575749014337807, 0.1702371702989952, 0.1153673385497323, 0.12099501360093876, 0.06893901937727906, 0.0633113443260726, 0.03798680659564356, 0.022510700204825814, 0.00422075628840484, 0.14368685940244214, 0.14368685940244214, 0.19158247920325616, 0.19158247920325616, 0.09579123960162808, 0.09579123960162808, 0.04789561980081404, 0.09579123960162808, 0.04789561980081404, 0.3726208588373907, 0.09315521470934768, 0.18631042941869536, 0.09315521470934768, 0.09315521470934768, 0.09315521470934768, 0.09315521470934768, 0.15641633037331668, 0.15641633037331668, 0.35193674333996255, 0.07820816518665834, 0.07820816518665834, 0.07820816518665834, 0.07820816518665834, 0.03910408259332917, 0.03910408259332917, 0.2023222142745671, 0.2023222142745671, 0.2023222142745671, 0.2023222142745671, 0.2023222142745671, 0.29736235935063837, 0.15859911654636313, 0.17128704587007218, 0.1776310105319267, 0.1459111872226541, 0.1776310105319267, 0.05709568195669073, 0.050751717294836206, 0.03171982330927263, 0.025375858647418103, 0.006343964661854526, 0.10730998533440653, 0.10243225872829714, 0.14145407157717224, 0.18047588442604734, 0.21949769727492244, 0.13657634497106286, 0.05365499266720326, 0.01951090642443755, 0.029266359636656327, 0.004877726606109387, 0.16883108708492092, 0.19726579648869708, 0.17416259509812895, 0.12795619231699268, 0.12617902297925668, 0.08174978953585645, 0.05153791079434428, 0.04442923344340024, 0.021326032052832115, 0.0035543386754720192, 0.2246373034665116, 0.17616293798163277, 0.16197531881532679, 0.12768857249675394, 0.10640714374729496, 0.06857349263714564, 0.061479683053992644, 0.039015952707341484, 0.030739841526996322, 0.004729206388768665, 0.08441900384843486, 0.25325701154530456, 0.15476817372213056, 0.12662850577265228, 0.12662850577265228, 0.14069833974739143, 0.05627933589895657, 0.04220950192421743, 0.014069833974739142, 0.19534061661087407, 0.39068123322174814, 0.23070014982481432, 0.17302511236861073, 0.17014136049580056, 0.13265258614926823, 0.08362880431149519, 0.06344254120182394, 0.0547912855833934, 0.06632629307463411, 0.023070014982481433, 0.005767503745620358, 0.23807561515669382, 0.13676684274959006, 0.2076829834345627, 0.09117789516639338, 0.10637421102745893, 0.06585070206461743, 0.04558894758319669, 0.07091614068497262, 0.02532719310177594, 0.005065438620355188, 0.2021820517568331, 0.19693056989301924, 0.16542167871013616, 0.11553260100390463, 0.0892751916848354, 0.0787722279572077, 0.06826926422958, 0.0446375958424177, 0.03413463211479, 0.0052514818638138465, 0.10390502291275967, 0.23378630155370927, 0.28573881301008913, 0.07792876718456976, 0.18183379009732945, 0.025976255728189918, 0.03896438359228488, 0.051952511456379837, 0.2145903578932511, 0.24912213962319954, 0.1257943477305265, 0.1381271269197938, 0.09866223351413843, 0.0567307842706296, 0.041931449243508834, 0.04686456091921576, 0.022199002540681148, 0.004933111675706921, 0.2846682297989598, 0.2846682297989598, 0.2846682297989598, 0.11987339383330796, 0.19179743013329273, 0.14384807259996954, 0.09589871506664636, 0.23974678766661592, 0.04794935753332318, 0.07192403629998477, 0.02397467876666159, 0.04794935753332318, 0.22171255766322745, 0.14780837177548495, 0.17244310040473246, 0.08211576209749165, 0.13138521935598663, 0.05748103346824415, 0.10675049072673914, 0.041057881048745824, 0.024634728629247492, 0.008211576209749164, 0.32820484247908155, 0.15316225982357137, 0.10940161415969384, 0.08752129132775507, 0.0656409684958163, 0.08752129132775507, 0.08752129132775507, 0.0656409684958163, 0.02188032283193877, 0.3512151857338901, 0.2587901368565506, 0.1293950684282753, 0.036970019550935804, 0.0924250488773395, 0.018485009775467902, 0.0554550293264037, 0.036970019550935804, 0.018485009775467902, 0.19003593659058512, 0.19003593659058512, 0.19003593659058512, 0.09501796829529256, 0.09501796829529256, 0.09501796829529256, 0.09501796829529256, 0.09501796829529256, 0.1349287595418604, 0.1349287595418604, 0.1349287595418604, 0.2698575190837208, 0.1349287595418604, 0.18997232430603425, 0.18997232430603425, 0.13429078097495525, 0.12118924136764254, 0.1277400111712989, 0.0982615470548453, 0.05240615842925083, 0.045855388625594476, 0.03602923392010995, 0.003275384901828177, 0.11137487903254933, 0.11137487903254933, 0.11137487903254933, 0.11137487903254933, 0.334124637097648, 0.11137487903254933, 0.11137487903254933, 0.11137487903254933, 0.1286553424885382, 0.21442557081423033, 0.3430809133027685, 0.08577022832569213, 0.08577022832569213, 0.042885114162846065, 0.08577022832569213, 0.042885114162846065, 0.042885114162846065, 0.158722468414608, 0.124710510897192, 0.17005978758707999, 0.147385149242136, 0.11337319172472, 0.158722468414608, 0.045349276689888, 0.05668659586236, 0.034011957517416, 0.1837786851332916, 0.21747144407439506, 0.16233783853440759, 0.09801529873775551, 0.10414125490886524, 0.07351147405331664, 0.07963743022442636, 0.05207062745443262, 0.024503824684438878, 0.006125956171109719, 0.1809126705082928, 0.3618253410165856, 0.1809126705082928, 0.0904563352541464, 0.0452281676270732, 0.0452281676270732, 0.0452281676270732, 0.0452281676270732, 0.0452281676270732, 0.14969031217972983, 0.29938062435945967, 0.14969031217972983, 0.14969031217972983, 0.14969031217972983, 0.14969031217972983, 0.23863317621725566, 0.23863317621725566, 0.23863317621725566, 0.23863317621725566, 0.23863317621725566, 0.25564723684438234, 0.25564723684438234, 0.1285123249439909, 0.1285123249439909, 0.36717807126854546, 0.1285123249439909, 0.05507671069028182, 0.05507671069028182, 0.05507671069028182, 0.036717807126854546, 0.018358903563427273, 0.199471687752372, 0.299207531628558, 0.099735843876186, 0.099735843876186, 0.099735843876186, 0.099735843876186, 0.099735843876186, 0.099735843876186, 0.2613723587563609, 0.17675540807984116, 0.21436294171384992, 0.07709544394971796, 0.07145431390461665, 0.05077017040591183, 0.05641130045101314, 0.058291677132713576, 0.026325273543806132, 0.0056411300451013145, 0.19525164162451764, 0.19525164162451764, 0.19525164162451764, 0.19525164162451764, 0.19525164162451764, 0.19525164162451764, 0.2874624338400027, 0.14373121692000135, 0.14373121692000135, 0.14373121692000135, 0.14373121692000135, 0.2335002810101487, 0.25419017932750365, 0.1596077870195953, 0.08275959326941978, 0.12118369014450756, 0.03546839711546562, 0.03546839711546562, 0.0502468959135763, 0.020689898317354946, 0.00591139951924427, 0.29044931398135493, 0.29044931398135493, 0.29044931398135493, 0.29044931398135493, 0.3792552111037275, 0.32357466980259364, 0.1713042369543143, 0.1713042369543143, 0.038067608212069844, 0.0951690205301746, 0.038067608212069844, 0.05710141231810476, 0.05710141231810476, 0.019033804106034922, 0.2659072827712653, 0.22083825179308475, 0.09915186815199724, 0.13070018983672363, 0.13971399603235973, 0.04506903097818056, 0.02704141858690834, 0.04506903097818056, 0.018027612391272225, 0.009013806195636112, 0.19108338337160843, 0.16594083292797573, 0.18102636319415535, 0.12571275221816344, 0.14079828248434306, 0.06034212106471845, 0.06034212106471845, 0.04525659079853884, 0.020114040354906153, 0.010057020177453076, 0.26655170002350387, 0.13327585001175193, 0.13327585001175193, 0.13327585001175193, 0.13327585001175193, 0.13327585001175193, 0.13327585001175193, 0.18439574239950374, 0.18439574239950374, 0.18439574239950374, 0.3687914847990075], \"Term\": [\"ackley\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"ag\", \"ag\", \"ag\", \"ag\", \"ag\", \"ag\", \"ag\", \"ag\", \"ala\", \"ala\", \"ala\", \"ala\", \"ala\", \"ala\", \"ala\", \"ala\", \"alexandre\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"alverez\", \"alverez\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"annealing\", \"annealing\", \"annealing\", \"annealing\", \"annealing\", \"annealing\", \"annealing\", \"annealing\", \"annealing\", \"annotation\", \"annotation\", \"annotation\", \"annotation\", \"annotation\", \"annotation\", \"annotation\", \"annotation\", \"annotation\", \"annotation\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"assignment\", \"assignment\", \"assignment\", \"assignment\", \"assignment\", \"assignment\", \"assignment\", \"assignment\", \"assignment\", \"ate\", \"ate\", \"ate\", \"ate\", \"ate\", \"ate\", \"auc\", \"auc\", \"auc\", \"auc\", \"auc\", \"auc\", \"auc\", \"auc\", \"auc\", \"auditory\", \"auditory\", \"auditory\", \"auditory\", \"auditory\", \"auditory\", \"auditory\", \"auditory\", \"auditory\", \"badoiu\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"baptiste\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"bayes\", \"bayes\", \"bayes\", \"bayes\", \"bayes\", \"bayes\", \"bayes\", \"bayes\", \"bayes\", \"bayes\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"blocking\", \"blocking\", \"blocking\", \"blocking\", \"blocking\", \"blocking\", \"blocking\", \"blocking\", \"blocking\", \"bone\", \"bone\", \"bone\", \"bone\", \"bone\", \"bone\", \"boosting\", \"boosting\", \"boosting\", \"boosting\", \"boosting\", \"boosting\", \"boosting\", \"boosting\", \"boosting\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bouts\", \"bouts\", \"brains\", \"brashers\", \"brashers\", \"brashers\", \"bse\", \"bse\", \"bse\", \"bse\", \"bse\", \"bse\", \"bse\", \"bse\", \"bvs\", \"bvs\", \"bvs\", \"bvs\", \"bvs\", \"ca\", \"ca\", \"ca\", \"ca\", \"ca\", \"ca\", \"ca\", \"ca\", \"ca\", \"ca\", \"campus\", \"campus\", \"campus\", \"campus\", \"campus\", \"campus\", \"cascade\", \"cascade\", \"cascade\", \"cascade\", \"cascade\", \"cascade\", \"cascade\", \"cascade\", \"cascade\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"catastrophic\", \"catastrophic\", \"catastrophic\", \"catastrophic\", \"catastrophic\", \"catastrophic\", \"catastrophic\", \"cdp\", \"cdp\", \"cdp\", \"cdp\", \"cdp\", \"centrality\", \"centrality\", \"centrality\", \"centrality\", \"centrality\", \"centrality\", \"centrality\", \"centrality\", \"cfn\", \"characteristic\", \"characteristic\", \"characteristic\", \"characteristic\", \"characteristic\", \"characteristic\", \"characteristic\", \"characteristic\", \"characteristic\", \"characteristic\", \"charge\", \"charge\", \"charge\", \"charge\", \"charge\", \"charge\", \"charge\", \"charge\", \"cholinergic\", \"cholinergic\", \"cholinergic\", \"cholinergic\", \"cholinergic\", \"cholinergic\", \"cholinergic\", \"cholinergic\", \"cholinergic\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"clause\", \"clause\", \"clause\", \"clause\", \"clause\", \"clause\", \"clause\", \"clause\", \"clause\", \"cleaned\", \"cleaned\", \"cleaned\", \"cleaned\", \"cleaned\", \"clm\", \"clm\", \"clm\", \"clm\", \"clm\", \"clm\", \"clm\", \"clockwise\", \"clockwise\", \"clockwise\", \"clockwise\", \"cm\", \"cm\", \"cm\", \"cm\", \"cm\", \"cm\", \"cm\", \"cm\", \"cm\", \"codeword\", \"codeword\", \"codeword\", \"cold\", \"colder\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"competition\", \"components\", \"components\", \"components\", \"components\", \"components\", \"components\", \"components\", \"components\", \"components\", \"components\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"compute\", \"compute\", \"compute\", \"compute\", \"compute\", \"compute\", \"compute\", \"compute\", \"compute\", \"compute\", \"concavities\", \"concavities\", \"concavities\", \"concavities\", \"concavities\", \"concavities\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"conductance\", \"confused\", \"confused\", \"confused\", \"confused\", \"confused\", \"confused\", \"connectomics\", \"connectomics\", \"connectomics\", \"connectomics\", \"connectomics\", \"consolidation\", \"consolidation\", \"consolidation\", \"consolidation\", \"consolidation\", \"consolidation\", \"consolidation\", \"consolidation\", \"constant\", \"constant\", \"constant\", \"constant\", \"constant\", \"constant\", \"constant\", \"constant\", \"constant\", \"constant\", \"constellation\", \"constellation\", \"constellation\", \"constellation\", \"constellation\", \"constellation\", \"constellation\", \"contingencies\", \"contingencies\", \"contingencies\", \"controller\", \"controller\", \"controller\", \"controller\", \"controller\", \"controller\", \"controller\", \"controller\", \"convolution\", \"convolution\", \"convolution\", \"convolution\", \"convolution\", \"convolution\", \"convolution\", \"convolution\", \"convolution\", \"corr\", \"corr\", \"corr\", \"corr\", \"corr\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"covers\", \"covers\", \"covers\", \"crfs\", \"crfs\", \"crfs\", \"crfs\", \"crfs\", \"crfs\", \"crfs\", \"crfs\", \"crfs\", \"cruz\", \"cruz\", \"cruz\", \"cruz\", \"cruz\", \"csat\", \"csat\", \"ctm\", \"ctm\", \"ctm\", \"ctm\", \"ctm\", \"ctm\", \"ctm\", \"ctm\", \"curl\", \"curl\", \"currents\", \"currents\", \"currents\", \"currents\", \"currents\", \"currents\", \"currents\", \"currents\", \"currents\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"dcts\", \"dcts\", \"dcts\", \"dcts\", \"dcts\", \"dcts\", \"dcts\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"def\", \"def\", \"demaine\", \"demodulator\", \"demodulator\", \"demodulator\", \"demodulator\", \"demodulator\", \"demodulator\", \"demodulator\", \"demodulator\", \"demodulator\", \"denition\", \"denition\", \"denition\", \"denition\", \"denition\", \"denition\", \"denote\", \"denote\", \"denote\", \"denote\", \"denote\", \"denote\", \"denote\", \"denote\", \"denote\", \"denote\", \"density\", \"density\", \"density\", \"density\", \"density\", \"density\", \"density\", \"density\", \"density\", \"density\", \"dereverberated\", \"dereverberated\", \"dereverberated\", \"dereverberated\", \"dereverberated\", \"dereverberated\", \"dereverberated\", \"dereverberated\", \"dereverberated\", \"dereverberation\", \"dereverberation\", \"dereverberation\", \"dereverberation\", \"dereverberation\", \"dereverberation\", \"dereverberation\", \"dereverberation\", \"dereverberation\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"dij\", \"dij\", \"dij\", \"dij\", \"dij\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"dimensional\", \"directional\", \"directional\", \"directional\", \"directional\", \"directional\", \"discrepancies\", \"discrepancies\", \"discrepancies\", \"discrepancies\", \"discrepancies\", \"discrepancies\", \"discrepancies\", \"discrepancies\", \"discrepancy\", \"discrepancy\", \"discrepancy\", \"discrepancy\", \"discrepancy\", \"discrepancy\", \"discrepancy\", \"discrepancy\", \"discrepancy\", \"dispersive\", \"dispersive\", \"dispersive\", \"dispersive\", \"dispersive\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"distributions\", \"dl\", \"dl\", \"dl\", \"dl\", \"dl\", \"dl\", \"dl\", \"dl\", \"dl\", \"ec\", \"ec\", \"ec\", \"ec\", \"ec\", \"ec\", \"ec\", \"ec\", \"edelman\", \"edelman\", \"edelman\", \"edelman\", \"edelman\", \"edelman\", \"elevated\", \"elevated\", \"elevated\", \"elevated\", \"elevated\", \"elevated\", \"elicitation\", \"elicitation\", \"elicitation\", \"elicitation\", \"elicitation\", \"elicitation\", \"elicitation\", \"elicitation\", \"elman\", \"elman\", \"elman\", \"elman\", \"elman\", \"elman\", \"emanuel\", \"emanuel\", \"emanuel\", \"emanuel\", \"engage\", \"epu\", \"epu\", \"epu\", \"epu\", \"epu\", \"epu\", \"epu\", \"epu\", \"epu\", \"equalization\", \"equalization\", \"equalization\", \"equalization\", \"equalization\", \"equalization\", \"equalization\", \"equalization\", \"equalization\", \"equalizer\", \"equalizer\", \"equalizer\", \"equalizer\", \"equalizer\", \"equalizer\", \"equalizer\", \"equalizer\", \"equalizer\", \"equalizer\", \"equalizers\", \"equalizers\", \"equalizers\", \"equalizers\", \"equalizers\", \"equalizers\", \"equalizers\", \"equations\", \"equations\", \"equations\", \"equations\", \"equations\", \"equations\", \"equations\", \"equations\", \"equations\", \"err\", \"err\", \"err\", \"err\", \"err\", \"err\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"ess\", \"ess\", \"ess\", \"ess\", \"ess\", \"ess\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"eus\", \"eus\", \"eus\", \"eus\", \"eus\", \"eus\", \"eus\", \"eus\", \"eus\", \"ewtron\", \"ewtron\", \"ewtron\", \"ewtron\", \"ewtron\", \"ewtron\", \"ewtron\", \"ewtron\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"excitatory\", \"excitatory\", \"excitatory\", \"excitatory\", \"excitatory\", \"excitatory\", \"excitatory\", \"excitatory\", \"excitatory\", \"exi\", \"exi\", \"exi\", \"exi\", \"exponential\", \"exponential\", \"exponential\", \"exponential\", \"exponential\", \"exponential\", \"exponential\", \"exponential\", \"exponential\", \"externally\", \"externally\", \"externally\", \"externally\", \"externally\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"firing\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fluorescence\", \"fluorescence\", \"fluorescence\", \"fluorescence\", \"fluorescence\", \"fluorescence\", \"fluorescence\", \"fluorescence\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"force\", \"force\", \"force\", \"force\", \"force\", \"force\", \"force\", \"force\", \"force\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"freezes\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"ft\", \"fti\", \"fti\", \"fti\", \"fti\", \"fti\", \"fti\", \"fti\", \"fukunaga\", \"fukunaga\", \"fukunaga\", \"fukunaga\", \"fukunaga\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"gael\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gdk\", \"gdk\", \"gdk\", \"gdk\", \"gdk\", \"gdk\", \"gdk\", \"gelfand\", \"gelfand\", \"gelfand\", \"gelfand\", \"gelfand\", \"genetic\", \"genetic\", \"genetic\", \"genetic\", \"genetic\", \"genetic\", \"genetic\", \"genetic\", \"genetic\", \"ghm\", \"ghm\", \"ghm\", \"ghm\", \"gibson\", \"gibson\", \"gibson\", \"gibson\", \"gibson\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"gkk\", \"gkk\", \"gkk\", \"gkk\", \"gkk\", \"gkk\", \"gkk\", \"gkk\", \"gkk\", \"gkkqg\", \"gkkqg\", \"gkkqg\", \"gkkqg\", \"gkkqg\", \"gkkqg\", \"glasso\", \"glasso\", \"glasso\", \"glasso\", \"glasso\", \"glasso\", \"glasso\", \"glm\", \"glm\", \"glm\", \"glm\", \"glm\", \"glm\", \"glm\", \"glm\", \"glm\", \"glrt\", \"glrt\", \"glrt\", \"glrt\", \"glrt\", \"glrt\", \"glrt\", \"gmm\", \"gmm\", \"gmm\", \"gmm\", \"gmm\", \"gmm\", \"gmm\", \"gmm\", \"gmm\", \"gmm\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammar\", \"grammatically\", \"grammatically\", \"grammatically\", \"grammatically\", \"grammatically\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"greedy\", \"greedy\", \"greedy\", \"greedy\", \"greedy\", \"greedy\", \"greedy\", \"greedy\", \"greedy\", \"greedy\", \"groenen\", \"hajiaghayi\", \"hausser\", \"herb\", \"herb\", \"herb\", \"herb\", \"herb\", \"herb\", \"herb\", \"herb\", \"herding\", \"herding\", \"herding\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"hiq\", \"hiq\", \"hiq\", \"hiq\", \"hiq\", \"hiq\", \"hln\", \"hln\", \"hln\", \"hln\", \"hln\", \"hln\", \"hmdm\", \"hmdm\", \"hmdm\", \"hmdm\", \"hmdm\", \"hmdm\", \"hmdm\", \"hmdm\", \"hmdm\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"hybrid\", \"hybrid\", \"hybrid\", \"hybrid\", \"hybrid\", \"hybrid\", \"hybrid\", \"hybrid\", \"hybrid\", \"hybrid\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ica\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"iio\", \"iio\", \"iio\", \"iio\", \"ij\", \"ij\", \"ij\", \"ij\", \"ij\", \"ij\", \"ij\", \"ij\", \"ij\", \"ij\", \"ilk\", \"ilk\", \"ilk\", \"ilk\", \"ilk\", \"ilk\", \"ilk\", \"ilk\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"imaginary\", \"imaginary\", \"imaginary\", \"imaginary\", \"imaginary\", \"imaginary\", \"imaginary\", \"implicit\", \"implicit\", \"implicit\", \"implicit\", \"implicit\", \"implicit\", \"implicit\", \"implicit\", \"implicit\", \"impulse\", \"impulse\", \"impulse\", \"impulse\", \"impulse\", \"impulse\", \"impulse\", \"impulse\", \"impulse\", \"ina\", \"ina\", \"ina\", \"ina\", \"ina\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"infonnation\", \"infonnation\", \"infonnation\", \"infonnation\", \"infonnation\", \"infonnation\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"initialisation\", \"initialisation\", \"initialisation\", \"injective\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"interclause\", \"interclause\", \"interclause\", \"interclause\", \"interclause\", \"interference\", \"interference\", \"interference\", \"interference\", \"interference\", \"interference\", \"interference\", \"interference\", \"interference\", \"junction\", \"junction\", \"junction\", \"junction\", \"junction\", \"junction\", \"junction\", \"junction\", \"junction\", \"jutten\", \"jutten\", \"jutten\", \"jutten\", \"jutten\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kaggle\", \"kcon\", \"kcon\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kinetics\", \"kinetics\", \"kinetics\", \"kinetics\", \"kkcon\", \"klin\", \"klin\", \"kludge\", \"knn\", \"knn\", \"knn\", \"knn\", \"knn\", \"knn\", \"knn\", \"knn\", \"knn\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"ko\", \"kpm\", \"kpm\", \"krg\", \"krg\", \"krg\", \"krg\", \"krg\", \"krg\", \"krg\", \"krg\", \"krug\", \"krug\", \"krug\", \"krug\", \"lag\", \"lag\", \"lags\", \"landauer\", \"landauer\", \"landauer\", \"landauer\", \"landauer\", \"landauer\", \"langevin\", \"langevin\", \"langevin\", \"langevin\", \"langevin\", \"langevin\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"lattices\", \"lattices\", \"lattices\", \"lattices\", \"lattices\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layered\", \"layered\", \"layered\", \"layered\", \"layered\", \"layered\", \"layered\", \"layered\", \"layered\", \"lc\", \"lc\", \"lc\", \"lc\", \"lc\", \"lda\", \"lda\", \"lda\", \"lda\", \"lda\", \"lda\", \"lda\", \"lda\", \"lda\", \"leaderboard\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"leftd\", \"leftd\", \"leftd\", \"leftd\", \"leftd\", \"leftd\", \"leftd\", \"leftdnk\", \"leftnk\", \"leftnk\", \"leftnk\", \"leftnk\", \"leftnk\", \"leinbach\", \"leinbach\", \"leinbach\", \"lemma\", \"lemma\", \"lemma\", \"lemma\", \"lemma\", \"lemma\", \"lemma\", \"lemma\", \"lemma\", \"lemma\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"letter\", \"letter\", \"letter\", \"letter\", \"letter\", \"letter\", \"letter\", \"letter\", \"letter\", \"lever\", \"lever\", \"lever\", \"lever\", \"lever\", \"lever\", \"lever\", \"lever\", \"li\", \"li\", \"li\", \"li\", \"li\", \"li\", \"li\", \"li\", \"li\", \"limbs\", \"limbs\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linial\", \"lmita\", \"lmita\", \"lmita\", \"lmita\", \"lmita\", \"lmita\", \"lmita\", \"lnk\", \"lnk\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"logd\", \"logd\", \"logd\", \"logd\", \"logd\", \"logd\", \"logd\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"lower\", \"lower\", \"lower\", \"lower\", \"lower\", \"lower\", \"lower\", \"lower\", \"lower\", \"lower\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"manipulandum\", \"manipulandum\", \"marchand\", \"marchand\", \"marchand\", \"marchand\", \"marchand\", \"marchand\", \"marchand\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"maxima\", \"maxima\", \"maxima\", \"maxima\", \"maxima\", \"maxima\", \"maxima\", \"maxima\", \"maxima\", \"maxm\", \"maxm\", \"maxm\", \"maxm\", \"maxm\", \"maxm\", \"maxm\", \"maxneggdnss\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"mcp\", \"mcp\", \"mcp\", \"mcp\", \"mcp\", \"mcp\", \"mcp\", \"mcp\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"medline\", \"medline\", \"medline\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"mf\", \"mf\", \"mf\", \"mf\", \"mf\", \"mf\", \"mf\", \"mf\", \"mg\", \"mg\", \"mg\", \"mg\", \"mg\", \"mg\", \"mg\", \"mg\", \"mi\", \"mi\", \"mi\", \"mi\", \"mi\", \"mi\", \"mi\", \"mi\", \"mi\", \"minsky\", \"mle\", \"mle\", \"mle\", \"mle\", \"mle\", \"mle\", \"mle\", \"mle\", \"mle\", \"mmd\", \"mmd\", \"mmd\", \"mmd\", \"mmd\", \"mmd\", \"mmd\", \"mmd\", \"mmd\", \"mmse\", \"mmse\", \"mmse\", \"mmse\", \"mmse\", \"mmse\", \"mmse\", \"mmse\", \"mmse\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"modularization\", \"modularization\", \"modularization\", \"modularization\", \"modularization\", \"modularization\", \"modularization\", \"modularization\", \"modularization\", \"modularized\", \"modularized\", \"modularized\", \"modularized\", \"modularized\", \"modularized\", \"modularized\", \"modularized\", \"modularized\", \"modulate\", \"modulator\", \"modulator\", \"modulator\", \"modulator\", \"modulator\", \"modulator\", \"modulator\", \"modulator\", \"modulators\", \"modulators\", \"modulators\", \"modulators\", \"modulators\", \"modulators\", \"modulators\", \"modulators\", \"module\", \"module\", \"module\", \"module\", \"module\", \"module\", \"module\", \"module\", \"module\", \"modules\", \"modules\", \"modules\", \"modules\", \"modules\", \"modules\", \"modules\", \"modules\", \"modules\", \"mog\", \"mog\", \"mog\", \"mog\", \"mog\", \"mog\", \"motif\", \"motif\", \"motif\", \"motif\", \"motif\", \"motif\", \"motif\", \"motif\", \"motif\", \"motifs\", \"motifs\", \"motifs\", \"motifs\", \"motifs\", \"motifs\", \"motifs\", \"motifs\", \"motifs\", \"mpi\", \"mpi\", \"mpi\", \"mpm\", \"mpm\", \"mpm\", \"mpm\", \"mpm\", \"mpm\", \"mpm\", \"mpm\", \"multinomial\", \"multinomial\", \"multinomial\", \"multinomial\", \"multinomial\", \"multinomial\", \"multinomial\", \"multinomial\", \"multinomial\", \"multiuser\", \"multiuser\", \"multiuser\", \"multiuser\", \"multiuser\", \"multiuser\", \"mussa\", \"mussa\", \"mussa\", \"mv\", \"mv\", \"mv\", \"mv\", \"mv\", \"mv\", \"mv\", \"mv\", \"mwal\", \"mwal\", \"mwal\", \"mwal\", \"mwal\", \"mwal\", \"mwal\", \"mwal\", \"mwal\", \"netinput\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neurogenesys\", \"neurogenesys\", \"neurogenesys\", \"neurogenesys\", \"neurogenesys\", \"neurogenesys\", \"neurogenesys\", \"neurogenesys\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"neurons\", \"nit\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norouzi\", \"norouzi\", \"norouzi\", \"norouzi\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"objects\", \"objects\", \"objects\", \"objects\", \"objects\", \"objects\", \"objects\", \"objects\", \"objects\", \"objects\", \"offspring\", \"offspring\", \"offspring\", \"offspring\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"operator\", \"operator\", \"operator\", \"operator\", \"operator\", \"operator\", \"operator\", \"operator\", \"operator\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"ordinal\", \"ordinal\", \"ordinal\", \"ordinal\", \"ordinal\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"ouyang\", \"ovie\", \"ovie\", \"ovie\", \"ovie\", \"oz\", \"oz\", \"oz\", \"oz\", \"oz\", \"oz\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parser\", \"parser\", \"parser\", \"parser\", \"parser\", \"parser\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parsing\", \"parzen\", \"parzen\", \"parzen\", \"parzen\", \"parzen\", \"parzen\", \"parzen\", \"parzen\", \"parzen\", \"passing\", \"passing\", \"passing\", \"passing\", \"passing\", \"passing\", \"passing\", \"passing\", \"passing\", \"pat\", \"pat\", \"pat\", \"pat\", \"pat\", \"pat\", \"paths\", \"paths\", \"paths\", \"paths\", \"paths\", \"paths\", \"paths\", \"paths\", \"paths\", \"pawelzik\", \"pawelzik\", \"pawelzik\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"periodic\", \"periodic\", \"periodic\", \"periodic\", \"periodic\", \"periodic\", \"periodic\", \"periodic\", \"periodic\", \"phi\", \"phi\", \"phi\", \"phi\", \"phi\", \"phi\", \"phi\", \"phi\", \"phoneme\", \"phoneme\", \"phoneme\", \"phoneme\", \"phoneme\", \"phoneme\", \"phoneme\", \"phoneme\", \"phra\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrases\", \"phrases\", \"phrases\", \"phrases\", \"phrases\", \"phrases\", \"pif\", \"pif\", \"pih\", \"pih\", \"pio\", \"planning\", \"planning\", \"planning\", \"planning\", \"planning\", \"planning\", \"planning\", \"planning\", \"planning\", \"plant\", \"plant\", \"pm\", \"pm\", \"pm\", \"pm\", \"pm\", \"pm\", \"pm\", \"pm\", \"pm\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"prec\", \"proakis\", \"proakis\", \"proakis\", \"proakis\", \"proakis\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"prom\", \"pruned\", \"pruned\", \"pruned\", \"pruned\", \"pruned\", \"pruned\", \"pruned\", \"pruning\", \"pruning\", \"pruning\", \"pruning\", \"pruning\", \"pruning\", \"pruning\", \"pruning\", \"pruning\", \"push\", \"push\", \"pyi\", \"qam\", \"qam\", \"qam\", \"qam\", \"qam\", \"qam\", \"qn\", \"qn\", \"qn\", \"qn\", \"qn\", \"qn\", \"qn\", \"qn\", \"qn\", \"qna\", \"qna\", \"qna\", \"qt\", \"qt\", \"qt\", \"qt\", \"qt\", \"qt\", \"qt\", \"qt\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rbfs\", \"rbfs\", \"rbfs\", \"rbfs\", \"rbfs\", \"rbfs\", \"rbfs\", \"rbfs\", \"rbfs\", \"rbs\", \"rbs\", \"rbs\", \"rbs\", \"rbs\", \"rbs\", \"rbs\", \"rbs\", \"rbs\", \"realisable\", \"recommendation\", \"recommendation\", \"recommendation\", \"recommendation\", \"recommendation\", \"recommendation\", \"recommendation\", \"recommendation\", \"rect\", \"rect\", \"rect\", \"rect\", \"rect\", \"rectifiable\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"regained\", \"regained\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"representing\", \"representing\", \"representing\", \"representing\", \"representing\", \"representing\", \"representing\", \"representing\", \"representing\", \"reproducing\", \"reproducing\", \"reproducing\", \"reproducing\", \"reproducing\", \"reproducing\", \"reproducing\", \"reproducing\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"reverberation\", \"reverberation\", \"reverberation\", \"reverberation\", \"reverberation\", \"reverberation\", \"reverberation\", \"reverberation\", \"reverberation\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"reza\", \"reza\", \"reza\", \"reza\", \"rf\", \"rf\", \"rf\", \"rf\", \"rf\", \"rf\", \"rf\", \"rf\", \"rf\", \"rgg\", \"rgg\", \"rgg\", \"rgg\", \"rgg\", \"rglasso\", \"rightd\", \"rightd\", \"rightd\", \"rightd\", \"rightd\", \"rightd\", \"rightd\", \"rightdnk\", \"rightdnk\", \"rightnk\", \"rightnk\", \"rightnk\", \"rightnk\", \"rightnk\", \"rightnk\", \"riv\", \"riv\", \"rkhss\", \"rkhss\", \"rn\", \"rn\", \"rn\", \"rn\", \"rn\", \"rn\", \"rn\", \"rn\", \"rn\", \"rn\", \"rnl\", \"rnl\", \"rnl\", \"rnl\", \"rnl\", \"rnl\", \"rnl\", \"rnl\", \"roles\", \"roles\", \"roles\", \"roles\", \"roles\", \"roles\", \"roles\", \"rout\", \"rout\", \"rout\", \"rout\", \"rtd\", \"rtd\", \"rtd\", \"rtd\", \"rtd\", \"rtd\", \"rtd\", \"rtd\", \"samad\", \"samad\", \"samad\", \"samad\", \"samad\", \"samad\", \"samad\", \"samad\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"satisfaction\", \"satisfaction\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scattering\", \"scattering\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"scheduled\", \"scheduled\", \"scheduled\", \"schema\", \"schreiber\", \"schreiber\", \"schreiber\", \"schreiber\", \"schreiber\", \"schreiber\", \"scores\", \"scores\", \"scores\", \"scores\", \"scores\", \"scores\", \"scores\", \"scores\", \"scores\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"sensorimotor\", \"sensorimotor\", \"sentences\", \"sentences\", \"sentences\", \"sentences\", \"sentences\", \"sentences\", \"sentences\", \"sentences\", \"sentences\", \"separability\", \"separability\", \"separability\", \"separability\", \"separability\", \"separability\", \"separability\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"sets\", \"sets\", \"sets\", \"sets\", \"sets\", \"sets\", \"sets\", \"sets\", \"sets\", \"sets\", \"sgd\", \"sgd\", \"sgd\", \"sgd\", \"sgd\", \"sgd\", \"sgd\", \"sgd\", \"sgn\", \"sgn\", \"sgn\", \"sgn\", \"sgn\", \"sgn\", \"sgn\", \"sgn\", \"sgn\", \"sh\", \"sh\", \"sh\", \"sh\", \"sh\", \"sh\", \"sh\", \"sh\", \"sh\", \"shadmehr\", \"shadmehr\", \"shadmehr\", \"shadmehr\", \"shadmehr\", \"shareboost\", \"shareboost\", \"shareboost\", \"shareboost\", \"shareboost\", \"shareboost\", \"shareboost\", \"shareboost\", \"shareboost\", \"shaw\", \"shaw\", \"shaw\", \"shaw\", \"shortest\", \"shortest\", \"shortest\", \"shortest\", \"shortest\", \"shortest\", \"shortest\", \"shortest\", \"shortest\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"si\", \"si\", \"si\", \"si\", \"si\", \"si\", \"si\", \"si\", \"si\", \"si\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"sinusoids\", \"sinusoids\", \"sinusoids\", \"sinusoids\", \"sinusoids\", \"sinusoids\", \"sio\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"slot\", \"slot\", \"slot\", \"slot\", \"slot\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"soho\", \"soho\", \"soho\", \"soho\", \"soho\", \"soho\", \"soho\", \"soho\", \"solla\", \"solla\", \"solla\", \"solla\", \"solla\", \"sounds\", \"sounds\", \"sounds\", \"sounds\", \"sounds\", \"sounds\", \"sounds\", \"sounds\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"spanner\", \"spanner\", \"spanner\", \"spanner\", \"spanner\", \"spanner\", \"spanner\", \"spanner\", \"spanner\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"squire\", \"squire\", \"sriperumbudur\", \"sriperumbudur\", \"sriperumbudur\", \"ssgd\", \"ssgd\", \"ssgd\", \"ssgd\", \"steerable\", \"steerable\", \"steerable\", \"steerable\", \"steerable\", \"steerable\", \"steerable\", \"steerable\", \"steerable\", \"stein\", \"stein\", \"stein\", \"stein\", \"stein\", \"stein\", \"stein\", \"stein\", \"stein\", \"stein\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"steven\", \"stft\", \"stft\", \"stft\", \"stft\", \"stft\", \"strings\", \"strings\", \"strings\", \"strings\", \"strings\", \"strings\", \"strings\", \"strings\", \"strokes\", \"strokes\", \"strokes\", \"strokes\", \"strokes\", \"strokes\", \"strokes\", \"strokes\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"subjects\", \"subjects\", \"subjects\", \"subjects\", \"subjects\", \"subjects\", \"subjects\", \"subjects\", \"subjects\", \"subschemas\", \"subtracted\", \"subtree\", \"subtree\", \"subtree\", \"subtree\", \"subtree\", \"subtree\", \"subtree\", \"svmd\", \"svmd\", \"svmd\", \"svmd\", \"tables\", \"tables\", \"tables\", \"tables\", \"tables\", \"tables\", \"tables\", \"tables\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"teaching\", \"teaching\", \"teaching\", \"teaching\", \"teaching\", \"teaching\", \"teaching\", \"teaching\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"testing\", \"testing\", \"testing\", \"testing\", \"testing\", \"testing\", \"testing\", \"testing\", \"testing\", \"testing\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"timme\", \"todorov\", \"todorov\", \"todorov\", \"todorov\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topics\", \"topics\", \"topics\", \"topics\", \"topics\", \"topics\", \"topics\", \"topics\", \"topics\", \"tp\", \"tp\", \"tp\", \"tp\", \"tp\", \"tp\", \"tp\", \"tp\", \"tp\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"trees\", \"trees\", \"trees\", \"trees\", \"trees\", \"trees\", \"trees\", \"trees\", \"trees\", \"trees\", \"twelve\", \"twelve\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"ucrl\", \"ucrl\", \"ucrl\", \"ucrl\", \"ucrl\", \"ucrl\", \"ucrl\", \"ucrl\", \"ucrl\", \"um\", \"um\", \"um\", \"um\", \"um\", \"um\", \"um\", \"undirected\", \"undirected\", \"undirected\", \"undirected\", \"undirected\", \"undirected\", \"undirected\", \"undirected\", \"undirected\", \"unif\", \"unif\", \"unif\", \"unif\", \"unif\", \"unifonn\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"utility\", \"valiant\", \"valiant\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"vcd\", \"vcd\", \"vcd\", \"vcd\", \"vcd\", \"vcd\", \"vcd\", \"vcd\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"verb\", \"verb\", \"verb\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visible\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"voltage\", \"voltage\", \"voltage\", \"voltage\", \"voltage\", \"voltage\", \"voltage\", \"voltage\", \"voltage\", \"vt\", \"vt\", \"vt\", \"vt\", \"vt\", \"vt\", \"vt\", \"vt\", \"vt\", \"wasserstein\", \"wasserstein\", \"wasserstein\", \"wasserstein\", \"wasserstein\", \"wasserstein\", \"wasserstein\", \"wasserstein\", \"weinshall\", \"weinshall\", \"weinshall\", \"weinshall\", \"weinshall\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"winning\", \"winning\", \"winning\", \"winning\", \"winning\", \"winning\", \"winning\", \"winning\", \"wn\", \"wn\", \"wn\", \"wn\", \"wn\", \"wn\", \"wn\", \"wn\", \"wn\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"wref\", \"wref\", \"wref\", \"wref\", \"wref\", \"wref\", \"wref\", \"wref\", \"wref\", \"wv\", \"wv\", \"wv\", \"wv\", \"wv\", \"wv\", \"xali\", \"xali\", \"xali\", \"xali\", \"xali\", \"xce\", \"xce\", \"xd\", \"xd\", \"xd\", \"xd\", \"xd\", \"xd\", \"xd\", \"xd\", \"xd\", \"xdt\", \"xdt\", \"xdt\", \"xdt\", \"xdt\", \"xdt\", \"xdt\", \"xdt\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xil\", \"xil\", \"xil\", \"xil\", \"xil\", \"xil\", \"xnk\", \"xnk\", \"xnk\", \"xnk\", \"xnk\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xt\", \"xuanlong\", \"xuanlong\", \"xuanlong\", \"xuanlong\", \"yedidia\", \"yj\", \"yj\", \"yj\", \"yj\", \"yj\", \"yj\", \"yj\", \"yj\", \"yj\", \"yt\", \"yt\", \"yt\", \"yt\", \"yt\", \"yt\", \"yt\", \"yt\", \"yt\", \"yt\", \"zero\", \"zero\", \"zero\", \"zero\", \"zero\", \"zero\", \"zero\", \"zero\", \"zero\", \"zero\", \"zkm\", \"zkm\", \"zkm\", \"zkm\", \"zkm\", \"zkm\", \"zkm\", \"zlt\", \"zlt\", \"zlt\", \"zlt\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [8, 1, 6, 4, 5, 2, 3, 10, 7, 9]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el4971389635014574562792230603\", ldavis_el4971389635014574562792230603_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el4971389635014574562792230603\", ldavis_el4971389635014574562792230603_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el4971389635014574562792230603\", ldavis_el4971389635014574562792230603_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "!pip install pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pickle\n",
        "import pyLDAvis\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "LDAvis_data_filepath = os.path.join('/content/NIPS Papers.zip'+str(num_topics))\n",
        "\n",
        "# # this is a bit time consuming - make the if statement True\n",
        "# # if you want to execute visualization prep yourself\n",
        "if 1 == 1:\n",
        "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)\n",
        "\n",
        "# load the pre-prepared pyLDAvis data from disk\n",
        "with open(LDAvis_data_filepath, 'rb') as f:\n",
        "    LDAvis_prepared = pickle.load(f)\n",
        "\n",
        "pyLDAvis.save_html(LDAvis_prepared, '/content/NIPS Papers.zip'+ str(num_topics) +'.html')\n",
        "\n",
        "LDAvis_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGawIp65w_AP"
      },
      "source": [
        "** **\n",
        "#### Closing Notes\n",
        "Machine learning has become increasingly popular over the past decade, and recent advances in computational availability have led to exponential growth to people looking for ways how new methods can be incorporated to advance the field of Natural Language Processing.\n",
        "\n",
        "Often, we treat topic models as black-box algorithms, but hopefully, this article addressed to shed light on the underlying math, and intuitions behind it, and high-level code to get you started with any textual data.\n",
        "\n",
        "In the next article, we’ll go one step deeper into understanding how you can evaluate the performance of topic models, tune its hyper-parameters to get more intuitive and reliable results.\n",
        "\n",
        "** **\n",
        "#### References:\n",
        "1. Topic model — Wikipedia. https://en.wikipedia.org/wiki/Topic_model\n",
        "2. Distributed Strategies for Topic Modeling. https://www.ideals.illinois.edu/bitstream/handle/2142/46405/ParallelTopicModels.pdf?sequence=2&isAllowed=y\n",
        "3. Topic Mapping — Software — Resources — Amaral Lab. https://amaral.northwestern.edu/resources/software/topic-mapping\n",
        "4. A Survey of Topic Modeling in Text Mining. https://thesai.org/Downloads/Volume6No1/Paper_21-A_Survey_of_Topic_Modeling_in_Text_Mining.pdf\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}