{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khf5AOzQ58L6"
      },
      "source": [
        "### Evaluate Topic Model in Python: Latent Dirichlet Allocation (LDA)\\\n",
        "##### A step-by-step guide to building interpretable topic models\n",
        "\n",
        "** **\n",
        "*Preface: This article aims to provide consolidated information on the underlying topic and is not to be considered as the original work. The information and the code are repurposed through several online articles, research papers, books, and open-source code*\n",
        "** **\n",
        "\n",
        "In the previous [article](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0), I introduced the concept of topic modeling and walked through the code for developing your first topic model using Latent Dirichlet Allocation (LDA) method in the python using Gensim implementation.\n",
        "\n",
        "Pursuing on that understanding, in this article, we’ll go a few steps deeper by outlining the framework to quantitatively evaluate topic models through the measure of topic coherence and share the code template in python using Gensim implementation to allow for end-to-end model development.\n",
        "\n",
        "### Why evaluate topic models?\n",
        "\n",
        "![img](https://tinyurl.com/y3xznjwq)\n",
        "\n",
        "We know probabilistic topic models, such as LDA, are popular tools for text analysis, providing both a predictive and latent topic representation of the corpus. However, there is a longstanding assumption that the latent space discovered by these models is generally meaningful and useful, and that evaluating such assumptions is challenging due to its unsupervised training process. Besides, there is a no-gold standard list of topics to compare against every corpus.\n",
        "\n",
        "Nevertheless, it is equally important to identify if a trained model is objectively good or bad, as well have an ability to compare different models/methods. To do so, one would require an objective measure for the quality. Traditionally, and still for many practical applications, to evaluate if “the correct thing” has been learned about the corpus, an implicit knowledge and “eyeballing” approaches are used. Ideally, we’d like to capture this information in a single metric that can be maximized, and compared.\n",
        "\n",
        "Let’s take a look at roughly what approaches are commonly used for the evaluation:\n",
        "\n",
        "**Eye Balling Models**\n",
        "- Top N words\n",
        "- Topics / Documents\n",
        "\n",
        "**Intrinsic Evaluation Metrics**\n",
        "- Capturing model semantics\n",
        "- Topics interpretability\n",
        "\n",
        "**Human Judgements**\n",
        "- What is a topic\n",
        "\n",
        "**Extrinsic Evaluation Metrics/Evaluation at task**\n",
        "- Is model good at performing predefined tasks, such as classification\n",
        "\n",
        "Natural language is messy, ambiguous and full of subjective interpretation, and sometimes trying to cleanse ambiguity reduces the language to an unnatural form. In this article, we’ll explore more about topic coherence, an intrinsic evaluation metric, and how you can use it to quantitatively justify the model selection.\n",
        "\n",
        "### What is Topic Coherence?\n",
        "\n",
        "Before we understand topic coherence, let’s briefly look at the perplexity measure. Perplexity as well is one of the intrinsic evaluation metric, and is widely used for language model evaluation. It captures how surprised a model is of new data it has not seen before, and is measured as the normalized log-likelihood of a held-out test set.\n",
        "\n",
        "Focussing on the log-likelihood part, you can think of the perplexity metric as measuring how probable some new unseen data is given the model that was learned earlier. That is to say, how well does the model represent or reproduce the statistics of the held-out data.\n",
        "\n",
        "However, recent studies have shown that predictive likelihood (or equivalently, perplexity) and human judgment are often not correlated, and even sometimes slightly anti-correlated.\n",
        "\n",
        "*Optimizing for perplexity may not yield human interpretable topics*\n",
        "\n",
        "This limitation of perplexity measure served as a motivation for more work trying to model the human judgment, and thus *Topic Coherence*.\n",
        "\n",
        "The concept of topic coherence combines a number of measures into a framework to evaluate the coherence between topics inferred by a model. But before that…\n",
        "\n",
        "#### What is topic coherence?\n",
        "Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference. But,\n",
        "\n",
        "#### What is coherence?\n",
        "Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference. But …\n",
        "\n",
        "### Coherence Measures\n",
        "Let’s take quick look at different coherence measures, and how they are calculated:\n",
        "\n",
        "1. `C_v` measure is based on a sliding window, one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity\n",
        "2. `C_p` is based on a sliding window, one-preceding segmentation of the top words and the confirmation measure of Fitelson's coherence\n",
        "3. `C_uci` measure is based on a sliding window and the pointwise mutual information (PMI) of all word pairs of the given top words\n",
        "4. `C_umass` is based on document cooccurrence counts, a one-preceding segmentation and a logarithmic conditional probability as confirmation measure\n",
        "5. `C_npmi` is an enhanced version of the C_uci coherence using the normalized pointwise mutual information (NPMI)\n",
        "6. `C_a` is based on a context window, a pairwise comparison of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity\n",
        "\n",
        "There is, of course, a lot more to the concept of topic model evaluation, and the coherence measure. However, keeping in mind the length, and purpose of this article, let’s apply these concepts into developing a model that is at least better than with the default parameters. Also, we’ll be re-purposing already available online pieces of code to support this exercise instead of re-inventing the wheel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzZeWBpg58MG"
      },
      "source": [
        "### Model Implementation\n",
        "1. Loading Data\n",
        "2. Data Cleaning\n",
        "3. Phrase Modeling: Bi-grams and Tri-grams\n",
        "4. Data Transformation: Corpus and Dictionary\n",
        "5. Base Model\n",
        "6. Hyper-parameter Tuning\n",
        "7. Final model\n",
        "8. Visualize Results\n",
        "\n",
        "** **\n",
        "\n",
        "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_158/img/nips_logo.png\" alt=\"The logo of NIPS (Neural Information Processing Systems)\">\n",
        "\n",
        "Let’s start by looking at the content of the file\n",
        "\n",
        "** **\n",
        "#### Step 1: Loading Data\n",
        "** **\n",
        "\n",
        "For this tutorial, we’ll use the dataset of papers published in NIPS conference. The NIPS conference (Neural Information Processing Systems) is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "Let’s start by looking at the content of the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "qVGGg1dw58MH",
        "outputId": "d58ea95e-ce58-4c8f-a6a1-950d173bf775"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 6560,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1901,\n        \"min\": 1,\n        \"max\": 6603,\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          3087,\n          78,\n          5412\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1987,\n        \"max\": 2016,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          1992,\n          1990,\n          2012\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"Natural Actor-Critic for Road Traffic Optimisation\",\n          \"Learning Representations by Recirculation\",\n          \"Quantized Kernel Learning for Feature Matching\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Oral\",\n          \"Spotlight\",\n          \"Poster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"3087-natural-actor-critic-for-road-traffic-optimisation.pdf\",\n          \"78-learning-representations-by-recirculation.pdf\",\n          \"5412-quantized-kernel-learning-for-feature-matching.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3244,\n        \"samples\": [\n          \"Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP decomposition algorithms based on sketching. We build on the idea of count sketches, but introduce many novel ideas which are unique to tensors. We develop novel methods for randomized com- putation of tensor contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in decomposition methods such as ten- sor power iterations and alternating least squares. We also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine these sketching ideas with existing whitening and tensor power iter- ative techniques to obtain the fastest algorithm on both sparse and dense tensors. The quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc. We apply the method for topic mod- eling and obtain competitive results.\",\n          \"Many spectral unmixing methods rely on the non-negative decomposition of spectral data onto a dictionary of spectral templates. In particular, state-of-the-art music transcription systems decompose the spectrogram of the input signal onto a dictionary of representative note spectra. The typical measures of fit used to quantify the adequacy of the decomposition compare the data and template entries frequency-wise. As such, small displacements of energy from a frequency bin to another as well as variations of timber can disproportionally harm the fit. We address these issues by means of optimal transportation and propose a new measure of fit that treats the frequency distributions of energy holistically as opposed to frequency-wise. Building on the harmonic nature of sound, the new measure is invariant to shifts of energy to harmonically-related frequencies, as well as to small and local displacements of energy. Equipped with this new measure of fit, the dictionary of note templates can be considerably simplified to a set of Dirac vectors located at the target fundamental frequencies (musical pitch values). This in turns gives ground to a very fast and simple decomposition algorithm that achieves state-of-the-art performance on real musical data.\",\n          \"The problem of  multiclass boosting is considered. A new framework,based on multi-dimensional codewords and predictors is introduced. The optimal set of codewords is derived, and a margin enforcing loss proposed. The resulting risk is minimized by gradient descent on a multidimensional functional space. Two algorithms are proposed: 1) CD-MCBoost, based on coordinate descent, updates one predictor component at a time, 2) GD-MCBoost, based on gradient descent, updates all components jointly. The algorithms differ in the weak learners that they support but are both shown to be 1) Bayes consistent, 2) margin enforcing, and 3) convergent to the global minimum of the risk. They also reduce to AdaBoost when there are only two classes. Experiments show that both methods outperform previous multiclass boosting approaches on a number of datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6553,\n        \"samples\": [\n          \"550\\n\\nAckley and Littman\\n\\nGeneralization and scaling in reinforcement\\nlearning\\nDavid H. Ackley\\nMichael L. Littman\\nCognitive Science Research Group\\nBellcore\\nMorristown, NJ 07960\\n\\nABSTRACT\\nIn associative reinforcement learning, an environment generates input\\nvectors, a learning system generates possible output vectors, and a reinforcement function computes feedback signals from the input-output\\npairs. The task is to discover and remember input-output pairs that\\ngenerate rewards. Especially difficult cases occur when rewards are\\nrare, since the expected time for any algorithm can grow exponentially\\nwith the size of the problem. Nonetheless, if a reinforcement function\\npossesses regularities, and a learning algorithm exploits them, learning\\ntime can be reduced below that of non-generalizing algorithms. This\\npaper describes a neural network algorithm called complementary reinforcement back-propagation (CRBP), and reports simulation results\\non problems designed to offer differing opportunities for generalization.\\n\\n1\\n\\nREINFORCEMENT LEARNING REQUIRES SEARCH\\n\\nReinforcement learning (Sutton, 1984; Barto & Anandan, 1985; Ackley, 1988; Allen,\\n1989) requires more from a learner than does the more familiar supervised learning\\nparadigm. Supervised learning supplies the correct answers to the learner, whereas\\nreinforcement learning requires the learner to discover the correct outputs before\\nthey can be stored. The reinforcement paradigm divides neatly into search and\\nlearning aspects: When rewarded the system makes internal adjustments to learn\\nthe discovered input-output pair; when punished the system makes internal adjustments to search elsewhere.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n1.1\\n\\nMAKING REINFORCEMENT INTO ERROR\\n\\nFollowing work by Anderson (1986) and Williams (1988), we extend the backpropagation algorithm to associative reinforcement learning. Start with a \\\"garden variety\\\" backpropagation network: A vector i of n binary input units propagates\\nthrough zero or more layers of hidden units, ultimately reaching a vector 8 of m\\nsigmoid units, each taking continuous values in the range (0,1). Interpret each 8j\\nas the probability that an associated random bit OJ takes on value 1. Let us call\\nthe continuous, deterministic vector 8 the search vector to distinguish it from the\\nstochastic binary output vector o.\\nGiven an input vector, we forward propagate to produce a search vector 8, and\\nthen perform m independent Bernoulli trials to produce an output vector o. The\\ni - 0 pair is evaluated by the reinforcement function and reward or punishment\\nensues. Suppose reward occurs. We therefore want to make 0 more likely given i.\\nBackpropagation will do just that if we take 0 as the desired target to produce an\\nerror vector (0 - 8) and adjust weights normally.\\nNow suppose punishment occurs, indicating 0 does not correspond with i. By choice\\nof error vector, backpropagation allows us to push the search vector in any direction;\\nwhich way should we go? In absence of problem-specific information, we cannot pick\\nan appropriate direction with certainty. Any decision will involve assumptions. A\\nvery minimal \\\"don't be like 0\\\" assumption-employed in Anderson (1986), Williams\\n(1988), and Ackley (1989)-pushes s directly away from 0 by taking (8 - 0) as the\\nerror vector. A slightly stronger \\\"be like not-o\\\" assumption-employed in Barto &\\nAnandan (1985) and Ackley (1987)-pushes s directly toward the complement of 0\\nby taking ((1 - 0) - 8) as the error vector. Although the two approaches always\\nagree on the signs of the error terms, they differ in magnitudes. In this work,\\nwe explore the second possibility, embodied in an algorithm called complementary\\nreinforcement back-propagation ( CRBP).\\nFigure 1 summarizes the CRBP algorithm. The algorithm in the figure reflects three\\nmodifications to the basic approach just sketched. First, in step 2, instead of using\\nthe 8j'S directly as probabilities, we found it advantageous to \\\"stretch\\\" the values\\nusing a parameter v. When v < 1, it is not necessary for the 8i'S to reach zero or\\none to produce a deterministic output. Second, in step 6, we found it important\\nto use a smaller learning rate for punishment compared to reward. Third, consider\\nstep 7: Another forward propagation is performed, another stochastic binary output vector 0* is generated (using the procedure from step 2), and 0* is compared\\nto o. If they are identical and punishment occurred, or if they are different and\\nreward occurred, then another error vector is generated and another weight update\\nis performed. This loop continues until a different output is generated (in the case\\nof failure) or until the original output is regenerated (in the case of success). This\\nmodification improved performance significantly, and added only a small percentage\\nto the total number of weight updates performed.\\n\\n551\\n\\n\\f552\\n\\nAckley and Littman\\n\\nO. Build a back propagation network with input dimensionality n and output\\ndimensionality m. Let t = 0 and te = O.\\n1. Pick random i E 2n and forward propagate to produce a/s.\\n2. Generate a binary output vector o. Given a uniform random variable ~ E [0,1]\\nand parameter 0 < v < 1,\\nOJ\\n\\n=\\n\\n{1,\\n\\n0,\\n\\nif(sj - !)/v+! ~ ~j\\notherwise.\\n\\n3. Compute reinforcement r = f(i,o). Increment t. If r < 0, let te = t.\\n4. Generate output errors ej. If r > 0, let tj = OJ, otherwise let tj = 1- OJ. Let\\nej = (tj - sj)sj(l- Sj).\\n5. Backpropagate errors.\\n6. Update weights. 1:::..Wjk = 1]ekSj, using 1] = 1]+ if r ~ 0, and 1] = 1]- otherwise,\\nwith parameters 1]+,1]- > o.\\n7. Forward propagate again to produce new Sj's. Generate temporary output\\nvector 0*. If (r > 0 and 0* #- 0) or (r < 0 and 0* = 0), go to 4.\\n8. If te ~ t, exit returning te, else go to 1.\\n\\nFigure 1: Complementary Reinforcement Back Propagation-CRBP\\n\\n2\\n\\nON-LINE GENERALIZATION\\n\\nWhen there are many possible outputs and correct pairings are rare, the computational cost associated with the search for the correct answers can be profound.\\nThe search for correct pairings will be accelerated if the search strategy can effectively generalize the reinforcement received on one input to others. The speed of\\nan algorithm on a given problem relative to non-generalizing algorithms provides a\\nmeasure of generalization that we call on-line generalization.\\nO. Let z be an array of length 2n. Set the z[i] to random numbers from 0 to\\n2m - 1. Let t = te = O.\\n1. Pick a random input i E 2n.\\n2. Compute reinforcement r = f(i, z[i]). Increment t.\\n3. If r < 0 let z[i] = (z[i] + 1) mod 2m , and let te = t.\\n4. If te <t:: t exit returning t e, else go to 1.\\n\\nFigure 2: The Table Lookup Reference Algorithm Tref(f, n, m)\\nConsider the table-lookup algorithm Tref(f, n, m) summarized in Figure 2. In this\\nalgorithm, a separate storage location is used for each possible input. This prevents\\nthe memorization of one i - 0 pair from interfering with any other. Similarly,\\nthe selection of a candidate output vector depends only on the slot of the table\\ncorresponding to the given input. The learning speed of T ref depends only on the\\ninput and output dimensionalities and the number of correct outputs associated\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\nwith each input. When a problem possesses n input bits and n output bits, and\\nthere is only one correct output vector for each input vector, Tre{ runs in about 4n\\ntime (counting each input-output judgment as one.) In such cases one expects to\\ntake at least 2n - 1 just to find one correct i - 0 pair, so exponential time cannot be\\navoided without a priori information. How does a generalizing algorithm such as\\nCRBP compare to Trer?\\n\\n3\\n\\nSIMULATIONS ON SCALABLE PROBLEMS\\n\\nWe have tested CRBP on several simple problems designed to offer varying degrees\\nand types of generalization. In all of the simulations in this section, the following\\ndetails apply: Input and output bit counts are equal (n). Parameters are dependent\\non n but independent of the reinforcement function f. '7+ is hand-picked for each\\nn,l 11- = 11+/10 and II = 0.5. All data points are medians of five runs. The stopping\\ncriterion te ~ t is interpreted as te +max(2000, 2n+l) < t. The fit lines in the figures\\nare least squares solutions to a x bn , to two significant digits.\\nAs a notational convenience, let c = ~\\n\\n3.1\\n\\nn\\n\\nE ij\\n\\n;=1\\n\\n-\\n\\nthe fraction of ones in the input.\\n\\nn-MAJORlTY\\n\\nConsider this \\\"majority rules\\\" problem: [if c > ~ then 0 = In else 0 = on]. The i-o\\nmapping is many-to-l. This problem provides an opportunity for what Anderson\\n(1986) called \\\"output generalization\\\": since there are only two correct output states,\\nevery pair of output bits are completely correlated in the cases when reward occurs.\\n\\nG)\\n\\n'iii\\nu\\nrn\\n\\nC)\\n\\n0\\n\\n::::.\\nG)\\n\\nE\\n\\n;\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n\\nTable\\n\\nD\\n\\nCRBP n-n-n\\n\\n+ CRBP n-n\\n\\n10 3\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n456\\n\\n78\\n\\n91011121314\\n\\nn\\nFigure 3: The n-majority problem\\n\\nFigure 3 displays the simulation results. Note that although Trer is faster than\\nCRBP at small values of n, CRBP's slower growth rate (1.6n vs 4.2n ) allows it to\\ncross over and begin outperforming Trer at about 6 bits. Note also--in violation of\\n1 For n = 1 to 12. we used '1+\\n0.219. 0.170. 0.121}.\\n\\n= {2.000. 1.550. 1.130.0.979.0.783.0.709.0.623.0.525.0.280.\\n\\n553\\n\\n\\f554\\n\\nAckley and Littman\\n\\nsome conventional wisdom-that although n-majority is a linearly separable problem, the performance of CRBP with hidden units is better than without. Hidden\\nunits can be helpful--even on linearly separable problems-when there are opportunities for output generalization.\\n\\n3.2\\n\\nn-COPY AND THE 2k -ATTRACTORS FAMILY\\n\\nAs a second example, consider the n-copy problem: [0 = i]. The i-o mapping is now\\n1-1, and the values of output bits in rewarding states are completely uncorrelated,\\nbut the value of each output bit is completely correlated with the value of the\\ncorresponding input bit. Figure 4 displays the simulation results. Once again, at\\n\\nG)\\n\\n'ii\\n\\ntA\\nQ\\n0\\n\\n::::.\\nG)\\n\\n-\\n\\n.5\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n150*2.0I\\\\n\\n\\nD\\n\\n10 3\\n10 2\\n\\n12*2.2I\\\\n\\n\\n+\\n\\nTable\\nCRBP n-n-n\\nCRBP n-n\\n\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 4: The n-copy problem\\nlow values of n, Trer is faster, but CRBP rapidly overtakes Trer as n increases. In\\nn-copy, unlike n-majority, CRBP performs better without hidden units.\\nThe n-majority and n-copy problems are extreme cases of a spectrum. n-majority\\ncan be viewed as a \\\"2-attractors\\\" problem in that there are only two correct\\noutputs-all zeros and all ones-and the correct output is the one that i is closer\\nto in hamming distance. By dividing the input and output bits into two groups\\nand performing the majority function independently on each group, one generates\\na \\\"4-aUractors\\\" problem. In general, by dividing the input and output bits into\\n1 ~ Ie ~ n groups, one generates a \\\"2i:-attractors\\\" problem. When Ie = 1, nmajority results, and when Ie n, n-copy results.\\n\\n=\\n\\nFigure 5 displays simulation results on the n = 8-bit problems generated when Ie is\\nvaried from 1 to n. The advantage of hidden units for low values of Ie is evident,\\nas is the advantage of \\\"shortcut connections\\\" (direct input-to-output weights) for\\nlarger values of Ie. Note also that combination of both hidden units and shortcut\\nconnections performs better than either alone.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n105~--------------------------------~\\n\\nCASP 8-10-8\\n-+- CASP 8-8\\n.... CASP 8-10-Sls\\n-0-\\n\\n... Table\\n\\n3\\n\\n2\\n\\n1\\n\\n5\\n\\n4\\n\\n7\\n\\n6\\n\\n8\\n\\nk\\n\\nFigure 5: The 21:- attractors family at n = 8\\n\\n3.3\\n\\nn-EXCLUDED MIDDLE\\n\\nAll of the functions considered so far have been linearly separable. Consider this\\n\\\"folded majority\\\" function: [if\\n< c < then 0 on else 0 In]. Now, like\\nn-majority, there are only two rewarding output states, but the determination of\\nwhich output state is correct is not linearly separable in the input space. When\\nn = 2, the n-excluded middle problem yields the EQV (i.e., the complement of\\nXOR) function, but whereas functions such as n-parity [if nc is even then 0\\non\\nelse 0 = In] get more non-linear with increasing n, n-excluded middle does not.\\n\\ni\\n\\ni\\n\\n=\\n\\n=\\n\\n=\\n\\n107~------------------------------~~\\n\\n-\\n\\n10 6\\n10 5\\n\\nD)\\n\\n10 4\\n10 3\\n\\nI)\\n\\n'ii\\nu\\nf)\\n\\n.2\\n\\nI)\\n\\nE\\n\\n:::\\n\\nx\\nc\\n\\n17oo*1.6\\\"n\\n\\nTable\\n\\nCRSP n-n-n/s\\n\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 6: The n-excluded middle problem\\nFigure 6 displays the simulation results. CRBP is slowed somewhat compared to\\nthe linearly separable problems, yielding a higher \\\"cross over point\\\" of about 8 bits.\\n\\n555\\n\\n\\f556\\n\\nAckley and Littman\\n\\n4\\n\\nSTRUCTURING DEGENERATE OUTPUT SPACES\\n\\nAll of the scaling problems in the previous section are designed so that there is\\na single correct output for each possible input. This allows for difficult problems\\neven at small sizes, but it rules out an important aspect of generalizing algorithms\\nfor associative reinforcement learning: If there are multiple satisfactory outputs\\nfor given inputs, a generalizing algorithm may impose structure on the mapping it\\nproduces.\\nWe have two demonstrations of this effect, \\\"Bit Count\\\" and \\\"Inverse Arithmetic.\\\"\\nThe Bit Count problem simply states that the number of I-bits in the output should\\nequal the number of I-bits in the input. When n = 9, Tref rapidly finds solutions\\ninvolving hundreds of different output patterns. CRBP is slower--especially with\\nrelatively few hidden units-but it regularly finds solutions involving just 10 output\\npatterns that form a sequence from 09 to 19 with one bit changing per step.\\n0+Ox4=0\\n1+0x4=1\\n2+0x4=2\\n3+0x4=3\\n\\n0+2x4=8\\n1+2x4=9\\n2 + 2 x 4 = 10\\n3+2x4=11\\n\\n4+0x4=4 4+ 2 x 4 =\\n5+0x4=5 5 + 2 x 4 =\\n6+0x4=6 6 + 2 x 4 =\\n7+0x4=7 7 + 2 x 4 =\\n\\n12\\n13\\n14\\n15\\n\\n2+2-4=0 2+2+4=8\\n3+2-4=1 3+2+4=9\\n2+2+4=2 2 + 2 x 4 = 10\\n3+2+4=3 3+2x4=1l\\n6+2-4=4\\n7+2-4=5\\n6+2+4=6\\n7+2-.;-4=7\\n\\n6+\\n7+\\n6+\\n7+\\n\\n2+ 4 =\\n2+ 4 =\\n2x4=\\n2x4=\\n\\n0+4 x 4 = 16 0+6 x 4 =\\n1+4x4=17 1 + 6 x 4 =\\n2 + 4 x 4 = 18 2 + 6 x 4 =\\n3 +4 x 4 = 19 3 + 6 x 4 =\\n\\n24\\n25\\n26\\n27\\n\\n4+4\\n5+ 4\\n6+ 4\\n7+ 4\\n\\n=\\n=\\n=\\n=\\n\\n28\\n29\\n30\\n31\\n24\\n25\\n26\\n27\\n\\nx\\nx\\nx\\nx\\n\\n4=\\n4=\\n4=\\n4=\\n\\n6+ 6 + 4 =\\n7+6+4=\\n2+ 4 x 4 =\\n3+ 4 x 4=\\n\\n12 4 x 4 +\\n13 5 + 4 x\\n14 6 + 4 x\\n15 7 +4 x\\n\\n4=\\n4=\\n4\\n4=\\n\\n=\\n\\n20 4 + 6 x\\n21 5 + 6 x\\n22 6 + 6 x\\n23 7 + 6 x\\n\\n4\\n4\\n4\\n4\\n\\n16\\n17\\n18\\n19\\n\\n0+6 x\\n1+ 6 x\\n2+ 6x\\n3+ 6x\\n\\n4=\\n4=\\n4=\\n4=\\n\\n20\\n21\\n22\\n23\\n\\n4+\\n5+\\n6+\\n7+\\n\\n4 = 28\\n4 = 29\\n4 30\\n4 = 31\\n\\n6\\n6\\n6\\n6\\n\\nx\\nx\\nx\\nx\\n\\n=\\n\\nFigure 7: Sample CRBP solutions to Inverse Arithmetic\\n\\nThe Inverse Arithmetic problem can be summarized as follows: Given i E 25 , find\\n:1:, y, z E 23 and 0, <> E {+(OO)' -(01)' X (10)' +(11)} such that :I: oy<>z = i. In all there are\\n13 bits of output, interpreted as three 3-bit binary numbers and two 2-bit operators,\\nand the task is to pick an output that evaluates to the given 5-bit binary input\\nunder the usual rules: operator precedence, left-right evaluation, integer division,\\nand division by zero fails.\\nAs shown in Figure 7, CRBP sometimes solves this problem essentially by discovering positional notation, and sometimes produces less-globally structured solutions,\\nparticularly as outputs for lower-valued i's, which have a wider range of solutions.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n5\\n\\nCONCLUSIONS\\n\\nSome basic concepts of supervised learning appear in different guises when the\\nparadigm of reinforcement learning is applied to large output spaces. Rather than\\na \\\"learning phase\\\" followed by a \\\"generalization test,\\\" in reinforcement learning\\nthe search problem is a generalization test, performed simultaneously with learning.\\nInformation is put to work as soon as it is acquired.\\nThe problem of of \\\"overfitting\\\" or \\\"learning the noise\\\" seems to be less of an issue,\\nsince learning stops automatically when consistent success is reached. In experiments not reported here we gradually increased the number of hidden units on\\nthe 8-bit copy problem from 8 to 25 without observing the performance decline\\nassociated with \\\"too many free parameters.\\\"\\nThe 2 k -attractors (and 2 k -folds-generalizing Excluded Middle) families provide\\na starter set of sample problems with easily understood and distinctly different\\nextreme cases.\\nIn degenerate output spaces, generalization decisions can be seen directly in the\\ndiscovered mapping. Network analysis is not required to \\\"see how the net does it.\\\"\\nThe possibility of ultimately generating useful new knowledge via reinforcement\\nlearning algorithms cannot be ruled out.\\nReferences\\nAckley, D.H. (1987) A connectionist machine for genetic hillclimbing. Boston, MA: Kluwer\\nAcademic Press.\\nAckley, D.H. (1989) Associative learning via inhibitory search. In D.S. Touretzky (ed.),\\nAdvances in Neural Information Processing Systems 1, 20-28. San Mateo, CA: Morgan\\nKaufmann.\\nAllen, R.B. (1989) Developing agent models with a neural reinforcement technique. IEEE\\nSystems, Man, and Cybernetics Conference. Cambridge, MA.\\nAnderson, C.W. (1986) Learning and problem solving with multilayer connectionist systems. University of Mass. Ph.D. dissertation. COINS TR 86-50. Amherst, MA.\\nBarto, A.G. (1985) Learning by statistical cooperation of self-interested neuron-like computing elements. Human Neurobiology, 4:229-256.\\nBarto, A.G., & Anandan, P. (1985) Pattern recognizing stochastic learning automata.\\nIEEE Transactions on Systems, Man, and Cybernetics, 15, 360-374.\\nRumelhart, D.E., Hinton, G.E., & Williams, R.J. (1986) Learning representations by backpropagating errors. Nature, 323, 533-536.\\nSutton, R.S. (1984) Temporal credit assignment in reinforcement learning. University of\\nMass. Ph.D. dissertation. COINS TR 84-2. Amherst, MA.\\nWilliams, R.J. (1988) Toward a theory of reinforcement-learning connectionist systems.\\nCollege of Computer Science of Northeastern University Technical Report NU-CCS-88-3.\\nBoston, MA.\\n\\n557\\n\\n\\f\",\n          \"Dynamics of Supervised Learning with\\nRestricted Training Sets and Noisy Teachers\\n\\nA.C.C. Coolen\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ntcoolen@mth.kc1.ac.uk\\n\\nC.W.H.Mace\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ncmace@mth.kc1.ac.uk\\n\\nAbstract\\nWe generalize a recent formalism to describe the dynamics of supervised\\nlearning in layered neural networks, in the regime where data recycling\\nis inevitable, to the case of noisy teachers. Our theory generates reliable\\npredictions for the evolution in time of training- and generalization errors, and extends the class of mathematically solvable learning processes\\nin large neural networks to those situations where overfitting can occur.\\n\\n1 Introduction\\nTools from statistical mechanics have been used successfully over the last decade to study\\nthe dynamics of learning in layered neural networks (for reviews see e.g. [1] or [2]). The\\nsimplest theories result upon assuming the data set to be much larger than the number\\nof weight updates made, which rules out recycling and ensures that any distribution of\\nrelevance will be Gaussian. Unfortunately, both in terms of applications and in terms of\\nmathematical interest, this regime is not the most relevant one. Most complications and\\npeculiarities in the dynamics of learning arise precisely due to data recycling, which creates\\nfor the system the possibility to improve performance by memorizing answers rather than\\nby learning an underlying rule. The dynamics of learning with restricted training sets was\\nfirst studied analytically in [3] (linear learning rules) and [4] (systems with binary weights).\\nThe latter studies were ahead of their time, and did not get the attention they deserved just\\nbecause at that stage even the simpler learning dynamics without data recycling had not\\nyet been studied. More recently attention has moved back to the dynamics of learning\\nin the recycling regime. Some studies aimed at developing a general theory [5, 6, 7],\\nsome at finding exact solutions for special cases [8]. All general theories published so far\\nhave in common that they as yet considered realizable scenario's: the rule to be learned\\nwas implementable by the student, and overfitting could not yet occur. The next hurdle is\\nthat where restricted training sets are combined with unrealizable rules. Again some have\\nturned to non-typical but solvable cases, involving Hebbian rules and noisy [9] or 'reverse\\nwedge' teachers [10]. More recently the cavity method has been used to build a general\\ntheory [11] (as yet for batch learning only). In this paper we generalize the general theory\\nlaunched in [6,5,7], which applies to arbitrary learning rules, to the case of noisy teachers.\\nWe will mirror closely the presentation in [6] (dealing with the simpler case of noise-free\\nteachers), and we refer to [5, 7] for background reading on the ideas behind the formalism.\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n238\\n\\n2 Definitions\\nAs in [6, 5] we restrict ourselves for simplicity to perceptrons. A student perceptron operates a linear separation, parametrised by a weight vector J E iRN :\\nS:{-I,I}N -t{-I,I}\\n\\nS(e) = sgn[J?e]\\n\\nIt aims to emulate a teacher o~erating a similar rule, which, however, is characterized by a\\nvariable weight vector BE iR ,drawn at random from a distribution P(B) such as\\nP(B) = >'6[B+B*]\\n\\noutput noise:\\n\\n+ (1->')6[B-B*]\\n\\n(1)\\n\\nP(B) = [~~/NrN e- tN (B-B')2/E2\\n(2)\\nThe parameters>. and ~ control the amount of teacher noise, with the noise-free teacher\\nB = B* recovered in the limits>. -t 0 and ~ -t O. The student modifies J iteratively, using\\nexamples of input vectors which are drawn at random from a fixed (randomly composed)\\nE {-I, I}N with a> 0, and the corresponding\\ntraining set containing p = aN vectors\\nvalues of the teacher outputs. We choose the teacher noise to be consistent, i.e. the answer\\nwill remain the same when that particular question\\ngiven by the teacher to a question\\nre-appears during the learning process. Thus T(e?) = sgn[BJL . e], with p teacher weight\\nvectors BJL, drawn randomly and independently from P(B), and we generalize the training\\nl , B l ), . .. , (e, BP)}. Consistency of teacher noise is natural\\nset accordingly to jj =\\nin terms of applications, and a prerequisite for overfitting phenomena. Averages over the\\ntraining set will be denoted as ( ... ) b; averages over all possible input vectors E {-I, I}N\\nas ( ... )e. We analyze two classes of learning rules, of the form J (? + 1) = J (?) + f).J (?):\\n\\nGaussian weight noise:\\n\\ne\\n\\ne\\n\\ne\\n\\nHe\\n\\ne\\n\\n= 11 {e(?) 9 [J(?)?e(?), B(?)?e(?)] - ,J(?) }\\nf).J(?) = 11 {(e 9 [J(?)?e, B?eDl> - ,J(m) }\\n\\non-line:\\n\\nf).J(?)\\n\\nbatch :\\n\\n(3)\\n\\nIn on-line learning one draws at each step ? a question/answer pair (e (?), B (?)) at random from the training set. In batch learning one iterates a deterministic map which is an\\naverage over all data in the training set. Our performance measures are the training- and\\ngeneralization errors, defined as follows (with the step function O[x > 0] = 1, O[x < 0] = 0):\\nEt(J)\\n\\n= (O[-(J ?e)(B ?em b\\n\\nEg(J)\\n\\n= (O[-(J ?e)(B* ?e)])e\\n\\n(4)\\n\\nWe introduce macroscopic observables, taylored to the present problem, generalizing [5, 6]:\\nQ[J]=J 2,\\nR[J]=J?B*,\\nP[x,y,z;J]=(6[x-J?e]6[y-B*?e]6[z-B?eDl> (5)\\nAs in [5, 6] we eliminate technical subtleties by assuming the number of arguments (x, y, z)\\nfor which P[x, y, z; J] is evaluated to go to infinity after the limit N -t 00 has been taken.\\n\\n3 Derivation of Macroscopic Laws\\nUpon generalizing the calculations in [6, 5], one finds for on-line learning:\\n\\n!\\n!\\n\\nQ = 2'f} !dXdydZ P[x, y, z] xg[x, z] - 2'f},Q + 'f}2!dXdYdZ P[x, y, z] g2[x, z]\\n\\n(6)\\n\\nR = 'f} !dXdydZ P[x, y, z] y9[x, z]- 'f},R\\n\\n(7)\\n\\n:t\\n\\nP[x, y, z] =\\n\\n~\\n\\n!\\n\\ndx' P[x', y, z] {6[x-x' -'f}G[x', z]] -6[x-x']}\\n\\n-'f}! / dx'dy'dz' / dx'dy'dz'9[x', z]A[x, y, z; x',y', z']\\n\\n1\\n+'i'f}2\\n\\n!\\n\\n+ 'f}, :x\\n\\nEP2P[x, y, z]\\ndx'dy'dz' P[x', y', z']92[x', z'] 8x\\n\\n{xP[x , y, z]}\\n\\n(8)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n239\\n\\nThe complexity of the problem is concentrated in a Green's function:\\nA[x, y, Zj x', y', z'] = lim\\nN-+oo\\n\\n(( ([1-6ee , ]6[x-J?e]6[y-B*?e]6[z-B?e] (e?e')6[x' -J?e']6[y' - B*?e']6[y' - B?e'])i?i> )QW;t\\n\\nJ\\n\\nIt involves a conditional average of the form (K[J])QW;t = dJ Pt(JIQ,R,P)K[J], with\\nPt(J) 6[Q-Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] -P[x, y, Zj J]]\\nPt(JIQ,R,P)\\nJdJ Pt(J) 6[Q - Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] - P[x, y, z; J]]\\n\\n=\\n\\nin which Pt (J) is the weight probability density at time t. The solution of (6,7,8) can be\\nused to generate the N -+ 00 performance measures (4) at any time:\\nEt\\n\\n=/\\n\\ndxdydz P[x, y, z]O[-xz]\\n\\nEg\\n\\n= 11\\\"-1 arccos[RIVQ]\\n\\n(9)\\n\\nExpansion of these equations in powers of\\\"\\\" and retaining only the terms linear in \\\"\\\" gives\\nthe corresponding equations describing batch learning. So far this analysis is exact.\\n\\n4\\n\\nClosure of Macroscopic Laws\\n\\nAs in [6, 5] we close our macroscopic laws (6,7,8) by making the two key assumptions\\nunderlying dynamical replica theory:\\n(i) For N -+ 00 our macroscopic observables obey closed dynamic equations.\\n(ii) These equations are self-averaging with respect to the specific realization of D.\\n\\n(i) implies that probability variations within {Q, R, P} subshells are either absent or irrelevant to the macroscopic laws. We may thus make the simplest choice for Pt (J IQ, R, P):\\nPt(JIQ,R,P) -+ 6[Q-Q[J]] 6[R-R[J]]\\n\\nII 6[P[x,y,z]-P[x,y,ZjJ]]\\n\\n(10)\\n\\nxyz\\n\\nThe procedure (10) leads to exact laws if our observables {Q, R, P} indeed obey closed\\nequations for N -+ 00. It is a maximum entropy approximation if not. (ii) allows us\\nto average the macroscopic laws over all training sets; it is observed in simulations, and\\nproven using the formalism of [4]. Our assumptions (10) result in the closure of (6,7,8),\\nsince now the Green's function can be written in terms of {Q, R, Pl. The final ingredient\\nof dynamical replica theory is doing the average of fractions with the replica identity\\n\\n/ JdJ W[JID]GIJID])\\n\\n\\\\\\n\\nJdJ W[JID]\\n\\n= lim\\nsets\\n\\n/dJ I\\n\\n???\\n\\ndJn (G[J 1 ID]\\n\\nn-+O\\n\\nIT\\n\\nW[JO<ID])sets\\n\\na=1\\n\\nOur problem has been reduced to calculating (non-trivial) integrals and averages. One\\nfinds that P[x, y, z] P[x, zly]P[y] with Ply] (211\\\")-!exp[-!y 21With the short-hands\\nDy = P[y]dy and (f(x, y, z)) = Dydxdz P[x, zly]f(x, y, z) we can write the resulting\\nmacroscopic laws, for the case of output noise (1), in the following compact way:\\n\\n=\\n\\nd\\n\\ndt Q = 2\\\",(V - ,Q)\\n\\n[)\\n\\n[)tP[x,zly] =\\n\\n=\\n\\nJ\\n\\n+ rJ2 Z\\n\\nd\\n\\ndtR = \\\",(W - ,R)\\n\\n(11)\\n\\n1 [)x[)22P[x,zIY]\\na1/dx'P[x',zly] {6[x-x'-\\\",G[x',z]]-6[x-x'] }+2\\\",2Z\\n\\n-\\\",:x {P[x,zly]\\n\\n[U(x-RY)+Wy-,x+[V-RW-(Q-R2)U]~[x,y,z])}\\n\\n(12)\\n\\nwith\\n\\nU = (~[x, y, z]9[x, z]),\\n\\nv = (x9[x, z]),\\n\\nW = (y9[x, z]),\\n\\nZ = (9 2[x, z])\\n\\nThe solution of (12) is at any time of the following form:\\n\\nP[x,zly]\\n\\n= (1-,x)6[y-z]P+[xly] + ,x6[y+z]P-[xly]\\n\\n(13)\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n240\\n\\nFinding the function <I> [x, y, z] (in replica symmetric ansatz) requires solving a saddle-point\\nproblem for a scalar observable q and two functions M?[xly]. Upon introducing\\n\\nB = . . :. V. .,. .q.,-Q___R,-2\\nQ(I-q)\\n(with Jdx M?[xly]\\n\\nJdx M?[xly]eBxs J[x, y]\\nJdx M?[xly]eBxs\\n\\n(f[x, y])? =\\n*\\n\\n= 1 for all y) the saddle-point equations acquire the fonn\\np?[Xly] =\\n\\nfor all X, y :\\n\\n((x-Ry)2) + (qQ-R 2)[I-!:.]\\na\\n\\n!\\n\\nDs (O[X -xl);\\n\\n2 !DYDS S[(I-A)(X); + A(X);]\\n= qQ+Q-2R\\n..jqQ_R2\\n\\n(14)\\n(15)\\n\\nThe equations (14) which detennine M?[xly] have the same structure as the corresponding\\n(single) equation in [5, 6], so the proofs in [5, 6] again apply, and the solutions M?[xly],\\ngiven a q in the physical range q E [R2/Q, 1], are unique. The function <I> [x, y, z] is then\\ngiven by\\n<I> [X,\\n\\ny, z]\\n\\n=!\\n\\nDs s\\n{(I-A)O[Z-y](o[X -x)); + AO[Z+Y](o[X -xl);}\\n..jqQ_R2 P[X, zly]\\n(16)\\n\\nWorking out predictions from these equations is generally CPU-intensive, mainly due to\\nthe functional saddle-point equation (14) to be solved at each time step. However, as in [7]\\none can construct useful approximations of the theory, with increasing complexity:\\n\\n(i) Large a approximation (giving the simplest theory, without saddle-point equations)\\n(ii) Conditionally Gaussian approximation for M[xly] (with y-dependent moments)\\n(iii) Annealed approximation of the functional saddle-point equation\\n\\n5 Benchmark Tests: The Limits a --+ 00 and ,\\\\ --+ 0\\nWe first show that in the limit a --+ 00 our theory reduces to the simple (Q, R) formalism\\nof infinite training sets, as worked out for noisy teachers in [12]. Upon making the ansatz\\n\\np?[xly] = P[xly] = [27r(Q-R 2)]-t e- t [x- Rv]2/(Q-R 2)\\n\\n(17)\\n\\none finds\\n\\n<I>[x,y,Z] = (x-Ry)/(Q-R 2)\\n\\nM?[xly] = P[xly],\\n\\nInsertion of our ansatz into (12), followed by rearranging of terms and usage of the above\\nexpression for <I> [x, y, z], shows that (12) is satisfied. The remaining equations (11) involve\\nonly averages over the Gaussian distribution (17), and indeed reduce to those of [12]:\\n\\n~! Q =\\n\\n(I-A) { 2(x9[x, y))\\n1 d\\n--d R\\n1} t\\n\\n+ 1}{92[x, y)) } + A {2(x9[x,-y)) + 1}(92[x,-y)) } - 2,Q\\n\\n= (I-A)(y9[x,y)) + A(y9[x,-yl) -,R\\n\\nNext we turn to the limit A --+ 0 (restricted training sets & noise-free teachers) and show that\\nhere our theory reproduces the fonnalism of [6,5]. Now we make the following ansatz:\\n\\nP+[xly] = P[xly],\\n\\nP[x, zly]\\n\\n= o[z-y]P[xIY]\\n\\n(18)\\n\\nInsertion shows that for A = 0 solutions of this fonn indeed solve our equations, giving\\n<p[x, y, z]--+ <I> [x, y] and M+[xly]\\nM[xly), and leaving us exactly with the fonnalism\\nof [6, 5] describing the case of noise-free teachers and restricted training sets (apart from\\nsome new tenns due to the presence of weight decay, which was absent in [6, 5]).\\n\\n=\\n\\n\\f241\\n\\nSupervised Learning with Restricted Training Sets\\n0. , r------~--__,\\n\\n0..4\\n\\n~-------_____I\\n\\n0..4\\n\\n11>=0.'\\n\\n0..3\\n\\na=4\\n\\n0. ,\\n\\n0..0.\\n\\n--\\n\\n, 0.\\n\\n0.2\\n\\n_ __ ___ _____ _\\n\\na= 1\\n\\n0;=1\\n\\n------- ---- -- --- -\\n\\n0.\\n\\n0;=2\\n\\n=-=\\n-\\n\\n0;=2\\n\\n- - ----- -\\n\\na=4\\na=4\\n\\n= =-=\\n--=-=--=-=--=-=-=-- -=-=-_oed\\n\\na=4\\n\\n,\\n\\n0;=2\\n\\n':::::========:::j\\n\\n0..3\\n\\n-- - ----\\n\\n0;=1\\n\\n:::---- - -----1\\n\\n0;=2\\n\\n0..2\\n\\n11>=0.'\\n\\n~-------~\\n\\n0;=1\\n\\n0.,\\n\\n11>=0,\\n\\n\\\"\\n\\n,\\n\\nno. I\\n\\n0.\\n\\n, 0.\\n\\n\\\"\\n\\nFigure 1: On-line Hebbian learning: conditionally Gaussian approximation versus exact\\nsolution in [9] (.,., = 1, ,X = 0.2). Left: \\\"I = 0.1, right: \\\"I = 0.5. Solid lines: approximated\\ntheory, dashed lines: exact result. Upper curves: Eg as functions of time (here the two\\ntheories agree), lower curves: E t as functions of time.\\n\\n6\\n\\nBenchmark Tests: Hebbian Learning\\n\\nThe special case of Hebbian learning, i.e. Q[x, z] = sgn(z), can be solved exactly at any\\ntime, for arbitrary {a, ,x, \\\"I} [9], providing yet another excellent benchmark for our theory.\\nFor batch execution of Hebbian learning the macroscopic laws are obtained upon expanding\\n(11,12) and retaining only those terms which are linear in.,.,. All integrations can now be\\ndone and all equations solved explicitly, resulting in U =0, Z = 1, W = (I-2,X)J2/7r, and\\n\\nQ\\n\\n= Qo e-2rryt +\\n\\n2Ro(I-2'x) e-17\\\"Yt[I_e-rrrt]\\n\\\"I\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I-e- 17 \\\"Y tF\\n\\\"12\\n\\nR = Ro e- 17\\\"Y t +(I-2'x)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\nq = [aR2+(I_e- 17\\\"Yt)2 i'l]/aQ\\np?[xIY] = [27r(Q-R2)] -t e-tlz-RH sgn(y)[1-e-\\\"..,t]/a\\\"Y]2/(Q-R2)\\n(19)\\nFrom these results, in tum, follow the performance measures Eg = 7r- 1 arccos[ R/ JQ) and\\n\\nE = ! - !(1-,X)!D\\n2\\n\\nt\\n\\n2\\n\\nerf[IYIR+[I-e- 77\\\"Y t ]/a\\\"l] + !,X!D erf[IYIR-[I-e- 17\\\"Y t ]/a\\\"l]\\nY\\nJ2(Q-R2)\\n2\\ny\\nJ2(Q-R2)\\n\\nComparison with the exact solution, calculated along the lines of [9] or, equivalently, obtained upon putting t ?\\nin [9], shows that the above expressions are all exact.\\n\\n.,.,-2\\n\\nFor on-line execution we cannot (yet) solve the functional saddle-point equation in general.\\nHowever, some analytical predictions can still be extracted from (11,12,13):\\n\\nQ = Qo e-217\\\"Yt + 2Ro(I-2,X) e-77\\\"Yt[I_e-17\\\"Yt]\\n\\\"I\\n\\nR = Ro e- 17\\\"Y t + (I-2,X)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\n\\nJ\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I_e- 17\\\"Y t ]2\\n\\\"12\\n\\n+ !L[I_e- 217\\\"Y t ]\\n2\\\"1\\n\\ndx xP?[xIY] = Ry ? sgn(y)[I-e- 17\\\"Y t ]/a\\\"l\\n\\nwith U =0, W = (I-2,X)J2/7r, V = W R+[I-e- 17\\\"Y t ]/a\\\"l, and Z = 1. Comparison with the\\nresults in [9] shows that the above expressions, and thus also that of E g , are all fully exact,\\nat any time. Observables involving P[x, y, z] (including the training error) are not as easily\\nsolved from our equations. Instead we used the conditionally Gaussian approximation\\n(found to be adequate for the noiseless Hebbian case [5, 6, 7]). The result is shown in\\nfigure 1. The agreement is reasonable, but significantly less than that in [6]; apparently\\nteacher noise adds to the deformation of the field distribution away from a Gaussian shape.\\n\\n\\f242\\n\\nA. C. C. Coolen and C. W H. Mac\\n\\n~\\n\\n0.6\\n\\n000000\\n\\n0.4\\n\\n0.4\\n\\nE\\n\\n~\\n\\n0.2\\n\\nI\\ni\\n0.0\\n\\n0\\n\\n4\\n\\n2\\n\\n6\\n\\n10\\n\\n0.0\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\nX\\n\\n0.6\\n\\nf\\n\\n0.4\\n\\n0.4 [\\n\\nE\\n0.2\\n\\n0.2\\n\\n0.0\\n\\nL-o!i6iIII.\\\"\\\"\\\"\\\"\\\"',-\\\"--~_~~_ _--'\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\n\\n2\\n\\n3\\n\\nX\\n\\n,=\\n\\nFigure 2: Large a approximation versus numerical simulations (with N = 10,000), for\\n0 and A = 0.2. Top row: Perceptron rule, with.,., = ~. Bottom row: Adatron rule,\\nwith.,., = ~. Left: training errors E t and generalisation errors Eg as functions of time, for\\naE {~, 1, 2}. Lines: approximated theory, markers: simulations (circles: E t , squares: Eg) .\\nRight: joint distributions for student field and teacher noise p?[x] = dy P[x, y, z = ?y]\\n(upper: P+[x], lower: P-[x]). Histograms: simulations, lines: approximated theory.\\n\\nJ\\n\\n7\\n\\nNon-Linear Learning Rules: Theory versus Simulations\\n\\nIn the case of non-linear learning rules no exact solution is known against which to test our\\nformalism, leaving numerical simulations as the yardstick. We have evaluated numerically\\nthe large a approximation of our theory for Perceptron learning, 9[x, z] = sgn(z)O[-xz],\\nand for Adatron learning, 9[x, z] = sgn(z)lzIO[-xz]. This approximation leads to the\\nfollowing fully explicit equation for the field distributions:\\n\\n1/\\n\\nd\\n-p?[xly]\\n= dt\\na\\n.\\n\\nWith\\n\\nU=\\n\\n' +1\\n\\ndx' p?[x'ly]{o[x-x'-.,.,.1'[x', ?y]] -o[x-x]}\\n\\n_ ~ {P[ I ] [W _\\n.,., 8\\nx y\\ny\\n\\nJ\\n\\nX\\n\\n~ p?[xly]\\n\\n_.,.,2 Z!:I 2\\n2\\nuX\\n\\n,X + U[X?(y)-RY]+(V-RW)[X-X?(y)]]}\\nQ _ R2\\n\\nDydx {(I-A)P+[xly][x-P(y)]9[x,Y]+AP-[xly][x-x-(y)]9[x,-y])\\nV =\\nW=\\nZ=\\n\\n!\\n1\\n1\\n\\nDydx x {(I-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\nDydx y {(1-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\n\\nDydx {(I-A)P+[xly]92[x, Y]+AP-[xly]9 2[x,-yJ)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n243\\n\\nJ\\n\\nand with the short-hands X?(y) = dx xP?[xly). The result of our comparison is shown\\nin figure 2. Note: E t increases monotonically with a, and Eg decreases monotonically\\nwith a, at any t. As in the noise-free formalism [7], the large a approximation appears to\\ncapture the dominant terms both for a -7 00 and for a -7 O. The predicting power of our\\ntheory is mainly limited by numerical constraints. For instance, the Adatron learning rule\\ngenerates singularities at x = 0 in the distributions P?[xly) (especially for small \\\"I) which,\\nalthough predicted by our theory, are almost impossible to capture in numerical solutions.\\n\\n8 Discussion\\nWe have shown how a recent theory to describe the dynamics of supervised learning with\\nrestricted training sets (designed to apply in the data recycling regime, and for arbitrary online and batch learning rules) [5, 6, 7] in large layered neural networks can be generalized\\nsuccessfully in order to deal also with noisy teachers. In our generalized approach the joint\\ndistribution P[x, y, z) for the fields of student, 'clean' teacher, and noisy teacher is taken to\\nbe a dynamical order parameter, in addition to the conventional observables Q and R. From\\nthe order parameter set {Q, R, P} we derive the generalization error Eg and the training\\nerror E t . Following the prescriptions of dynamical replica theory one finds a diffusion\\nequation for P[x, y, z], which we have evaluated by making the replica-symmetric ansatz.\\nWe have carried out several orthogonal benchmark tests of our theory: (i) for a -7 00 (no\\ndata recycling) our theory is exact, (ii) for A -7 0 (no teacher noise) our theory reduces\\nto that of [5, 6, 7], and (iii) for batch Hebbian learning our theory is exact. For on-line\\nHebbian learning our theory is exact with regard to the predictions for Q, R, Eg and the\\ny-dependent conditional averages Jdx xP?[xly), at any time, and a crude approximation\\nof our equations already gives reasonable agreement with the exact results [9] for E t . For\\nnon-linear learning rules (Perceptron and Adatron) we have compared numerical solution\\nof a simple large a aproximation of our equations to numerical simulations, and found\\nsatisfactory agreement. This paper is a preliminary presentation of results obtained in the\\nsecond stage of a research programme aimed at extending our theoretical tools in the arena\\nof learning dynamics, building on [5, 6, 7]. Ongoing work is aimed at systematic application of our theory and its approximations to various types of non-linear learning rules, and\\nat generalization of the theory to multi-layer networks.\\n\\nReferences\\n[1]\\n[2]\\n[3]\\n[4]\\n[5]\\n[6]\\n[7]\\n[8]\\n[9]\\n[10]\\n[11]\\n[12]\\n\\nMace C.W.H. and Coolen AC.C (1998), Statistics and Computing 8, 55\\nSaad D. (ed.) (1998), On-Line Learning in Neural Networks (Cambridge: CUP)\\nHertz J.A., Krogh A and Thorgersson G.I. (1989), J. Phys. A 22, 2133\\nHomerH. (1992a), Z. Phys. B 86, 291 and Homer H. (1992b), Z. Phys. B 87,371\\nCoolen A.C.C. and Saad D. (1998), in On-Line Learning in Neural Networks, Saad\\nD. (ed.), (Cambridge: CUP)\\nCoolen AC.C. and Saad D. (1999), in Advances in Neural Information Processing\\nSystems 11, Kearns D., Solla S.A., Cohn D.A (eds.), (MIT press)\\nCoolen A.C.C. and Saad D. (1999), preprints KCL-MTH-99-32 & KCL-MTH-99-33\\nRae H.C., Sollich P. and Coolen AC.C. (1999), in Advances in Neural Information\\nProcessing Systems 11, Kearns D., Solla S.A., Cohn D.A. (eds.), (MIT press)\\nRae H.C., Sollich P. and Coolen AC.C. (1999),J. Phys. A 32, 3321\\nInoue J.I. (1999) private communication\\nWong K.YM., Li S. and Tong YW. (1999),preprint cond-mat19909004\\nBiehl M., Riegler P. and Stechert M. (1995), Phys. Rev. E 52, 4624\\n\\n\\f\",\n          \"Predicting Action Content On-Line and in\\nReal Time before Action Onset ? an\\nIntracranial Human Study\\n\\nShengxuan Ye\\nCalifornia Institute of Technology\\nPasadena, CA\\nsye@caltech.edu\\n\\nUri Maoz\\nCalifornia Institute of Technology\\nPasadena, CA\\nurim@caltech.edu\\nIan Ross\\nHuntington Hospital\\nPasadena, CA\\nianrossmd@aol.com\\n\\nAdam Mamelak\\nCedars-Sinai Medical Center\\nLos Angeles, CA\\nadam.mamelak@cshs.org\\n\\nChristof Koch\\nCalifornia Institute of Technology\\nPasadena, CA\\nAllen Institute for Brain Science\\nSeattle, WA\\nkoch@klab.caltech.edu\\n\\nAbstract\\nThe ability to predict action content from neural signals in real time before the action occurs has been long sought in the neuroscientific study of decision-making,\\nagency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious,\\nvoluntary action as well as for brain-machine interfaces. Here, epilepsy patients,\\nimplanted with intracranial depth microelectrodes or subdural grid electrodes for\\nclinical purposes, participated in a ?matching-pennies? game against an opponent.\\nIn each trial, subjects were given a 5 s countdown, after which they had to raise\\ntheir left or right hand immediately as the ?go? signal appeared on a computer\\nscreen. They won a fixed amount of money if they raised a different hand than\\ntheir opponent and lost that amount otherwise. The question we here studied was\\nthe extent to which neural precursors of the subjects? decisions can be detected in\\nintracranial local field potentials (LFP) prior to the onset of the action.\\nWe found that combined low-frequency (0.1?5 Hz) LFP signals from 10 electrodes\\nwere predictive of the intended left-/right-hand movements before the onset of the\\ngo signal. Our ORT system predicted which hand the patient would raise 0.5 s\\nbefore the go signal with 68?3% accuracy in two patients. Based on these results,\\nwe constructed an ORT system that tracked up to 30 electrodes simultaneously,\\nand tested it on retrospective data from 7 patients. On average, we could predict\\nthe correct hand choice in 83% of the trials, which rose to 92% if we let the system\\ndrop 3/10 of the trials on which it was less confident. Our system demonstrates?\\nfor the first time?the feasibility of accurately predicting a binary action on single\\ntrials in real time for patients with intracranial recordings, well before the action\\noccurs.\\n\\n1\\n\\n\\f1\\n\\nIntroduction\\n\\nThe work of Benjamin Libet [1, 2] and others [3, 4] has challenged our intuitive notions of the relation between decision making and conscious voluntary action. Using electrocorticography (EEG),\\nthese experiments measured brain potentials from subjects that were instructed to flex their wrist at a\\ntime of their choice and note the position of a rotating dot on a clock when they felt the urge to move.\\nThe results suggested that a slow cortical wave measured over motor areas?termed ?readiness potential? [5], and known to precede voluntary movement [6]?begins a few hundred milliseconds before the average reported time of the subjective ?urge? to move. This suggested that action onset and\\ncontents could be decoded from preparatory motor signals in the brain before the subject becomes\\naware of an intention to move and of the contents of the action. However, the readiness potential\\nwas computed by averaging over 40 or more trials aligned to movement onset after the fact. More\\nrecently, it was shown that action contents can be decoded using functional magnetic-resonance\\nimaging (fMRI) several seconds before movement onset [7]. But, while done on a single-trial basis,\\ndecoding the neural signals took place off-line, after the experiment was concluded, as the sluggish\\nnature of fMRI hemodynamic signals precluded real-time analysis. Moreover, the above studies\\nfocused on arbitrary and meaningless action?purposelessly raising the left or right hand?while\\nwe wanted to investigate prediction of reasoned action in more realistic, everyday situations with\\nconsequences for the subject.\\nIntracranial recordings are good candidates for single-trial, ORT analysis of action onset and contents [8, 9], because of the tight temporal pairing of LFP to the underlying neuronal signals. Moreover, such recordings are known to be cleaner and more robust, with signal-to-noise ratios up to\\n100 times larger than surface recordings like EEG [10, 11]. We therefore took advantage of a rare\\nopportunity to work with epilepsy patients implanted with intracranial electrodes for clinical purposes. Our ORT system (Fig. 1) predicts, with far above chance accuracy, which one of two future\\nactions is about to occur on this one trial and feeds the prediction back to the experimenter, all\\nbefore the onset of the go signal that triggers the patient?s movement (see Experimental Methods).\\nWe achieve relatively high prediction performance using only part of the data?learning from brain\\nactivity in past trials only (Fig. 2) to predict future ones (Fig. 3)?while still running the analysis\\nquickly enough to act upon the prediction before the subject moved.\\n\\n2\\n2.1\\n\\nExperimental Methods\\nSubjects\\n\\nSubjects in this experiment were 8 consenting intractable epilepsy patients that were implanted with\\nintracranial electrodes as part of their presurgical clinical evaluation (ages 18?60, 3 males). They\\nwere inpatients in the neuro-telemetry ward at the Cedars Sinai Medical Center or the Huntington\\nMemorial Hospital, and are designated with CS or HMH after their patient numbers, respectively. Six\\nof them?P12CS, P15CS, P22CS and P29?31HMH were implanted with intracortical depth electrodes targeting their bilateral anterior-cingulate cortex, amygdala, hippocampus and orbitofrontal\\ncortex. These electrodes had eight 40 ?m microwires at their tips, 7 for recording and 1 serving as\\na local ground. Two patients, P15CS and P22CS, had additional microwires in the supplementary\\nmotor area. We utilized the LFP recorded from the microwires in this study. Two other patients,\\nP16CS and P19CS, were implanted with an 8?8 subdural grid (64 electrodes) over parts of their\\ntemporal and prefrontal dorsolateral cortices. The data of one patient?P31HMH?was excluded\\nbecause microwire signals were too noisy for meaningful analysis. The institutional review boards\\nof Cedars Sinai Medical Center, the Huntington Memorial Hospital and the California Institute of\\nTechnology approved the experiments.\\nDuring the experiment, the subject sat in a hospital bed in a semi-inclined ?lounge chair? position.\\nThe stimulus/analysis computer (bottom left of Fig. 4) displaying the game screen (bottom right\\ninset of Fig. 4) was positioned to be easily viewable for the subject. When playing against the\\nexperimenter, the latter sat beside the bed. The response box was placed within easy reach of the\\nsubject (Fig. 4).\\n2\\n\\n\\f2.2\\n\\nExperiment Design\\n\\nAs part of our focus on purposeful, reasoned action, we had the subjects play a matching-pennies\\ngame?a 2-choice version of ?rock paper scissors??either against the experimenter or against a\\ncomputer. The subjects pressed down a button with their left hand and another with their right on a\\nresponse box. Then, in each trial, there was a 5 s countdown followed by a go signal, after which\\nthey had to immediately lift one of their hands. It was agreed beforehand that the patient would win\\nthe trial if she lifted a different hand than her opponent, and lose if she raised the same hand as her\\nopponent. Both players started off with a fixed amount of money, $5, and in each trial $0.10 was\\ndeducted from the loser and awarded to the winner. If a player lifted her hand before the go signal,\\ndid not lift her hand within 500 ms of the go signal, or lifted no hand or both hands at the go signal?\\nan error trial?she lost $0.10 without her opponent gaining any money. The subjects were shown the\\ncountdown, the go signal, the overall score, and various instructions on a stimulus computer placed\\nbefore them (Fig. 4). Each game consisted of 50 trials. If, at the end of the game, the subject had\\nmore money than her opponent, she received that money in cash from the experimenter.\\nBefore the experimental session began, the experimenter explained the rules of the game to the subject, and she could practice playing the game until she was familiar with it. Consequently, patients\\nusually made only few errors during the games (<6% of the trials). Following the tutorial, the subject played 1?3 games against the computer and then once against the experimenter, depending on\\ntheir availability and clinical circumstances. The first 2 games of P12CS were removed because\\nthe subject tended to constantly raise the right hand regardless of winning or losing. Two patients,\\nP15CS and P19CS, were tested in actual ORT conditions. In such sessions?3 games each?the\\nsubjects always played against the experimenter. These ORT games were different from the other\\ngames in two respects. First, a computer screen was placed behind the patient, in a location where\\nshe could not see it. Second, the experimenter was wearing earphones (Fig. 1,4). Half a second before go-signal onset, an arrow pointing towards the hand that the system predicted the experimenter\\nhad to raise to win the trial was displayed on that screen. Simultaneously, a monophonic tone was\\nplayed in the experimenter?s earphone ipsilateral to that hand. The experimenter then lifted that hand\\nat the go signal (see Supplemental Movie).\\n\\nCheetah Machine\\nCollect\\nand save\\ndata\\n\\nPatient\\nwith intracranial electrodes\\n\\nDown\\nsampling\\n\\nBuffer\\n\\n1Gbps\\nRouter\\n\\nTTL Signal\\n\\nThe winner is\\nPlayer 1\\nPLAYER 1 PLAYER 2\\nSCORE 1\\n\\nAnalysis/stimulus machine\\n\\nSCORE 2\\n\\nResponse Box Game Screen\\n\\n/\\nExperimenter\\n\\nResult\\nInterpreta\\ntion\\n\\nAnalysis\\n\\nFiltering\\n\\nDisplay/Sound\\n\\nFigure 1: A schematic diagram of the on-line real-time (ORT) system. Neural signals flow from\\nthe patient through the Cheetah machine to the analysis/stimulus computer, which controls the input\\nand output of the game and computes the prediction of the hand the patient would raise at the go\\nsignal. It displays it on a screen behind the patient and informs the experimenter which hand to raise\\nby playing a tone in his ipsilateral ear using earphones.\\n\\n3\\n\\n\\f3\\n3.1\\n\\nThe real-time system\\nHardware and software overview\\n\\n?V\\n\\n?V\\n\\n?V\\n\\nNeural data from the intracranial electrodes were transferred to a recording system (Neuralynx,\\nDigital Lynx), where it was collected and saved to the local Cheetah machine, down sampled\\nfrom 32 kHz to 2 kHz and buffered. The data were then transferred, through a dedicated 1 Gbps\\nlocal-area network, to the analysis/stimulus machine. This computer first band-pass-filtered the\\ndata to the 0.1?5 Hz range (delta and lower theta bands) using a second-order zero-lag elliptic\\nfilter with an attenuation of 40 dB (cf. Figs. 2a and 2b). We found that this frequency range?\\ngenerally comparable to that of the readiness potential?resulted in optimal prediction performance.\\nIt then ran the analysis algorithm (see below) on the filtered data. This computer also controlled\\nthe game screen, displaying the names of the players, their current scores and various instructions.\\nThe analysis/stimulus computer further\\ncontrolled the response box, which con- (a)\\n800\\nsisted of 4 LED-lit buttons. The buttons of the subject and her opponent\\n600\\nflashed red or blue whenever she or her\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nopponent won, respectively. Addition(b)100\\nally, the analysis/stimulus computer sent\\n0\\na unique transistor-transistor logic (TTL)\\n?100\\n?200\\npulse whenever the game screen changed\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nor a button was pressed on the response\\nbox, which synchronized the timing of (c) 100\\n0\\nthese events with the LFP recordings.\\n?100\\nIn real-time game sessions, the analy?200\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nsis/stimulus computer also displayed the\\nappropriate arrow on the computer screen (d) 1\\nbehind the subject and played the tone\\n0\\nto the appropriate ear of the experimenter\\n?1\\n0.5 s before go-signal onset (Figs. 1,4).\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nThe analysis software was based on a\\nmachine-learning algorithm that trained\\non past-trials data to predict the current\\ntrial and is detailed below. The training phase included the first 70% of the\\ntrials, with the prediction carried out on\\nthe remaining 30% using the trained parameters, together with an online weighting system (see below). The system examined only neural activity, and had no\\naccess to the subject?s left/right-choice\\nhistory. After filtering all the training\\ntrials (Fig. 2b), the system found the\\nmean and standard error over all leftward\\nand rightward training trials, separately\\n(Fig. 2c, left designated in red). It then\\nfound the electrodes and time windows\\nwhere the left/right separation was high\\n(Fig. 2d,e; see below), and trained the classifiers on these time windows (Fig. 2f?g).\\nThe best electrode/time-window/classifier\\n(ETC) combinations were then used to\\npredict the current trial in the prediction\\nphase (Fig. 3). The number of ETCs that\\ncan be actively monitored is currently limited to 10 due to the computational power\\nof the real-time system.\\n\\nEl 49?T1\\n\\n(e)\\n\\nEl 49?T2\\n\\nEl 49?T3\\n\\n1\\n0\\n?1\\n?5\\n\\n?4\\n\\n?3\\n?2\\n?1\\nCountdown to go signal at t=0 (seconds)\\n\\n0\\n\\n(f)\\nClassifier\\nCf1\\n\\nClassifier\\nCf2\\n\\n...\\n\\nClassifier\\nCf6\\n\\nEl 49?T1?Cf1\\nEl 49?T1?Cf2\\nEl 49?T1?Cf6\\n...\\nEl 49?T2?Cf1\\nEl 49?T2?Cf2\\nEl 49?T2?Cf6\\nEl 49?T3?Cf1\\nEl 49?T3?Cf2\\nEl 49?T3?Cf6\\n\\n(g)\\nCombination\\nEl49-T1-Cf2\\n\\nCombination\\nEl49-T2-Cf2\\n\\n...\\n\\nCombination\\nEl49-T2-Cf6\\n\\nFigure 2: The ORT-system?s training phase. Left (in\\nred) and right (in blue) raw signals (a) are low-pass filtered (b). Mean?standard errors of signals preceeding left- and right-hand movments (c) are used to compute a left/right separability index (d), from which time\\nwindows with good separation are found (e). Seven\\nclassifiers are then applied to all the time windows (f)\\nand the best electrode/time-window/classifier combinations are selected (g) and used in the prediction phase\\n(Fig. 3).\\n\\n4\\n\\n\\f?V\\n\\n100\\n0\\n?100\\n?200\\n?5\\n\\n?4\\n\\n?3\\n\\n?2\\n\\n?1\\n\\n0\\n\\nTrained classifiers\\n\\nCombination\\nE l 49?T1?Cf2\\n\\nCombination\\nE l 49?T2?Cf2\\n\\nWeight = 1\\n\\nWeight = 1\\n\\nCombination\\nE l 49?T2?Cf6\\n\\n&\\n\\nWeight = 1\\n\\nPredicted result\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nR\\n\\nL\\n\\n&\\n\\nR\\n\\nL\\nReal result\\n\\nAdjust the weights\\n\\nL\\n\\n==\\n\\nFigure 3: The ORT-system?s prediction phase. A new signal?from 5 to 0.5 seconds before the\\ngo signal?is received in real time, and each electrode/time-window/classifier combination (ETC)\\nclassifies it as resulting in left- or right-hand movement. These predictions are then compared to the\\nactual hand movement, with the weights associated with ETCs that correctly (incorrectly) predicted\\nincreasing (decreasing).\\n\\n3.2\\n\\nComputing optimal left/right-separating time windows\\n\\nThe algorithm focused on finding the time windows with the best left/right separation for the different recording electrodes over the training set (Fig. 2c?e). That is, we wanted to predict whether\\nthe signal aN (t) on trial N will result in a leftward or rightward movement?i.e., whether the label of the N th trial will be Lt or Rt, respectively. For each electrode, we looked at the N ? 1\\nprevious trials a1 (t), a2 (t), . . . , aN ?1 (t), and their associated labels as l1 , l2 , . . . , lN ?1 . Now, let\\nN ?1\\n?1\\nL(t) = {ai (t) | li = Lt}N\\ni=1 and R(t) = {ai (t) | li = Rt}i=1 be the set of previous leftward and\\nrightward trials in the training set, respectively. Furthermore, let Lm (t) (Rm (t)) and Ls (t) (Rs (t))\\nbe the mean and standard error of L(t) (R(t)), respectively. We can now define the normalized\\nrelative left/right separation for each electrode at time t (see Fig. 2d):\\n?\\n[Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)]\\n?\\n?\\nif [Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)] > 0\\n?\\n?\\nLm (t) ? Rm (t)\\n?\\n?\\n?\\n?\\n?\\n[Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)]\\n?(t) =\\n?\\nif [Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)] > 0\\n?\\n?\\n?\\nRm (t) ? Lm (t)\\n?\\n?\\n?\\n?\\n?\\n?\\n0\\notherwise\\nThus, ?(t) > 0 (?(t) < 0) means that the leftward trials tend to be considerably higher (lower)\\nthan rightward trials for that electrode at time t, while ?(t) = 0 suggests no left/right separation at\\ntime t. We define a consecutive time period of |?(t)| > 0 for t < prediction time (the time before\\nthe go signal when we want the system to output a prediction; -0.5 s for the ORT trials) as a time\\nwindow (Fig. 2e). After all time windows are found for all electrodes, time windows lessRthan M ms\\nt\\napart are combined into one. Then, for each time window from t1 to t2 we define a = t12 |?(t)|dt.\\nWe then eliminate all time windows satisfying a < A. We found the values M = 200 ms and\\nA = 4, 500 ?V ? ms to be optimal for real-time analysis. This resulted in 20?30 time windows over\\nall 64 electrodes that we monitored.\\n5\\n\\n\\f1\\n$4.80\\n\\n$5.20\\n\\nP15CS\\n\\nUri\\n\\nFigure 4: The experimental setup in the clinic. At 400 ms before the go signal, the patient and\\nexperimenter are watching the game screen (inset on bottom right) on the analysis/stimulus computer\\n(bottom left) and still pressing down the buttons of the response box. The realtime system already\\ncomputed a prediction, and thus displays an arrow on the screen behind the patient and plays a tone\\nin the experimenter?s ear ipsilateral to the hand it predicts he should raise to beat the patient (see\\nSupplemental Movie).\\n3.3\\n\\nClassifiers selection and ETC determination\\n\\nWe used ensemble learning with 7 types of relatively simple binary classifiers (due to real-time\\nprocessing considerations) on every electrode?s time windows (Fig. 2f). Classifiers A to G would\\nclassify aN (t) as Lt if:\\nP\\nP\\nP\\n(A) Defining aN,M , Lm,M and Rm,M as aN (t), Lm (t) and Rm (t) over time window M ,\\n\\u0001\\n\\u0001\\n\\u0001\\n(i) sign Rm,M 6= sign aN,M = sign Lm,M , or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(ii) sign Rm,M = sign aN,M = sign Lm,M and \\fLm,M \\f > \\fRm,M \\f, or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(iii) sign Rm (t) 6= sign SN,M 6= sign Lm (t) and \\fLm,M \\f < \\fRm,M \\f;\\n\\f\\n\\u0001\\n\\u0001\\f \\f\\n\\u0001\\n\\u0001\\f\\n(B) \\fmean aN (t) ? mean Lm (t) \\f < \\fmean aN (t) ? mean Rm (t) \\f;\\n\\f\\n\\f\\n\\u0001\\n\\u0001\\f\\n\\u0001\\n\\u0001\\f\\n(C) \\fmedian aN (t) ? median Lm (t) \\f < \\fmedian aN (t) ? median Rm (t) \\f over the time\\nwindow;\\n\\f\\n\\f\\n\\f\\n\\f\\n\\f\\n(D) aN (t) ? Lm (t)\\fL2 < \\faN (t) ? Rm (t)\\fL2 over the time window;\\n(E) aN (t) is convex/concave like Lm (t) while Rm (t) is concave/convex, respectively;\\n(F) Linear support-vector machine (SVM) designates it as so; and\\n(G) k-nearest neighbors (KNN) with Euclidean distance designates it as so.\\nEach classifier is optimized for certain types of features. To estimate how well its classification\\nwould generalize from the training to the test set, we trained and tested it using a 70/30 crossvalidation procedure within the training set. We tested each classifier on every time window of every\\nelectrode, discarding those with accuracy <0.68, which left 12.0 ? 1.6% of the original 232 ? 18\\nETCs, on average (?standard error). The training phase therefore ultimately output a set of S binary\\nETC combinations (Fig. 2g) that were used in the prediction phase (Fig. 3).\\n3.4\\n\\nThe prediction-phase weighting system\\n\\nIn the prediction phase, each of the overall S binary ETCs calculates a prediction, ci ? {?1, 1} (for\\nright and left, respectively), independently at the desired prediction time. All classifiers are initially\\n6\\n\\n\\fPS\\ngiven the same weight, w1 = w2 = ? ? ? = wS = 1. We then calculate ? = i=1 wi ? ci and predict\\nleft (right) if ? > d (? < ?d), or declare it an undetermined trial if ?d < ? < d. Here d is the\\ndrop-off threshold for the prediction. Thus the larger d is, the more confident the system needs to be\\nto make a prediction, and the larger the proportion of trials on which the system abstains?the dropoff rate. Weight wi associated with ETCi is increased (decreased) by 0.1 whenever ETCi predicts\\nthe hand movement correctly (incorrectly). A constantly erring ETC would therefore be associated\\nwith an increasingly small and then increasingly negative weight.\\n3.5\\n\\nImplementation\\n\\nThe algorithm was implemented in MATLAB 2011a (MathWorks, Natick, MA) as well as in C++\\non Visual Studio 2008 (Microsoft, Redmond, WA) for enhanced performance. The neural signals\\nwere collected by the Digital Lynx S system using Cheetah 5.4.0 (Neuralynx, Redmond, WA). The\\nsimulated-ORT system was also implemented in MATLAB 2011a. The simulated-ORT analyses\\ncarried out in this paper used real patient data saved on the Digital Lynx system.\\n1\\n\\n0.9\\n\\nDrop rate:\\nNone\\n0.18\\n0\\u0011\\u0016\\u0013\\n\\nPrediction accuracy\\n\\n0.8\\n\\n0.7\\nSignificant accuracy\\n(p=0.05)\\n0.6\\n\\n0.5\\n\\n?5\\n\\n?4.5\\n\\n?4\\n\\n?3.5\\n\\n?3\\n\\n?2.5\\nTime (s)\\n\\n?2\\n\\n?1.5\\n\\n?1\\n\\n?0.5\\n\\n0\\nGo-signal\\nonset\\n\\nFigure 5: Across-subjects average of the prediction accuracy of simulated-ORT versus time before\\nthe go signal. The mean accuracies over time when the system predicts on every trial, is allowed\\nto drop 19% or 30% of the trials, are depicted in blue, green and red, respectively (?standard error\\nshaded). Values above the dashed horizontal line are significant at p = 0.05.\\n\\n4\\n\\nResults\\n\\nWe tested our prediction system in actual real time on 2 patients?P15CS and P19CS (a depth\\nand grid patient, respectively), with a prediction time of 0.5 s before the go signal (see Supplementary Movie). Because of computational limitations, the ORT system could only track 10\\nelectrodes with just 1 ETC per electrode in real time. For P15CS, we achieved an accuracy of\\n72?2% (?standard error; accuracy = number of accurately predicted trials / [total number of trials - number of dropped trials]; p = 10?8 , binomial test) without modifying the weights online during the prediction (see Section 3.4). For P19CS we did not run patient-specific training of the ORT system, and used parameter values that were good on average over previous patients instead. The prediction accuracy was significantly above chance 63?2% (?standard error; p = 7 ? 10?4 , binomial test). To understand how much we could improve our accuracy\\nwith optimized hardware/software, we ran the simulated-ORT at various prediction times along\\n7\\n\\n\\fAccuracy\\n\\nthe 5 s countdown leading to the go signal. We further tested 3 drop-off rates?0, 0.19 and\\n0.30 (Fig. 5; drop-off rate = number of dropped trials / total number of trials; these resulted\\nfrom 3 drop-off thresholds?0, 0.1 and 0.2?respectively, see Section 3.4:). Running offline,\\nwe were able to track 20?30 ETCs, which resulted in considerably higher accuracies (Figs. 5,6).\\nAveraged over all subjects, the accuracy rose from about 65% more than\\n1\\n4 s before the go signal to 83?92%\\nclose to go-signal onset, depending\\n0.9\\non the allowed drop-off rate. In particular, we found that for a predic0.8\\ntion time of 0.5 s before go-signal\\nonset, we could achieve accuracies\\n0.7\\nof 81?5% and 90?3% (?standard\\nerror) for P15CS and P19CS, re0.6\\nspectively, with no drop off (Fig. 6).\\nPatients:\\nP12CS\\nWe also analyzed the weights that\\nP15CS\\nour weighting system assigned to the\\n0.5\\nP16CS\\nP19CS\\ndifferent ETCs. We found that the\\nP22CS\\nempirical distribution of weights to\\nP29HMH\\n0.4\\nP30HMH\\nETCs associated with classifiers A to\\nG was, on average: 0.15, 0.12, 0.16,\\n?5 ?4.5 ?4 ?3.5 ?3 ?2.5 ?2 ?1.5 ?1 ?0.5 0\\n0.22, 0.01, 0.26 and 0.07, respecTime before go signal (at t=0) (seconds)\\ntively. This suggests that the linear\\nSVM and L2-norm comparisons (of\\naN to Lm and Rm ) together make up Figure 6: Simulated-ORT accuracy over time for individual\\nnearly half of the overall weights at- patients with no drop off.\\ntributed to the classifiers, while the\\ncurrent concave/convex measure is of\\nlittle use as a classifier.\\n\\n5\\n\\nDiscussion\\n\\nWe constructed an ORT system that, based on intracranial recordings, predicted which hand a person would raise well before movement onset at accuracies much greater than chance in a competitive environment. We further tested this system off-line, which suggested that with optimized\\nhardware/software, such action contents would be predictable in real time at relatively high accuracies already several seconds before movement onset. Both our prediction accuracy and drop-off\\nrates close to movement onset are superior to those achieved before movement onset with noninvasive methods like EEG and fMRI [7, 12?14]. Importantly, our subjects played a matching pennies game?a 2-choice version of rock-paper-scissors [15]?to keep their task realistic, with minor\\nthough real consequences, unlike the Libet-type paradigms whose outcome bears no consequences\\nfor the subjects. It was suggested that accurate online, real-time prediction before movement onset\\nis key to investigating the relation between the neural correlates of decisions, their awareness, and\\nvoluntary action [16, 17]. Such prediction capabilities would facilitate many types of experiments\\nthat are currently infeasible. For example, it would make it possible to study decision reversals on\\na single-trial basis, or to test whether subjects can guess above chance which of their action contents are predictable from their current brain activity, potentially before having consciously made up\\ntheir mind [16, 18]. Accurately decoding these preparatory motor signals may also result in earlier\\nand improved classification for brain-computer interfaces [13, 19, 20]. The work we present here\\nsuggests that such ORT analysis might well be possible.\\nAcknowledgements\\nWe thank Ueli Rutishauser, Regan Blythe Towel, Liad Mudrik and Ralph Adolphs for meaningful\\ndiscussions. This research was supported by the Ralph Schlaeger Charitable Foundation, Florida\\nState University?s ?Big Questions in Free Will? initiative and the G. Harold & Leila Y. Mathers\\nCharitable Foundation.\\n8\\n\\n\\fReferences\\n[1] B. Libet, C. Gleason, E. Wright, and D. Pearl. Time of conscious intention to act in relation to\\nonset of cerebral activity (readiness-potential): The unconscious initiation of a freely voluntary\\nact. Brain, 106:623, 1983.\\n[2] B. Libet. Unconscious cerebral initiative and the role of conscious will in voluntary action.\\nBehavioral and brain sciences, 8:529?539, 1985.\\n[3] P. Haggard and M. Eimer. On the relation between brain potentials and the awareness of\\nvoluntary movements. Experimental Brain Research, 126:128?133, 1999.\\n[4] A. Sirigu, E. Daprati, S. Ciancia, P. Giraux, N. Nighoghossian, A. Posada, and P. Haggard.\\nAltered awareness of voluntary action after damage to the parietal cortex. Nature Neuroscience,\\n7:80?84, 2003.\\n[5] H. Kornhuber and L. Deecke. Hirnpotenti?alanderungen bei Willk?urbewegungen und passiven\\nBewegungen des Menschen: Bereitschaftspotential und reafferente Potentiale. Pfl?ugers Archiv\\nEuropean Journal of Physiology, 284:1?17, 1965.\\n[6] H. Shibasaki and M. Hallett. What is the Bereitschaftspotential? Clinical Neurophysiology,\\n117:2341?2356, 2006.\\n[7] C. Soon, M. Brass, H. Heinze, and J. Haynes. Unconscious determinants of free decisions in\\nthe human brain. Nature Neuroscience, 11:543?545, 2008.\\n[8] I. Fried, R. Mukamel, and G. Kreiman. Internally generated preactivation of single neurons in\\nhuman medial frontal cortex predicts volition. Neuron, 69:548?562, 2011.\\n[9] M. Cerf, N. Thiruvengadam, F. Mormann, A. Kraskov, R. Quian Quiorga, C. Koch, and\\nI. Fried. On-line, voluntary control of human temporal lobe neurons. Nature, 467:1104?1108,\\n2010.\\n[10] T. Ball, M. Kern, I. Mutschler, A. Aertsen, and A. Schulze-Bonhage. Signal quality of simultaneously recorded invasive and non-invasive EEG. Neuroimage, 46:708?716, 2009.\\n[11] G. Schalk, J. Kubanek, K. Miller, N. Anderson, E. Leuthardt, J. Ojemann, D. Limbrick,\\nD. Moran, L. Gerhardt, and J. Wolpaw. Decoding two-dimensional movement trajectories\\nusing electrocorticographic signals in humans. Journal of Neural engineering, 4:264, 2007.\\n[12] O. Bai, V. Rathi, P. Lin, D. Huang, H. Battapady, D. Y. Fei, L. Schneider, E. Houdayer, X. Chen,\\nand M. Hallett. Prediction of human voluntary movement before it occurs. Clinical Neurophysiology, 122:364?372, 2011.\\n[13] O. Bai, P. Lin, S. Vorbach, J. Li, S. Furlani, and M. Hallett. Exploration of computational\\nmethods for classification of movement intention during human voluntary movement from\\nsingle trial EEG. Clinical Neurophysiology, 118:2637?2655, 2007.\\n[14] U. Maoz, A. Arieli, S. Ullman, and C. Koch. Using single-trial EEG data to predict laterality\\nof voluntary motor decisions. Society for Neuroscience, 38:289.6, 2008.\\n[15] C. Camerer. Behavioral game theory: Experiments in strategic interaction. Princeton University Press, 2003.\\n[16] J. D. Haynes. Decoding and predicting intentions. Annals of the New York Academy of Sciences, 1224:9?21, 2011.\\n[17] P. Haggard. Decision time for free will. Neuron, 69:404?406, 2011.\\n[18] J. D. Haynes. Beyond libet. In W. Sinnott-Armstrong and L. Nadel, editors, Conscious will\\nand responsibility, pages 85?96. Oxford University Press, 2011.\\n[19] A. Muralidharan, J. Chae, and D. M. Taylor. Extracting attempted hand movements from EEGs\\nin people with complete hand paralysis following stroke. Frontiers in neuroscience, 5, 2011.\\n[20] E. Lew, R. Chavarriaga, S. Silvoni, and J. R. Milln. Detection of self-paced reaching movement\\nintention from EEG signals. Frontiers in Neuroengineering, 5:13, 2012.\\n\\n9\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "papers"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-dcac0149-0836-4999-88ae-9d6dc8810862\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dcac0149-0836-4999-88ae-9d6dc8810862')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dcac0149-0836-4999-88ae-9d6dc8810862 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dcac0149-0836-4999-88ae-9d6dc8810862');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ac1fab12-bbcb-4a7f-bc20-1105d7c566ef\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ac1fab12-bbcb-4a7f-bc20-1105d7c566ef')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ac1fab12-bbcb-4a7f-bc20-1105d7c566ef button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(\"/NIPS Papers.zip\", \"r\") as zip_ref:\n",
        "    # Extract the file to a temporary directory\n",
        "    zip_ref.extractall(\"temp\")\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "papers = pd.read_csv(\"temp/NIPS Papers/papers.csv\")\n",
        "\n",
        "# Print head\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnfBbMw958MJ"
      },
      "source": [
        "** **\n",
        "#### Step 2: Data Cleaning\n",
        "** **\n",
        "\n",
        "Since the goal of this analysis is to perform topic modeling, we will solely focus on the text data from each paper, and drop other metadata columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IelGIQ8v58MJ",
        "outputId": "b3087214-4982-45b7-da87-6b1890fd02cf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Catastrophic Interference in Human\\nMotor Learning\\n\\nTom Brashers-Krug, Reza Shadmehr t , and Emanuel Todorov\\nDept. of Brain and Cognitive Sciences, M. I. T., Cambridge, MA 02139\\ntCurrently at Dept. of Biomedical Eng., Johns Hopkins Univ., Baltimore, MD 21205\\nEmail: tbk@ai.mit.edu, reza@bme.jhu.edu, emo@aLmit.edu\\n\\nAbstract\\nBiological sensorimotor systems are not static maps that transform\\ninput (sensory information) into output (motor behavior). Evidence from many lines of research suggests that their representations are plastic, experience-dependent entities. While this plasticity is essential for flexible behavior, it presents the nervous system\\nwith difficult organizational challenges. If the sensorimotor system\\nadapts itself to perform well under one set of circumstances, will it\\nthen perform poorly when placed in an environment with different\\ndemands (negative transfer)? Will a later experience-dependent\\nchange undo the benefits of previous learning (catastrophic interference)? We explore the first question in a separate paper in this\\nvolume (Shadmehr et al. 1995). Here we present psychophysical\\nand computational results that explore the question of catastrophic\\ninterference in the context of a dynamic motor learning task. Under some conditions, subjects show evidence of catastrophic interference. Under other conditions, however, subjects appear to be\\nimmune to its effects. These results suggest that motor learning\\ncan undergo a process of consolidation. Modular neural networks\\nare well suited for the demands of learning multiple input/output\\nmappings. By incorporating the notion of fast- and slow-changing\\nconnections into a modular architecture, we were able to account\\nfor the psychophysical results.\\n\\n\\f20\\n\\n1\\n\\nTom Brashers-Krug, Reza Shadmelzr, Emanuel Todorov\\n\\nIntroduction\\n\\nInteracting physically with the world changes the dynamics of one's limbs. For\\nexample, when holding a heavy load, a different pattern of muscular activity is\\nneeded to move one's arm along a particular path than when not holding a load.\\nPrevious work in our laboratory has shown that humans learn a novel dynamic\\ntask by forming an internal model of the new inverse dynamics of thier limbs.\\n(Shadmehr and Mussa-Ivaldi 1994, Shadmehr et aI, 1995). Preliminary evidence\\nsuggests that subjects can retain one of these internal models over time (BrashersKrug, et al. 1994). Humans are required, however, to move their limbs effectively\\nunder a large number of dynamic conditions. Are people able to learn and store\\nan inverse dynamic model appropriate for each condition, or do they form such\\nmodels from scratch as they need them? In particular, can learning a new inverse\\ndynamic model overwrite or displace a previous model? We will present evidence\\nthat certain conditions must be met before a subject is able to retain more than\\none inverse dynamic model in a given experimental context. These conditions can\\nbe modeled as leading to a process of consolidation, whereby learning is transfered\\nfrom vulnerable, low-capacity storage to a long-term, high-capacity storage.\\n\\n2\\n\\nExperimental Protocol\\n\\nWe have developed a motor learning paradigm that allows us to alter the dynamics\\nof a subject's arm and so to monitor a subject's ability to learn dynamic tasks.\\nA subject moves the handle on the free end of a two-link planar robot arm-called\\na manipulandum-to guide a cursor to a series of targets displayed on a computer\\nscreen (fig la). The position and velocity of the handle of the manipulandum are\\nrecorded at ten-millisecond intervals and are used to deliver state-dependent forces\\nto the subject's hand. In order to test a subject's ability to learn a novel dynamic\\ntask, we present the subject with a viscous force field as slhe moves from one\\ntarget to the next (fig Ib). Initially, such forces perturb the subject's movements,\\ncausing them to depart from the smooth, straight-line trajectories of the baseline\\ncondition (i .e., the condition before the viscous forces were presented) (figs lc,ld).\\nThe extent of learning is measured as the degree to which a subject's movements\\nin the force field over time come to resemble that subject's baseline movements.\\nWe have shown in previous work that subjects adapt to the imposed force fileds by\\nforming a predictive model of the velocity-dependent forces, and that subjects use\\nthis inverse dynamic model to control their arms in what appears to be a feedforward\\nmanner (Shadmehr and Mussa-Ivaldi 1994).\\n\\n3\\n3.1\\n\\nPsychophysical Findings\\nCatastrophic Interference\\n\\nHere, we employed this paradigm to explore the consequences of learning two different dynamic tasks in a row. In an initial series of experiments, we allowed twelve\\nsubjects to learn to move the manipulandum in a first force field (Field A) for approximately 5 minutes. Immediately after this first set of movements, we presented\\nthe subjects with an anti-correlated force field (Field B). For example, if we pre-\\n\\n\\fCatastrophic Interference in Human Motor Learning\\n\\n<Y'/'/ .\\n\\nC\\n\\n21\\n\\n\\\\\\n\\nD\\n\\nFigure 1: A: The experimental setup. B: An example of a viscous field, plotted in velocity\\nspace (mm/sec). The arrows indicate the direction and magnitude of the forces exerted\\nby the manipulandum on the subject's hand at each location in velocity space. C: One\\nsubject's trajectories before forces were introduced. Targets are indicated by the open\\ncircles. D: Trajectories immediately after the force field in (B) was presented.\\nsen ted the counter-clockwise curl field depicted in fig. 1B as Field A, we would next\\npresent a clockwise curl field as Field B. Half the subjects learned the clockwise curl\\nfield first and the counter-clockwise field second; the other halflearned the two fields\\nin the reverse order. (The first field will be referred to as Field A and the second\\nfield as Field B, whichever field was learned first.) The subjects' mean performance\\nin Field B was worse (p< 0.0001, paired t-test) than in Field A. This phenomenon\\nhas been called negative transfer in the psychophysical literature. Negative transfer\\nin this motor learning paradigm is explored more fully in a separate paper in this\\nvolume (Shadmehr et aI, 1995). In that paper, we suggested that this negative\\ntransfer could result from the fact that the same neural elements are learning both\\ntasks. We predicted that, if this is the case, learning to move in Field B would\\ninterfere with a subject's ability to retain an inverse dynamic model of Field A.\\nLearning to move in Field B would, in effect, cause subjects to \\\"unlearn\\\" Field A,\\nresulting in catastrophic interference.\\nIn order to test this prediction, we compared the improvement in performance from\\none day to the next of two groups of subjects, with twelve subjects in each group.\\nThe subjects in the control group learned to move in one force field on Day One\\nand were then tested on Day Two in the same force field. The subjects in the\\nexperimental group learned two separate force fields in a row on Day One and were\\nthen tested on Day Two in the first force field they learned. The experimental\\ngroup retained significantly less of their learning (p< 0.01, paired t-test) from Day\\nOne to Day Two than the control group (figs 2a,2b) . In other words, learning the\\nsecond force field resulted in catastrophic interference. (The question of whether\\nthis represents a storage or a retrieval phenomenon is beyond the scope of this\\npaper.)\\n\\n\\f22\\n\\n3.2\\n\\nTom Brashers-Krug, Reza Shadmehr, Emanuel Todorov\\n\\nConsolidation\\n\\nHaving found evidence for catastrophic interference, we wanted to know whether\\nthere were circumstances under which dynamic motor learning was immune to being\\nfunctionally erased by subsequent learning. We therefore tested two further groups\\nof subjects. We allowed these subjects to practice longer in one field before they\\nlearned the second field. We also allowed 24 hours to pass between the time subjects\\nfirst learned one field and when they learned the second field . The subjects in\\nthe experimental group (n = 10) practiced in one force field for approximately 15\\nminutes on Day One. They returned on Day Two and practiced in the same force\\nfield for five more minutes. They were then allowed to practice in a second force\\nfield for 15 minutes. By the end of this fifteen minutes, they were performing in the\\nsecond field at a level comparable to the level they acheived in the first force field.\\nWe had the subjects return on Day Three, when they were tested for their retention\\nof the first field they learned. We compared the difference in performance on Day\\nTwo and Day Three of this experimental group with that of a control group (n =\\n9) who followed the same protocol in all respects except that they never learned a\\nsecond force field . In this way we could determine whether learning the second field\\nresulted in a decrement in performance for the experimental group when compared\\nwith the control group.\\nUnder these conditions, we found no difference in the retention of learning between\\nthe experimental and control groups (fig 2c, 2d) . That is, learning the second\\nfield under these conditions no longer resulted in catastrophic interference. What\\nsubjects had learned about the first field had become resistant to such interference.\\nIt had become consolidated.\\nWe can not tell from these experiments whether such consolidation is the result of\\nthe increased practice in the first field, or whether it is the result of the 24 hours\\nthat elapsed between when the first field was first learned and when the second field\\nwas learned. There is evidence that increased practice in a motor task can engage\\ndifferent neural circuits than those that are active during initial learning (Jenkins,\\net al 1994). The shift to \\\"practiced\\\" circuits may represent the neural substrate of\\nconsolidation. There is also evidence that time can be an important factor in the\\nconsolidation of skill learning. (Karni and Sagi 1993) In the next section, we present\\na model of our results that assumes that time is the key variable in the consolidation\\nof motor learning. The model could also be applied to a practice-based model of\\nconsolidation with minor modifications.\\n\\n4\\n\\nComputational Modeling of the Experimental Results\\n\\nIn order to model the results presented above we need a network that learns to\\ncompute an appropriate control signal Y given the current state and the desired\\nnext state X of the plant. More precisely, it needs to compute a mapping from joint\\nangles 0, joint velocities iJ, and desired joint accelerations jj to torques. Several approaches for solving this problem have been proposed. One way to learn a mapping\\nfrom X into Y is to use direct inverse modeling: apply a control signal, measure the\\nnext state of the plant, and use the current state, new state, and control signal as\\na training pair for the controller. This approach is not suitable for explaining non-\\n\\n\\f23\\n\\nCatastrophic Interference ill Human Motor Learning\\n\\n0.95\\n\\n0.95\\n0.85\\n0.75\\n\\n~\\n\\n\\\",\\\"\\n\\n0.75\\n\\nA\\n\\n0.65\\n0\\n\\n50\\n\\n0.85\\n\\n100\\n\\n0\\n\\n150\\n0.95\\n\\n0.85\\n\\n0.85\\n\\n0.75\\n0\\n\\n50\\n\\n100\\n\\n150\\n\\nB\\n\\n0.65\\n\\n0.95\\n\\nC\\n\\n~\\n50\\n\\n100\\n\\n150\\n\\n~\\nD\\n\\n0.75\\n0\\n\\n50\\n\\n100\\n\\n150\\n\\nFigure 2: Plots of average learning curves. The correlation of trajectories in the force\\nfield to baseline trajectories (before the force field was applied) is plotted as a function\\nof movement number. A: Learning curves for the first experimental group. Dark curve:\\nlearning curve on Day One in Field A . Light curve: learning curve in Field A on Day Two.\\nThis group learned Field B immediately after Field A on Day One (learning curve for\\nField B not shown). Note minimal improvement from Day One to Day Two. B: Learning\\ncurves for the first control group. Dark curve: learning curve in Field A on Day One.\\nLight curve: learning curve in Field A on Day Two. Control group never learned Field\\nB. Note significant improvement from Day One to Day Two. C: Second experimental\\ngroup. Dark line: learning curve in Field A on Day Two immediately before learning\\nField B (Field B curve not shown). Light line: learning curve in Field A on Day Three.\\nD: Second control group. Dark and light lines: learning curves in FIeld A on Day Two\\nand Three, respectively. Note the similarity of the curves in C and D. This indicates that\\nlearning Field B did not significantly affect the experimental group's retention of Field A.\\nAll curves in C and D are significantly higher than curves for the initial learning of Field\\nA on Day One.\\nconvex mappings, however. The learning situation we must model is non-convex:\\nwe change the dynamic environment the controller operates in by presenting force\\nfileds, and so there will be different Y values corresponding to any X value. A different approach that solves the non convexity problem is distal supervised learning\\n(Jordan and Rumelhart 1992): produce control signals, observe the new state of\\nthe plant, and use that information to train a forward model that maps actions into\\nstates; then learn a controller, using the forward model to backpropagate error from\\nobservable cartesian coordinates to the unknown control space. Distal supervised\\nlearning solves the non convexity problem by learning one correct value of Y for each\\nX. But that can not explain the consolidation of learning - when the force field\\nchanges back to something already learned, our controller should rapidly recover its\\nperformance in that old field, meaning that it should retain information about all\\nY s that a particular X can map into.\\nAn architecture that seems to have most of the desirable properties discussed above\\nis the Mixture of Experts (ME) model (Jordan and Jacobs 1994): several experts\\nlearn a mapping from X to Y . A separate gating network selects an expert which is\\nmost likely to be correct at each moment. Such a model has been used previously\\n(Jacobs and Jordan 1993) to learn a feedforward controller for a 2-joint planar arm\\noperating under different loads. In their model however they assumed that the\\n\\n\\f24\\n\\nTom Brashers-Krug, Reza Shadmelzr, Emanuel Todorov\\n\\nidentity of the load is known to the controller. The subjects in our study were\\nnot given any explicit cues about the identity of the fields they were learning. The\\nmixture of experts cannot be used directly here because it decides which expert\\nto select based on a soft partitioning of the input space, and in our experiment\\nany force field is active over any portion of the input space at different moments\\nin time. Here we propose an extension to the ME architecture that is able to deal\\nwith mappings overlapping in X space.\\nAnother aspect of the results that is difficult to model using standard computational\\narchitectures is memory consolidation. To account for this effect we introduce two\\ndifferent learning rates (Alverez and Squire 1994). Some connections in the network\\nchange faster, as a result of which they can serve as short-term memory. We also\\nintroduce an off-line training phase (possibly corresponding to sleep) in which random inputs are generated, the part of the network containing the fast connections\\nis used to produce a target output, and the resulting input-output pair is used to\\ntrain the slowly changing connections. During the offline phase the faster changing\\nconnections are fixed, after that they are randomized.\\n\\n4.1\\n\\nModified Mixture of Experts\\n\\nThe ME model assumes that Y is generated from X by one of N processes\\n(Wl, ... , WN) and therefore the likelihood function is:\\n\\nL(8IXt, Yt) = P(Ytlxt, 8) =\\n\\nI: gi t P(YtIWi, Xt, 8)\\ni\\n\\ng/ = P(WiIXt, 8),\\nwhere 8 represents the parameters of the model and gi is the prior probability. We\\nwant to use the posterior probability P(Wi IXt, Yt) instead, because the processes\\n(different force fields) are separable in XY space, but not in X space. If we want to\\nimplement an on-line controller such a term is not available, because at time t, Yt\\nis still unknown (the task of the controller is to produce it). We could approximate\\nP(WiIXt, Yt) with P(WdXt-l. Yt-!) , because dynamic conditions do not change very\\noften. Now the gating network (which computes P(Wdxt) in ME) is going to select\\nexpert i based on the previous XY pair. This approach would obviously lead to a\\nsingle large error at the moment when the force field changes, but so will any model\\nusing only Xt to compute Yt. In fact such an error seems to be consistent with our\\npsychophysics data. Thus the learning rule is:\\n~8i = lIih/(Yt - J.'i(Xt, 8i))\\n\\ngI.t -_ h.t-1\\nI\\nh .t\\nI\\n\\nh:\\n\\n= P(W~I\\nI\\n\\n~ ~ 8) =\\nXII\\n\\ny\\\"\\n\\ngi t P(YtIWi , Xt, 8)\\n\\n'\\\"\\nL..tj g,.tp(Yt IW.\\n\\\"Xt, 8)'\\n\\nwhere\\nis the posterior probability and J.'i is the output of exper i. J.'i is a linear\\nfunction of the inputs. In our model we used 4 experts. In order to model the process\\nof consolidation, we gave one expert a learning rate that was 10 times higher than\\nthe learning rate of the other 3 experts.\\n\\n\\fCatastrophic Interference in Human Motor Learning\\n\\n25\\n\\n4.2 The Model\\nWe simulated the dynamics of a 2-joint planar arm similar to the one used in our\\nprevious work (Shadmehr and Mussa-Ivaldi, 1994). The torque applied to the arm\\nat every point in time is the sum of the outputs of a fixed controller, a PO controller,\\nand an adaptive controller with the architecture described above.\\n\\nThe fixed controller maps (), iJ (current state), and jj (desired next state) into a\\ntorque T. The mapping is exact when no external forces are applied. The desired trajectories are minimum-jerk trajectories (Flash and Hogan 1985) sampled\\nat 100Hz. The desired trajectories are 10 cm long and last 0.5 seconds. The PO\\ncontroller is used to compensate for the troques produced by the force field while\\nthe adaptive controller has not yet learned to do that. The adaptive part of the\\ncontroller consists of a mixture of 4 linear experts (whose initial output is small)\\nand a modified gating network described above. The system operates as follows:\\n(), iJ, jj are sent to the fixed controller, which outputs a torque Tl; the PO controller\\noutputs a torque T2 based on the current deviation from the desired joint position\\nand velocity; 8 terms describing the current state of the arm (and chosen to linearize\\nthe mapping to be learned) are sent to the mixture model, which outputs a torque\\nT3; Te = Tl + T2 - T3 is applied to the plant as a control signal; the actual torque\\nT = Te + TJ is computed. The mixture model is trained to produce the torque TJ\\nresulting from the force field. In other words, the adaptive part of the controller\\nlearns to compensate for the force field exerted by the environment.\\nThe parameters of the mixture model are updated after every movement, so a\\ntraining pair (Xt, Yt) is actually a batch of 50 points. The input, Xt, consists of\\nthe 8 terms describing the current state and the desired next state; the output, Yt,\\nis the torque vector that the force field produces.The compensatory torques for a\\ncomplete movement are computed before the movement starts. The only processing\\ndone during the movement is the computations necessary for the PO controller.\\n4.3 Results\\n4.3.1 Negative Transfer\\nWhen the network was given two successive, incompatable mappings to learn (this\\ncorresponds to learning to move in two opposite force fields), the resulting performance very much resembled that of our human subjects. The performance in the\\nsecond mapping was much poorer than that in the first mapping. The fast-learning\\nexpert changed its weights to learn both tasks. Since the two tasks involved anticorrelated maps, the fast expert's weights after learning the first mapping were very\\ninappropriate for the second task, leading to the observed negative transfer.\\n4.3.2 Catastrophic Interference\\nWhen the network was trained on two successive, opposite force fields, with no\\nconsolidation occurring between the two training sessions, the learning in second\\ntraining session overwrote the learning that occurred during the first training session\\n(fig 3A). Since the expert with the fast-changing weights attempted to learn both\\nmappings, this catastrophic interference is not unexpected.\\n4.3.3 Consolidation\\nWhen the network was allowed to undergo consolidation between learning the first\\nand the second force field, the network no longer suffered from catastrophic inter-\\n\\n\\f26\\n\\nTom Braslzers-Krug, Reza Slzadmelzr, Emanuel Todorov\\n\\nA\\n--:=\\\"\\n\\nD.65~_ _ _ _~_ _ _\\n\\no\\n\\n10\\n\\n20\\n\\nFigure 3: A: Learning curves for the ME architecture. Dark line: curve when first learning\\nField A. light line: curve when given Field A a second time, after learning FIeld B (no\\nconsolidation allowed between learning Field A and Field B). Note lack of retention of\\nField A. B: Learning curves for the same architecture in Field A before and after learning\\nField B. Consolidations was allowed between learning Field A and Field B.\\nference (fig 3B). The learning that had initially resided in the fast-learning expert\\nwas transfered to one of the slower-learning networks. Thus, when the expert with\\nthe fast-changing connections learned the second mapping, the original learning was\\nno longer destroyed. In addition, when the network was allowed to consolidate the\\nsecond force field, a different slow-learning expert stored the second mapping. In\\nthis way, the network stored multiple maps in long-term memory.\\n\\n5\\n\\nConclusions\\n\\nWe have presented psychophysical evidence for catastrophic interference. We have\\nalso shown results that suggest that motor learning can undergo a process of consolidation. By adding distinct fast- and slow-changing weights to a mixture of experts\\narchitecture, we were able to account for these psychophysical findings. We plan to\\ninvestigate further the neural correlates of consolidation using both brain imaging\\nin humans and electrophysiological studies in primates.\\nReferences\\nP. Alverez and L. Squire. (1994) Memory consolidation and the medial temporal love: a\\nsimple network model, PNAS91:15:7041-7045. T. Brashers-Krug, et al. (1994) Temporal\\naspects of human motor learning, Soc. Neurosci. Abstract in press.\\nT. Flash and N. Hogan. (1985) The coordination of arm movements: an experimentally\\nconfirmed mathematical model. J. Neurosci. 5:1688-1703.\\nR. Jacobs and M. Jordan. (1993) Learning piecewise control strategies in a modular neural\\nnetwork architecture. IEEE Trans. on Systems, Man and Cyber. 23:2: 337-345.\\nI. Jenkins, et al. (1994) Motor sequence learning: a study with positron emission tomography, J. Neurosci.14:3775-3790.\\nM. Jordan and R. Jacobs. (1994) Hierarchical mixture of experts and the EM algorithm,\\nNeural Computation 6:2: 181-214.\\nM. Jordan and D. Rumelhart. (1992) Forward models: supervised learning with a distal\\nteacher Cognitive Sci. 16:307-354\\nA. Karni and D. Sagi. (1993) Nature 365:250.\\nR. Shadmehr and F . Mussa-Ivaldi. (1994) Adaptive representation of dynamics during\\nlearning of a motor task, J. Neurosci.14:5:3208-3224.\\nR. Shadmehr, T. Brashers-Krug, F. Mussa-Ivaldi, (1995) Interference in learning internal\\nmodels of inverse dynamics in humans, Adv Neural Inform Proc Systvol 7 , in press\\n\\n\\f\",\n          \"Optimal learning rates for least squares SVMs using\\nGaussian kernels\\nM. Eberts, I. Steinwart\\nInstitute for Stochastics and Applications\\nUniversity of Stuttgart\\nD-70569 Stuttgart\\n{eberts,ingo.steinwart}@mathematik.uni-stuttgart.de\\n\\nAbstract\\nWe prove a new oracle inequality for support vector machines with Gaussian RBF\\nkernels solving the regularized least squares regression problem. To this end, we\\napply the modulus of smoothness. With the help of the new oracle inequality we\\nthen derive learning rates that can also be achieved by a simple data-dependent\\nparameter selection method. Finally, it turns out that our learning rates are asymptotically optimal for regression functions satisfying certain standard smoothness\\nconditions.\\n\\n1\\n\\nIntroduction\\n\\nOn the basis of i.i.d. observations D := ((x1 , y1 ) , . . . , (xn , yn )) of input/output observations drawn\\nfrom an unknown distribution P on X ? Y , where Y ? R, the goal of non-parametric least squares\\nregression is to find a function fD : X ! R such that, for the least squares loss L : Y ? R ! [0, 1)\\n2\\ndefined by L (y, t) = (y t) , the risk\\nZ\\nZ\\n2\\nRL,P (fD ) :=\\nL (y, fD (x)) dP (x, y) =\\n(y fD (x)) dP (x, y)\\nX?Y\\n\\nX?Y\\n\\nis small. This means RL,P (fD ) has to be close to the optimal risk\\n\\nR?L,P := inf {RL,P (f ) | f : X ! R measureable} ,\\n\\n?\\ncalled the Bayes risk with respect to P and L. It is well known that the function fL,P\\n: X ! R\\n?\\ndefined by fL,P (x) = EP (Y |x), x 2 X, is the only function for which the Bayes risk is attained.\\nFurthermore, some simple transformations show\\nZ\\n2\\n2\\n?\\n?\\n?\\nRL,P (f ) RL,P =\\nf fL,P\\ndPX = f fL,P\\n,\\n(1)\\nL (P )\\n2\\n\\nX\\n\\nX\\n\\nwhere PX is the marginal distribution of P on X.\\n\\nIn this paper, we assume that X ? Rd is a non-empty, open and bounded set such that its boundary\\n@X has Lebesgue measure 0, Y := [ M, M ] for some M > 0 and P is a probability measure on\\nX ?Y such that PX is the uniform distribution on X. In Section 2 we also discuss that this condition\\ncan easily be generalized by assuming that PX on X is absolutely continuous with respect to the\\nLebesgue measure on X such that the corresponding density of PX is bounded away from 0 and 1.\\nRecall that because of the first assumption, it suffices to restrict considerations to decision functions\\nf : X ! [ M, M ]. To be more precise, if, we denote the clipped value of some t 2 R by ?\\nt, that is\\n8\\n< M if t < M\\n?\\nt := t\\nif t 2 [ M, M ]\\n:\\nM\\nif t > M ,\\n1\\n\\n\\fthen it is easy to check that\\nRL,P (f?) ? RL,P (f ) ,\\n\\nfor all f : X ! R.\\n\\nThe non-parametric least squares problem can be solved in many ways. Several of them are e.g. described in [1]. In this paper, we use SVMs to find a solution for the non-parametric least squares\\nproblem by solving the regularized problem\\n2\\n\\nfD, = arg min kf kH + RL,D (f ) .\\n\\n(2)\\n\\nf 2H\\n\\nHere, > 0 is a fixed real number, H is a reproducing kernel Hilbert space (RKHS) over X, and\\nRL,D (f ) is the empirical risk of f , that is\\nn\\n\\nRL,D (f ) =\\n\\n1X\\nL (yi , f (xi )) .\\nn i=1\\n\\nIn this work we restrict our considerations to Gaussian RBF kernels k on X, which are defined by\\n!\\n2\\nkx x0 k2\\n0\\nk (x, x ) = exp\\n,\\nx, x0 2 X ,\\n2\\nfor some width 2 (0, 1]. Our goal is to deduce asymptotically optimal learning rates for the SVMs\\n(2) using the RKHS H of k . To this end, we first establish a general oracle inequality. Based on\\nthis oracle inequality, we then derive learning rates if the regression function is contained in some\\nBesov space. It will turn out, that these learning rates are asymptotically optimal. Finally, we show\\nthat these rates can be achieved by a simple data-dependent parameter selection method based on a\\nhold-out set.\\nThe rest of this paper is organized as follows: The next section presents the main theorems and as a\\nconsequence of these theorems some corollaries inducing asymptotically optimal learning rates for\\nregression functions contained in Sobolev or Besov spaces. Section 3 states some, for the proof of\\nthe main statement necessary, lemmata and a version of [2, Theorem 7.23] applied to our special\\ncase as well as the proof of the main theorem. Some further proofs and additional technical results\\ncan be found in the appendix.\\n\\n2\\n\\nResults\\n\\nIn this section we present our main results including the optimal rates for LS-SVMs using Gaussian\\nkernels. To this end, we first need to introduce some function spaces, which are later assumed to\\ncontain the regression function.\\nLet us begin by recalling from, e.g. [3, p. 44], [4, p. 398], and [5, p. 360], the modulus of smoothness:\\nDefinition 1. Let ? ? Rd with non-empty interior, ? be an arbitrary measure on ?, and f : ? ! Rd\\nbe a function with f 2 Lp (?) for some p 2 (0, 1). For r 2 N, the r-th modulus of smoothness of\\nf is defined by\\n!r,Lp (?) (f, t) = sup k4rh (f, ? )kLp (?) ,\\n\\nt\\n\\n0,\\n\\nkhk2 ?t\\n\\nwhere k ? k2 denotes the Euclidean norm and the r-th difference 4rh (f, ?) is defined by\\n(P\\nr\\nr j\\nr\\nf (x + jh) if x 2 ?r,h\\nj=0 j ( 1)\\n4rh (f, x) =\\n0\\nif x 2\\n/ ?r,h\\nfor h = (h1 , . . . , hd ) 2 Rd with hi\\n\\n0 and ?r,h := {x 2 ? : x + sh 2 ? 8 s 2 [0, r]}.\\n\\nIt is well-known that the modulus of smoothness with respect to Lp (?) is a nondecreasing function\\nof t and for the Lebesgue measure on ? it satisfies\\n?\\n?r\\nt\\n!r,Lp (?) (f, t) ? 1 +\\n!r,Lp (?) (f, s) ,\\n(3)\\ns\\n2\\n\\n\\ffor all f 2 Lp (?) and all s > 0, see e.g. [6, (2.1)]. Moreover, the modulus of smoothness can be\\nused to define the scale of Besov spaces. Namely, for 1 ? p, q ? 1, ? > 0, r := b?c + 1, and an\\n?\\narbitrary measure ?, the Besov space Bp,q\\n(?) is\\nn\\no\\n?\\nBp,q\\n(?) := f 2 Lp (?) : |f |B ? (?) < 1 ,\\np,q\\n\\nwhere, for 1 ? q < 1, the seminorm |? |Bp,q\\n? (?) is defined by\\n|f |B ?\\n\\np,q (?)\\n\\n:=\\n\\nand, for q = 1, it is defined by\\n|f |B ?\\n\\n?Z\\n\\n1\\n\\nt\\n\\n?\\n\\n0\\n\\np,1 (?)\\n\\n!r,Lp (?) (f, t)\\n\\n:= sup t\\nt>0\\n\\n?\\n\\nq\\n\\ndt\\nt\\n\\n? q1\\n\\n,\\n\\n!r,Lp (?) (f, t) .\\n\\n?\\nIn both cases the norm of Bp,q\\n(?) can be defined by kf kBp,q\\n? (?) := kf kL (?) + |f |B ? (?) , see\\np\\np,q\\n?\\ne.g. [3, pp. 54/55] and [4, p. 398]. Finally, for q = 1, we often write Bp,1\\n(?) = Lip? (?, Lp (?))\\nand call Lip? (?, Lp (?)) the generalized Lipschitz space of order ?. In addition, it is well-known,\\nsee e.g. [7, p. 25 and p. 44], that the Sobolev spaces Wp? (Rd ) fall into the scale of Besov spaces,\\nnamely\\n?\\nWp? (Rd ) ? Bp,q\\n(Rd )\\n\\n(4)\\n\\n?\\nfor ? 2 N, p 2 (1, 1), and max{p, 2} ? q ? 1 and especially W2? (Rd ) = B2,2\\n(Rd ).\\n\\nFor our results we need to extend functions f : ? ! R to functions f? : Rd ! R such that the\\nsmoothness properties of f described by some Sobolev or Besov space are preserved by f?. Recall\\nthat Stein?s Extension Theorem guarantees the existence of such an extension, whenever ? is a\\nbounded Lipschitz domain. To be more precise, in this case there exists a linear operator E mapping\\nfunctions f : ? ! R to functions Ef : Rd ! R with the properties:\\n(a) E (f )|? = f , that is, E is an extension operator.\\n\\n(b) E continuously maps Wpm (?) into Wpm Rd for all p 2 [1, 1] and all integer m\\nThat is, there exist constants am,p 0, such that, for every f 2 Wpm (?), we have\\nkEf kWpm (Rd ) ? am,p kf kWpm (?) .\\n\\n0.\\n(5)\\n\\n?\\n?\\n(c) E continuously maps Bp,q\\n(?) into Bp,q\\nRd for all p 2 (1, 1), q 2 (0, 1] and all ? > 0.\\n?\\nThat is, there exist constants a?,p,q 0, such that, for every f 2 Bp,q\\n(?), we have\\n\\nkEf kBp,q\\n? (Rd ) ? a?,p,q kf kB ? (?) .\\np,q\\nFor detailed conditions on ? ensuring the existence of E, we refer to [8, p. 181] and [9, p. 83].\\n?\\nProperty (c) follows by some interpolation argument since Bp,q\\ncan be interpreted as interpolation\\nm0\\nm1\\nspace of the Sobolev spaces Wp and Wp for q 2 [1, 1], p 2 (1, 1), ? 2 (0, 1) and m0 , m1 2 N0\\nwith m0 6= m1 and ? = m0 (1 ?) + m1 ?, see [10, pp. 65/66] for more details. In the following,\\nwe always assume that we do have such an extension operator E. Moreover, if ? is the Lebesgue\\nmeasure on ?, such that @? has Lebesgue measure 0, the canonical extension of ? to Rd is given\\nby ?\\ne(A) := ?(A \\\\ ?) for all measurable A ? Rd . However, in a slight abuse of notation, we\\noften write ? instead of ?\\ne, since this simplifies the presentation. Analogously, we proceed for the\\nuniform distribution on ? and its canonical extension to Rd and the same convention will be applied\\nto measures PX on ? that are absolutely continuous w.r.t. the Lebesgue measure.\\nFinally, in order to state our main results, we denote the closed unit ball of the d-dimensional Euclidean space by B`d2 .\\nTheorem 1. Let X ? B`d2 be a domain such that we have an extension operator E in the above\\nsense. Furthermore, let M > 0, Y := [ M, M ], and P be a distribution on X ? Y such that\\n?\\nPX is the uniform distribution on X. Assume that we have fixed a version fL,P\\nof the regression\\n3\\n\\n\\f?\\nfunction such that fL,P\\n(x) = EP (Y |x) 2 [ M, M ] for all x 2 X. Assume that, for ?\\nr := b?c + 1, there exists a constant c > 0 such that, for all t 2 (0, 1], we have\\n\\n1 and\\n\\nThen, for all \\\" > 0 and p 2 (0, 1) there exists a constant K > 0 such that for all n\\n> 0, the SVM using the RKHS H satisfies\\n\\n1, and\\n\\n?\\n!r,L2 (Rd ) EfL,P\\n, t ? ct? .\\n\\n2\\nkfD, kH + RL,P (f?D, )\\n\\nR?L,P ? K\\n\\nwith probability Pn not less than 1\\n\\ne\\n\\n?\\n\\nd\\n\\n+ Kc2\\n\\n2?\\n\\n1, ?\\n\\n(1 p)(1+\\\")d\\n\\n+K\\n\\npn\\n\\n+\\n\\nkfD,\\n\\n2\\n\\nn\\n\\nkH\\n\\nn\\n\\n+ RL,P (f?D,\\n\\nwith probability Pn not less than 1\\nn\\nn\\n\\ne\\n\\nK?\\nn\\n\\n.\\n\\nWith this oracle inequality we can derive learning rates for the learning method (2).\\nCorollary 1. Under the assumptions of Theorem 1 and for \\\" > 0, p 2 (0, 1), and ?\\nhave, for all n 1,\\nn\\n\\n(6)\\n\\nn\\n\\n)\\n\\nR?L,P ? Cn\\n\\n2?\\n2?+2?p+dp+(1\\n\\n1 fixed, we\\n\\np)(1+\\\")d\\n\\nand with\\n\\n?\\n\\n= c1 n\\n= c2 n\\n\\n2?+d\\n2?+2?p+dp+(1 p)(1+\\\")d\\n1\\n2?+2?p+dp+(1\\n\\np)(1+\\\")d\\n\\n,\\n.\\n\\nHere, c1 > 0 and c2 > 0 are user-specified constants and C > 0 is a constant independent of n.\\nNote that for every ? > 0 we can find \\\", p 2 (0, 1) sufficiently close to 0 such that the learning rate\\nin Corollary 1 is at least as fast as\\nn\\n\\n2?\\n2?+d +?\\n\\n.\\n\\nTo achieve these rates, however, we need to set n and n as in Corollary 1, which in turn requires\\nus to know ?. Since in practice we usually do not know this value, we now show that a standard\\ntraining/validation approach, see e.g. [2, Chapters 6.5, 7.4, 8.2], achieves the same rates adaptively,\\ni.e. without knowing ?. To this end, let ? := (?n ) and := ( n ) be sequences of finite subsets\\n?n , n ? (0, 1]. For a data set D := ((x1 , y1 ) , . . . , (xn , yn )), we define\\nD1 := ((x1 , y1 ) , . . . , (xm , ym ))\\nD2 := ((xm+1 , ym+1 ) , . . . , (xn , yn ))\\n\\nwhere m :=\\nfunctions\\n\\n?n?\\n2\\n\\nfD1 ,\\n\\n+ 1 and n\\n,\\n\\n4. We will use D1 as a training set by computing the SVM decision\\n\\n:= arg min\\nf 2H\\n\\n2\\n\\nkf kH + RL,D1 (f ) ,\\n\\nand use D2 to determine ( , ) by choosing a (\\nRL,D2 fD1 ,\\n\\n=\\n\\nD2 , D2\\n\\nD2 , D 2 )\\n\\nmin\\n\\n( , )2?n ?\\n\\n( , ) 2 ?n ?\\n2 ?n ?\\nn\\n\\nn\\n\\nn\\n\\nsuch that\\n\\nRL,D2 (fD1 ,\\n\\n,\\n\\n) .\\n\\nTheorem 2. Under the assumptions of Theorem 1 we fix sequences ? := (?n ) and := ( n )\\nof finite subsets ?n , n ? (0, 1] such that ?n is an ?n -net of (0, 1] and n is an n -net of (0, 1]\\n1\\nwith ?n ? n 1 and n ? n 2+d . Furthermore, assume that the cardinalities |?n | and | n | grow\\npolynomially in n. Then, for all ? > 0, the TV-SVM producing the decision functions fD1 , D2 , D2\\nlearns with the rate\\nn\\nwith probability Pn not less than 1\\n\\ne\\n\\n?\\n\\n2?\\n2?+d +?\\n\\n(7)\\n\\n.\\n\\nWhat is left to do is to relate Assumption (6) with the function spaces introduced earlier, such\\nthat we can show that the learning rates deduced earlier are asymptotically optimal under some\\ncircumstances.\\n4\\n\\n\\fCorollary 2. Let X ? B`d2 be a domain such that we have an extension operator E of the form\\ndescribed in front of Theorem 1. Furthermore, let M > 0, Y := [ M, M ], and P be a distribution\\n?\\non X ? Y such that PX is the uniform distribution on X. If, for some ? 2 N, we have fL,P\\n2\\n?\\nW2 (PX ), then, for all ? > 0, both the SVM considered in Corollary 1 and the TV-SVM considered\\nin Theorem 2 learn with the rate\\nn\\nwith probability Pn not less than 1\\noptimal in a minmax sense.\\n\\n?\\n\\ne\\n\\n2?\\n2?+d +?\\n\\n. Moreover, if ? > d/2, then this rate is asymptotically\\n\\nSimilar to Corollary 2 we can show assumption (6) and asymptotically optimal learning rates if the\\nregression function is contained in a Besov space.\\nCorollary 3. Let X ? B`d2 be a domain such that we have an extension operator E of the form\\ndescribed in front of Theorem 1. Furthermore, let M > 0, Y := [ M, M ], and P be a distribution\\n?\\non X ? Y such that PX is the uniform distribution on X. If, for some ?\\n1, we have fL,P\\n2\\n?\\nB2,1 (PX ), then, for all ? > 0, both the SVM considered in Corollary 1 and the TV-SVM considered\\nin Theorem 2 learn with the rate\\nn\\nwith probability Pn not less than 1\\n\\ne\\n\\n?\\n\\n2?\\n2?+d +?\\n\\n.\\n?\\n\\n?\\nSince for the entropy numbers ei ( id : B2,1\\n(PX ) ! L2 (PX )) ? i d holds (cf. [7, p. 151])\\n?\\n?\\nand since B2,1 (PX ) = B2,1 (X) is continuously embedded into the space `1 (X) of all bounded\\n2?\\n\\nfunctions on X, we obtain by [11, Theorem 2.2] that n 2?+d is the optimal learning rate in a\\nminimax sense for ? > d (cf. [12, Theorem 13]). Therefore, for ? > d, the learning rates obtained\\nin Corollary 3 are asymptotically optimal.\\nSo far, we always assumed that PX is the uniform distribution on X. This can be generalized by assuming that PX is absolutely continuous w.r.t. the Lebesgue measure ? such that the corresponding\\ndensity is bounded away from zero and from infinity. Then we have L2 (PX ) = L2 (?) with equivalent norms and the results for ? hold for PX as well. Moreover, to derive learning rates, we actually\\nonly need that the Lebesgue density of PX is upper bounded. The assumption that the density is\\nbounded away from zero is only needed to derive the lower bounds in Corollaries 2 and 3.\\nFurthermore, we assumed 2 (0, 1] in Theorem 1, and hence in Corollary 1 and Theorem 2 as\\nwell. Note that does not need to be restricted by one. Instead only needs to be bounded from\\nabove by some constant such that estimates on the entropy numbers for Gaussian kernels as used in\\nthe proofs can be applied. For the sake of simplicity we have chosen one as upper bound, another\\nupper bound would only have influence on the constants.\\nThere have already been made several investigations on learning rates for SVMs using the least\\nsquares loss, see e.g. [13, 14, 15, 16, 17] and the references therein. In particular, optimal rates\\nhave been established in [16], if fP? 2 H, and the eigenvalue behavior of the integral operator\\nassociated to H is known. Moreover, if fP? 62 H [17] and [12] establish both learning rates of\\nthe form n /( +p) , where is a parameter describing the approximation properties of H and\\np is a parameter describing the eigenvalue decay. Furthermore, in the introduction of [17] it is\\nmentioned that the assumption on the eigenvalues and eigenfunctions also hold for Gaussian kernels\\nwith fixed width, but this case as well as the more interesting case of Gaussian kernels with variable\\nwidths are not further investigated. In the first case, where Gaussian kernels with fixed width are\\nconsidered, the approximation error behaves very badly as shown in [18] and fast rates cannot be\\nexpected as we discuss below. In the second case, where variable widths are considered as in our\\npaper, it is crucial to carefully control the influence of on all arising constants which unfortunately\\nhas not been worked out in [17], either. In [17] and [12], however, additional assumptions on the\\ninterplay between H and L2 (PX ) are required, and [17] actually considers a different exponent\\nin the regularization term of (2). On the other hand, [12] shows that the rate n /( +p) is often\\nasymptotically optimal in a minmax sense. In particular, the latter is the case for H = W2m (X),\\nf 2 W2s (X), and s 2 (d/2, m], that is, when using a Sobolev space as the underlying RKHS H,\\n5\\n\\n\\fthen all target functions contained in a Sobolev of lower smoothness s > d/2 can be learned with the\\n2s\\nasymptotically optimal rate n 2s+d . Here we note that the condition s > d/2 ensures by Sobolev?s\\ns\\nembedding theorem that W2 (X) consists of bounded functions, and hence Y = [ M, M ] does not\\n?\\nimpose an additional assumption on fL,P\\n. If s 2 (0, d/2], then the results of [12] still yield the\\nabove mentioned rates, but we no longer know whether they are optimal in a minmax sense, since\\nY = [ M, M ] does impose an additional assumption. In addition, note that for Sobolev spaces this\\nresult, modulo an extra log factor, has already been proved by [1]. This result suggests that by using\\na C 1 -kernel such as the Gaussian RBF kernel, one could actually learn the entire scale of Sobolev\\nspaces with the above mentioned rates. Unfortunately, however, there are good reasons to believe\\nthat this is not the case. Indeed, [18] shows that for many analytic kernels the approximation error\\n?\\ncan only have polynomial decay if fL,P\\nis analytic, too. In particular, for Gaussian kernels with\\n?\\n1\\nfixed width and fL,P 62 C the approximation error does not decay polynomially fast, see [18,\\n?\\nProposition 1.1.], and if fL,P\\n2 W2m (X), then, in general, the approximation error function only\\nhas a logarithmic decay. Since it seems rather unlikely that these poor approximation properties can\\nbe balanced by superior bounds on the estimation error, the above-mentioned results indicate that\\nGaussian kernels with fixed width may have a poor performance. This conjecture is backed-up by\\nmany empirical experience gained throughout the last decade. Beginning with [19], research has thus\\nfocused on the learning performance of SVMs with varying widths. The result that is probably the\\nclosest to ours is [20]. Although these authors actually consider binary classification using convex\\nloss functions including the least squares loss, formulated it is relatively straightforward to translate\\nm\\ntheir finding to our least squares regression scenario. The result is the learning rate n m+2d+2 , again\\n?\\nunder the assumption fL,P\\n2 W2m (X) for some m > 0. Furthermore, [21] treats the case, where X\\nis isometrically embedded into a t-dimensional, connected and compact C 1 -submanifold of Rd . In\\nthis case, it turns out that the resulting learning rate does not depend on the dimension d, but on the\\ns\\nintrinsic dimension t of the data. Namely the authors show the rate n 8s+4t modulo a logarithmic\\n?\\nfactor, where s 2 (0, 1] and fL,P 2 Lip (s). Another direction of research that can be applied to\\nGaussian kernels with varying widths are multi-kernel regularization schemes, see [22, 23, 24] for\\n2m d\\nsome results in this direction. For example, [22] establishes learning rates of the form n 4(4m d) +?\\n?\\nwhenever fL,P\\n2 W2m (X) for some m 2 (d/2, d/2 + 2), where again ? > 0 can be chosen to be\\narbitrarily close to 0. Clearly, all these results provide rates that are far from being optimal, so that\\nit seems fair to say that our results represent a significant advance. Furthermore, we can conclude\\nthat, in terms of asymptotical minmax rates, multi-kernel approaches applied Gaussian RBFs cannot\\nprovide any significant improvement over a simple training/validation approach for determining the\\nkernel width and the regularization parameter, since the latter already leads to rates that are optimal\\nmodulo an arbitrarily small ? in the exponent.\\n\\n3\\n\\nProof of the main result\\n\\nTo prove Theorem 1 we deduce an oracle inequality for the least squares loss by specializing [2,\\nTheorem 7.23] (cf. Theorem 3). To be finally able to show Theorem 1 originating from Theorem 3,\\nwe have to estimate the approximation error.\\nLemma 1. Let X ? Rd be a domain such that we have an extension operator E of the form described in front of Theorem 1, PX be the uniform distribution on X and f 2 L1 (X). Furthermore,\\nlet f? be defined by\\nd\\np\\nf? (x) :=\\n? 2 Ef (x)\\n(8)\\n> 0, K : Rd ! R be defined by\\n?\\n? d2\\nr ? ?\\nX\\nr\\n2\\n1 j 1\\np\\nK (?) :=\\n( 1)\\nK pj (?)\\n2\\nj\\njd\\n?\\nj=1\\n\\nfor all x 2 Rd and, for r 2 N and\\n\\nwith\\n\\n2\\n\\nK (?) := exp\\n\\n6\\n\\nk?k2\\n2\\n\\n!\\n\\n.\\n\\n(9)\\n\\n\\fThen, for r 2 N,\\n\\ne X ) and\\n> 0, and q 2 [1, 1), we have Ef 2 Lq (P\\nK ? f?\\n\\nf\\n\\nq\\n\\nLq (PX )\\n\\nq\\n? Cr,q !r,L\\nd (Ef, /2) ,\\nq (R )\\n\\nwhere Cr,q is a constant only depending on r, q and ?(X).\\nIn order to use the conclusion of Lemma 1 in the proof of Theorem 1 it is necessary to know some\\nproperties of K ? f?. Therefore, we need the next two lemmata.\\nLemma 2. Let g 2 L2 Rd , H be the RKHS of the Gaussian RBF kernel k over X ? Rd and\\n!\\n?\\n? d2\\nr ? ?\\n2\\nX\\n2 kxk2\\nr\\n2\\n1 j 1\\np\\nK (x) :=\\n( 1)\\nexp\\nd\\nj\\nj\\nj2 2\\n?\\nj=1\\nfor x 2 Rd and a fixed r 2 N. Then we have\\nK ?g 2H ,\\nkK ? gkH ? (2r\\n\\n1) kgkL2 (Rd ) .\\n\\nLemma 3. Let g 2 L1 Rd , H be the RKHS of the Gaussian RBF kernel k over X ? Rd and\\nK be as in Lemma 2. Then\\np d2 r\\n|K ? g (x)| ?\\n? (2\\n1) kgkL1 (Rd )\\nholds for all x 2 X. Additionally, we assume that X is a domain in Rd such that we have an\\nextension operator E of the form described in front of Theorem 1, Y := [ M, M ] and, for all x 2\\nd\\np\\n?\\n?\\nRd , f? (x) := ( ?) 2 E fL,P\\n(x) , where fL,P\\ndenotes a version of the conditional expectation\\n?\\nsuch that f\\n(x) = EP (Y |x) 2 [ M, M ] for all x 2 X. Then we have f? 2 L1 Rd and\\nL,P\\n\\nfor all x 2 X, which implies\\n\\n|K ? f? (x) | ? a0,1 (2r\\n\\n1) M\\n\\nL(y, K ? f? (x)) ? 4r a2 M 2\\n\\nfor the least squares loss L and all (x, y) 2 X ? Y .\\n\\nNext, we modify [2, Theorem 7.23], so that the proof of Theorem 1 can be build upon it.\\nTheorem 3. Let X ? B`d2 , Y := [ M, M ] ? R be a closed subset with M > 0 and P be a\\ndistribution on X ? Y . Furthermore, let L : Y ? R ! [0, 1) be the least squares loss, k be\\nthe Gaussian RBF kernel over X with width 2 (0, 1] and H be the associated RKHS. Fix an\\nf0 2 H and a constant B0 4M 2 such that kL f0 k1 ? B0 . Then, for all fixed ?\\n1, > 0,\\n\\\" > 0 and p 2 (0, 1), the SVM using H and L satisfies\\n?\\n?\\n2\\nkfD, kH + RL,P f?D,\\nR?L,P\\n?9\\n\\n?\\n\\n2\\n\\nkf0 kH + RL,P (f0 )\\n\\n?\\nR?L,P + C\\\",p\\n\\nwith probability Pn not less than 1\\n\\ne\\n\\n?\\n\\n(1 p)(1+\\\")d\\npn\\n\\n+\\n\\n3456M 2 + 15B0 (ln(3) + 1)?\\nn\\n\\n, where C\\\",p is a constant only depending on \\\", p and M .\\n\\nWith the previous results we are finally able to prove the oracle inequality declared by Theorem 1.\\nProof of Theorem 1. First of all, we want to apply Theorem 3 for f0 := K ? f? with\\n!\\n?\\n? d2\\nr ? ?\\n2\\nX\\n2 kxk2\\nr\\n2\\n1 j 1\\np\\nK (x) :=\\n( 1)\\nexp\\nj\\njd\\nj2 2\\n?\\nj=1\\n7\\n\\n\\fand\\np\\n\\nf? (x) :=\\n\\nd\\n2\\n\\n?\\n\\n?\\nEfL,P\\n(x)\\n\\n?\\n?\\nfor all x 2 Rd . The choice fL,P\\n(x) 2 [ M, M ] for all x 2 X implies fL,P\\n2 L2 (X) and the latter\\ntogether with X ? B`d2 and (5) yields\\n\\nkf?kL2 (Rd ) =\\n?\\n?\\n\\n?\\n\\np\\n\\n?\\n\\np\\n\\n?\\n\\n2\\np\\n\\nd\\n2\\nd\\n2\\n\\n?\\n\\n? d2\\n\\n?\\nkEfL,P\\nkL2 (Rd )\\n?\\na0,2 kfL,P\\nkL2 (X)\\n\\n(10)\\n\\na0,2 M ,\\n\\ni.e. f? 2 L2 Rd . Because of this and Lemma 2\\nf0 = K ? f? 2 H\\nis satisfied and with Lemma 3 we have\\nkL f0 k1 =\\n\\nsup\\n(x,y)2X?Y\\n\\n|L (y, f0 (x))| =\\n\\nsup\\n(x,y)2X?Y\\n\\nFurthermore, (1) and Lemma 1 yield\\nRL,P (f0 )\\n\\n?\\n?\\nL y, K ? f? (x) ? 4r a2 M 2 =: B0 .\\n\\n?\\n?\\nR?L,P = RL,P K ? f?\\n= K ? f?\\n\\nR?L,P\\n2\\n\\n?\\nfL,P\\nL2 (PX )\\n?\\n?\\n2\\n?\\n? Cr,2 !r,L\\nEf\\n,\\nd)\\nL,P\\n(R\\n2\\n2\\n2 2?\\n? Cr,2 c\\n,\\n\\nwhere we used the assumption\\n\\nfor 2 (0, 1], ?\\nknow\\nkf0 kH\\n\\n?\\n?\\n?\\n!r,L2 (Rd ) EfL,P\\n,\\n?c\\n2\\n\\n?\\n\\n1, r = b?c + 1 and a constant c > 0 in the last step. By Lemma 2 and (10) we\\n= kK ? f?kH ? (2r\\n\\n1) kf?kL2 (Rd ) ? (2r\\n\\n1)\\n\\n?\\n\\n2\\np\\n\\nTherefore, Theorem 3 and the above choice of f0 yield, for all fixed ?\\np 2 (0, 1), that the SVM using H and L satisfies\\n?\\n?\\n2\\nkfD, kH + RL,P f?D,\\nR?L,P\\n!\\n?\\n?d\\n2\\n2\\np\\n?9\\n(2r 1)\\na20,2 M 2 + Cr,2 c2 2?\\n?\\n\\n? C1\\n\\npn\\nd\\n\\n+ 9 Cr c2\\n\\na0,2 M .\\n\\n1,\\n\\n> 0, \\\" > 0 and\\n\\n3456 + 15 ? 4r a2 M 2 (ln(3) + 1)?\\nn\\n(1 p)(1+\\\")d\\nC2 ?\\n2?\\n+ C\\\",p\\n+\\npn\\nn\\n\\n(1 p)(1+\\\")d\\n\\n+ C\\\",p\\n\\n?\\n\\n? d2\\n\\n+\\n\\n2\\n\\nd\\n\\nwith probability Pn not less than 1 e ? and with constants C1 := 9 (2r 1) 2d ? 2 a20,2 M 2 ,\\nC2 := (ln(3) + 1) 3456 + 15 ? 4r a2 M 2 , a := max {a0,1 , 1}, Cr := Cr,2 only depending on r\\nand ?(X) and C\\\",p as in Theorem 3.\\n8\\n\\n\\fReferences\\n[1] L. Gy?orfi, M. Kohler, A. Krzy?zak, and H. Walk. A Distribution-Free Theory of Nonparametric\\nRegression. Springer-Verlag New York, 2002.\\n[2] I. Steinwart and A. Christmann. Support Vector Machines. Springer-Verlag, New York, 2008.\\n[3] R.A. DeVore and G.G. Lorentz. Constructive Approximation. Springer-Verlag Berlin Heidelberg, 1993.\\n[4] R.A. DeVore and V.A. Popov. Interpolation of Besov Spaces. AMS, Volume 305, 1988.\\n[5] H. Berens and R.A. DeVore. Quantitative Korovin theorems for positive linear operators on\\nLp -spaces. AMS, Volume 245, 1978.\\n[6] H. Johnen and K. Scherer. On the equivalence of the K-functional and moduli of continuity and\\nsome applications. In Lecture Notes in Math., volume 571, pages 119?140. Springer-Verlag\\nBerlin, 1976.\\n[7] D.E. Edmunds and H. Triebel. Function Spaces, Entropy Numbers, Differential 0perators.\\nCambridge University Press, 1996.\\n[8] E.M. Stein. Singular Integrals and Differentiability Properties of Functions. Princeton Univ.\\nPress, 1970.\\n[9] R.A. Adams and J.J.F. Fournier. Sobolev Spaces. Academic Press, 2nd edition, 2003.\\n[10] H. Triebel. Theory of Function Spaces III. Birkh?auser Verlag, 2006.\\n[11] V. Temlyakov. Optimal estimators in learning theory. Banach Center Publications, Inst. Math.\\nPolish Academy of Sciences, 72:341?366, 2006.\\n[12] I. Steinwart, D. Hush, and C. Scovel. Optimal rates for regularized least squares regression.\\nProceedings of the 22nd Annual Conference on Learning Theory, 2009.\\n[13] F. Cucker and S. Smale. On the mathematical foundations of learning. Bull. Amer. Math. Soc.,\\n39:1?49, 2002.\\n[14] E. De Vito, A. Caponnetto, and L. Rosasco. Model selection for regularized least-squares\\nalgorithm in learning theory. Found. Comput. Math., 5:59?85, 2005.\\n[15] S. Smale and D.-X. Zhou. Learning theory estimates via integral operators and their approximations. Constr. Approx., 26:153?172, 2007.\\n[16] A. Caponnetto and E. De Vito. Optimal rates for regularized least squares algorithm. Found.\\nComput. Math., 7:331?368, 2007.\\n[17] S. Mendelson and J. Neeman. Regularization in kernel learning. Ann. Statist., 38:526?565,\\n2010.\\n[18] S. Smale and D.-X. Zhou. Estimating the approximation error in learning theory. Anal. Appl.,\\nVolume 1, 2003.\\n[19] I. Steinwart and C. Scovel. Fast rates for support vector machines using Gaussian kernels. Ann.\\nStatist., 35:575?607, 2007.\\n[20] D.-H. Xiang and D.-X. Zhou. Classification with Gaussians and convex loss. J. Mach. Learn.\\nRes., 10:1447?1468, 2009.\\n[21] G.-B. Ye and D.-X. Zhou. Learning and approximation by Gaussians on Riemannian manifolds. Adv. Comput. Math., Volume 29, 2008.\\n[22] Y. Ying and D.-X. Zhou. Learnability of Gaussians with flexible variances. J. Mach. Learn.\\nRes. 8, 2007.\\n[23] C.A. Micchelli, M. Pontil, Q. Wu, and D.-X. Zhou. Error bounds for learning the kernel. 2005.\\n[24] Y. Ying and C. Campbell. Generalization bounds for learning the kernel. In S. Dasgupta and\\nA. Klivans, editors, Proceedings of the 22nd Annual Conference on Learning Theory, 2009.\\n\\n9\\n\\n\\f\",\n          \"Constructive Algorithms for Hierarchical\\nMixtures of Experts\\n\\nS.R.Waterhouse\\nA.J.Robinson\\nCambridge University Engineering Department,\\nTrumpington St., Cambridge, CB2 1PZ, England.\\nTel: [+44] 1223 332754, Fax: [+44] 1223 332662,\\nEmail: srw1001.ajr@eng.cam.ac.uk\\n\\nAbstract\\nWe present two additions to the hierarchical mixture of experts\\n(HME) architecture. By applying a likelihood splitting criteria to\\neach expert in the HME we \\\"grow\\\" the tree adaptively during training. Secondly, by considering only the most probable path through\\nthe tree we may \\\"prune\\\" branches away, either temporarily, or permanently if they become redundant . We demonstrate results for\\nthe growing and path pruning algorithms which show significant\\nspeed ups and more efficient use of parameters over the standard\\nfixed structure in discriminating between two interlocking spirals\\nand classifying 8-bit parity patterns.\\nINTRODUCTION\\n\\nThe HME (Jordan & Jacobs 1994) is a tree structured network whose terminal\\nnodes are simple function approximators in the case of regression or classifiers in the\\ncase of classification. The outputs of the terminal nodes or experts are recursively\\ncombined upwards towards the root node, to form the overall output of the network,\\nby \\\"gates\\\" which are situated at the non-terminal nodes.\\nThe HME has clear similarities with tree based statistical methods such as Classification and Regression Trees (CART) (Breiman, Friedman, Olshen & Stone 1984).\\nWe may consider the gate as replacing the set of \\\"questions\\\" which are asked at\\neach branch of CART. From this analogy, we may consider the application of the\\nsplitting rules used to build CART. We start with a simple tree consisting of two\\nexperts and one gate. After partially training this simple tree we apply the splitting criterion to each terminal node. This evaluates the log-likelihood increase by\\nsplitting each expert into two experts and a gate. The split which yields the best\\nincrease in log-likelihood is then added permanently to the tree. This process of\\ntraining followed by growing continues until the desired modelling power is reached.\\n\\n\\f585\\n\\nConstructive Algorithms for Hierarchical Mixtures of Experts\\n\\nFigure 1: A simple mixture of experts.\\nThis approach is reminiscent of Cascade Correlation (Fahlman & Lebiere 1990) in\\nwhich new hidden nodes are added to a multi-layer perceptron and trained while\\nthe rest of the network is kept fixed.\\nThe HME also has similarities with model merging techniques such as stacked regression (Wolpert 1993), in which explicit partitions of the training set are combined. However the HME differs from model merging in that each expert considers\\nthe whole input space in forming its output. Whilst this allows the network more\\nflexibility since each gate may implicitly partition the whole input space in a \\\"soft\\\"\\nmanner, it leads to unnecessarily long computation in the case of near optimally\\ntrained models. At anyone time only a few paths through a large network may\\nhave high probability. In order to overcome this drawback, we introduce the idea\\nof \\\"path pruning\\\" which considers only those paths from the root node which have\\nprobability greater than a certain threshold.\\nCLASSIFICATION USING HIERARCHICAL MIXTURES OF EXPERTS\\n\\nThe mixture of experts, shown in Figure 1, consists of a set of \\\"experts\\\" which\\nperform local function approximation . The expert outputs are combined by a gate\\nto form the overall output. In the hierarchical case, the experts are themselves\\nmixtures of further experts , thus extending the architecture in a tree structured\\nfashion. Each terminal node or \\\"expert\\\" may take on a variety of forms, depending\\non the application. In the case of multi-way classification, each expert outputs a\\nvector Yj in which element m is the conditional probability of class m (m = 1 ... M)\\nwhich is computed using the soft max function:\\nP(Cm Ix(n>, Wj)\\n\\n=\\n\\nexp(w~jx(n?)\\n\\nIt exp(w~.kX(n?)\\nk=1\\n\\nwhere Wj\\nclass i.\\n\\n= [wlj W2j\\n\\n... WMj]\\n\\nis the parameter matrix for expert j and\\n\\nCi\\n\\ndenotes\\n\\nThe outputs of the experts are combined using a \\\"gate\\\" which sits at the nonterminal nodes. The gate outputs are estimates of the conditional probability of\\nselecting the daughters of the non-terminal node given the input and the path taken\\nto that node from the root node. This is once again computed using the softmax\\nfunction:\\n\\n~ (.), ~) =exp(~ J~(.?\\n\\nP(Zj I\\n\\nwhere';\\nj.\\n\\n= [';1';2\\n\\nIt\\n\\n<xp(\\n\\n~f ~(.?\\n\\n... ';f] is the parameter matrix for the gate, and\\n\\nZj\\n\\ndenotes expert\\n\\n\\f586\\n\\nS. R. WATERHOUSE, A. 1. ROBINSON\\n\\nThe overall output is given by a probabilistic mixture in which the gate outputs\\nare the mixture weights and the expert outputs are the mixture components. The\\nprobability of class m is then given by:\\n]\\n\\nP(cmlz(n),8)\\n\\n=\\n\\nI: P(zilz(n), ~)P(Cmlz(n), Wi).\\ni=1\\n\\nA straightforward extension of this model also gives us the conditional probability\\n\\nht) of selecting expert j given input zen) and correct class\\n\\nCk,\\n\\nIn order to train the HME to perform classification we maximise the log likelihood\\n= l:~=1 l:~=1 t~) log P(cmIz(n), 8), where the variable t~) is one if m is the correct\\nclass at exemplar (n) and zero otherwise. This is done via the expectation maximisation (EM) algorithm of Dempster, Laird & Rubin (1977), as described by Jordan\\n& Jacobs (1994).\\n\\nL\\n\\nTREE GROWING\\n\\nThe standard HME differs from most tree\\nbased statistical models in that its architecture\\nis fixed. By relaxing this constraint and allowing the tree to grow, we achieve a greater degree of flexibility in the network. Following the\\nwork on CART we start with a simple tree, for\\ninstance with two experts and one gate which\\nwe train for a small number of cycles. Given\\nthis semi-trained network, we then make a set\\nof candidate splits {&} of terminal nodes {z;}.\\nEach split involves replacin~ an expert Zi with\\na pair of new experts {Zu}j=1 and a gate, as\\nshown in Figure 2.\\n\\n\\\\\\n\\n\\\\\\n\\n\\\\\\n\\n\\\\\\n\\nL(P+I)\\nL(P)\\nWe wish to select eventually only the \\\"best\\\"\\nsplit S out of these candidate splits. Let us\\ndefine the best split as being that which maxFigure 2: Making a canimises the increase in overall log-likelihood due\\ndidate split of a terminal\\nto the split, IlL = L(P+1) - L(P) where L(P) is the\\nnode.\\nlikelihood at the pth generation of the tree. If\\nwe make the constraint that all the parameters\\nof the tree remain fixed apart from the parameters of the new split whenever a candidate split is made, then the maximisation\\nis simplified into a dependency on the increases in the local likelihoods {Li} of the\\nnodes {Zi}. We thus constrain the tree growing process to be localised such that we\\nfind the node which gains the most by being split.\\n\\nmax M(&)\\ni\\n\\n_ I\\nmax M?\\n\\ni\\n\\n=max(Ly*1)\\ni\\nI\\n\\nL(P?\\nI\\n\\n\\fConstructive Algorithms for Hierarchical Mixtures of Experts\\n\\n587\\n\\nFigure 3: Growing the HME. This figure shows the addition of a pair of experts to\\nthe partially grown tree.\\nwhere\\nL~+l)\\n\\n=\\n\\nn\\n\\nm\\n\\nn\\n\\nm\\n\\nL L t~) log L P(zijlz(n), c;;,zi)P(cmlz(n), zij, wij)\\nj\\n\\nThis splitting rule is similar in form to the CART splitting criterion which uses\\nmaximisation of the entropy of the node split, equivalent to our local increase in\\nlop;-likelihood.\\nTIle final growing algorithm starts with a tree of generation p and firstly fixes the\\nparameters of all non-terminal nodes. All terminal nodes are then split into two\\nexperts and a gate. A split is only made if the sum of posterior probabilities En h~n),\\nas described (1), at the node is greater than a small threshold. This prevents splits\\nbeing made on nodes which have very little data assigned to them. In order to break\\nsymmetry, the new experts of a split are initialised by adding small random noise\\nto the original expert parameters. The gate parameters are set to small random\\nweights. For each node i, we then evaluate M; by training the tree using the\\nstandard EM method. Since all non-terminal node parameters are fixed the only\\nchanges to the log-likelihood are due the new splits. Since the parameters of each\\nsplit are thus independent of one another, all splits can be trained at once, removing\\nthe need to train multiple trees separately.\\nAfter each split has been evaluated, the best split is chosen. This split is kept and\\nall other splits are discarded. The original tree structure is then recovered except\\nfor the additional winning split, as shown in Figure 3. The new tree, of generation\\np + I is then trained as usual using EM. At present the decision on when to add\\na new split to the tree is fairly straightforward: a candidate split is made after\\ntraining the fixed tree for a set number of iterations. An alternative scheme we\\nhave investigated is to make a split when the overall log-likelihood of the fixed tree\\nhas not increased for a set number of cycles. In addition, splits are rejected if they\\nadd too little to the local log-likelihood.\\nAlthough we have not discussed the issue of over-fitting in this paper, a number\\nof techniques to prevent over-fitting can be used in the HME. The most simple\\ntechnique, akin to those used in CART, involves growing a large tree and successively\\nremoving nodes from the tree until the performance on a cross validation set reaches\\nan optimum. Alternatively the Bayesian techniques of Waterhouse, MacKay &\\nRobinson (1995) could be applied.\\n\\n\\fS. R. WATERHOUSE, A. J. ROBINSON\\n\\n588\\nTree growing simulations\\n\\nThis algorithm was used to solve the 8-bit parity classification task. We compared\\nthe growing algorithm to a fixed HME with depth of 4 and binary branches. As can\\nbe seen in Figures 4(a) and (b), the factorisation enabled by the growing algorithm\\nsignificantly speeds up computation over the standard fixed structure. The final tree\\nshape obtained is shown in Figure 4(c). We showed in an earlier paper (Waterhouse\\n& Robinson 1994) that the XOR problem may be solved using at least 2 experts\\nand a gate. The 8 bit parity problem is therefore being solved by a series of XOR\\nclassifiers, each gated by its parent node, which is an intuitively appealing form\\nwith an efficient use of parameters.\\n\\n-200oL----1~0----2~0~--~~~--~4~0--~W\\nTime\\n\\n(a) Evolution of log-likelihood vs.\\ntime in CPU seconds.\\n\\nO'()OI\\n\\n0.001\\n\\n(c) Final tree structure obtained from\\n(i), showing utilisation U; of each node\\nwhere U; = L: P(z;, R;I:c(n?) I N, and Ri\\nis the path t~en from the root node\\nto node i .\\n\\n-50\\n\\n8\\n\\n,E\\n\\n~-100\\n\\\"I\\n\\n.?'\\n-150\\n\\n-2000 1 2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\nGeneration\\n\\n(b) Evolution of log-likelihood for (i)\\nvs generations of tree.\\nFigure 4: HME GROWING ON THE 8 BIT PARITY PROBLEM;(i) growing HME with 6\\ngenerations; (ii) 4 deep binary branching HME (no growing).\\nPATH PRUNING\\n\\nIf we consider the HME to be a good model for the data generation process, the\\ncase for path pruning becomes clear. In a tree with sufficient depth to model the\\n\\n\\fConstructive Algorithms for Hierarchical Mixtures of Experts\\n\\n589\\n\\nunderlying sub-processes producing each data point, we would expect the activation\\nof each expert to tend to binary values such that only one expert is selected at each\\ntime exemplar.\\nThe path pruning scheme is depicted in Figure 5. The pruning scheme utilises\\nthe \\\"activation\\\" of each node at each exemplar. The activation is defined as\\nthe product of node probabilities along a path from the root node to the current node, lin) = Li log P(zi/R i , :.:(n?), where Ri is the path taken to node i from\\nthe root node . If .l}n) for node l at exemplar n falls below a threshold value,\\nft, then we ignore the subtree Sl and we backtrack up to the parent node of l.\\nDuring training this involves not accumulating the statistics of the subtree Sl; during evaluation it involves\\nsetting the output of subtree Sl to\\nzero. In addition to this path pruning scheme we can use the activation of the nodes to do more permanent pruning. If the overall utilisation Vi = Ln P(Zi, Rd:.:(n?)IN of a node\\nfalls below a small threshold, then a\\nnode is pruned completely from the\\ntree. The sister subtrees of the removed node then subsume their parent nodes. This process is used solely\\nto improve computational efficiency\\nin this paper, although conceivably\\nit could be used as a regularisation\\nmethod, akin to the brain surgery\\ntechniques of Cun, Denker & Solla\\n..... _---_ .. - ..\\n(1990). In such a scheme, however,\\na more useful measure of node utiliFigure 5: Path pruning in the HME.\\nsation would be the effective number\\nof parameters (Moody 1992).\\nPath pruning simulations\\n\\nFigure 6 shows the application of the pruning algorithm to the task of discriminating\\nbetween two interlocking spirals. With no pruning the solution to the two-spirals\\ntakes over 4,000 CPU seconds, whereas with pruning the solution is achieved in 155\\nCPU seconds.\\nOne problem which we encountered when implementing this algorithm was in computing updates for the parameters of the tree in the case of high pruning thresholds.\\nIf a node is visited too few times during a training pass, it will sometimes have too\\nlittle data to form reliable statistics and thus the new parameter values may be\\nunreliable and lead to instability. This is particularly likely when the gates are\\nsaturated. To avoid this saturation we use a simplified version of the regularisation\\nscheme described in Waterhouse et al. (1995).\\nCONCLUSIONS\\n\\nWe have presented two extensions to the standard HME architecture. By pruning\\nbranches either during training or evaluation we may significantly reduce the computational requirements of the HME. By applying tree growing we allow greater\\nflexibility in the HME which results in faster training and more efficient use of\\nparameters.\\n\\n\\fS. R. WATERHOUSE, A. J. ROBINSON\\n\\n590\\n\\n(a)\\n\\n(b)\\n\\n0\\n\\n-20\\n\\\"C\\n0\\n0\\n\\n;5\\n\\n~\\n\\nT0>\\n0\\n\\n,.,fi\\n\\n\\\"\\n\\n-40\\n\\n,:..\\n\\n(iii\\n(iv)\\n\\n.i\\n\\n-60\\n\\n....' :1\\n,.,.\\n\\n, ,\\n, ,\\n,I\\n\\n(c)\\n\\n-80\\n\\n....J\\n\\n-100\\nI .\\n\\n-120\\n\\n.\\\"\\\"\\n\\n~ -,~ '.'- '\\n\\n10\\n\\n100\\n\\n( .' /\\n.\\\"..: '\\\"\\n' ,,,..\\n\\nTime (5)\\n\\n/\\n\\n1000\\n\\nFigure 6: The effect of pruning on the two spirals classification problem by a 8\\ndeep binary branching hme:(a) Log-likelihood vs. Time (CPU seconds), with log pruning\\nthresholds for experts and gates f: (i) f = -5. 6,(ii) f = -lO,(iii) f = -15,(iv) no pruning,\\n(b) training set for two-spirals task; the two classes are indicated by crosses and circles,\\n(c) Solution to two spirals problem.\\n\\nReferences\\nBreiman, L., Friedman, J., Olshen, R. & Stone, C . J. (1984), Classification and\\nRegression Trees, Wadswoth and Brooks/Cole.\\nCun, Y . L., Denker, J. S. & Solla, S. A. (1990), Optimal brain damage, in D. S.\\nTouretzky, ed., 'Advances in Neural Information Processing Systems 2', Morgan Kaufmann, pp. 598-605.\\nDempster, A. P., Laird, N. M. & Rubin, D. B. (1977), 'Maximum likelihood from\\nincomplete data via the EM algorithm', Journal of the Royal Statistical Society,\\nSeries B 39, 1-38.\\nFahlman, S. E. & Lebiere, C. (1990), The Cascade-Correlation learning architecture, Technical Report CMU-CS-90-100, School of Computer Science, Carnegie\\nMellon University, Pittsburgh, PA 15213.\\nJordan, M. I. & Jacobs, R. A. (1994), 'Hierarchical Mixtures of Experts and the\\nEM algorithm', Neural Computation 6, 181-214.\\nMoody, J. E. (1992), The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems, in J. E. Moody, S. J.\\nHanson & R. P. Lippmann, eds, 'Advances in Neural Information Processing\\nSystems 4', Morgan Kaufmann, San Mateo, California, pp. 847-854.\\nWaterhouse, S. R. & Robinson, A. J . (1994), Classification using hierarchical mixtures of experts, in 'IEEE Workshop on Neural Networks for Signal Processing',\\npp. 177-186.\\nWaterhouse, S. R., MacKay, D. J. C. & Robinson, A. J . (1995), Bayesian methods\\nfor mixtures of experts, in M. C. M. D. S. Touretzky & M. E. Hasselmo, eds,\\n'Advances in Neural Information Processing Systems 8', MIT Press.\\nWolpert, D. H . (1993), Stacked generalization, Technical Report LA-UR-90-3460,\\nThe Santa Fe Institute, 1660 Old Pecos Trail, Suite A, Santa Fe, NM, 87501.\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "papers"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-55cad292-1739-45cb-8659-238206f9c8f6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2332</th>\n",
              "      <td>Stability of K-Means Clustering\\nAlexander Rak...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3690</th>\n",
              "      <td>Bayesian Partitioning of Large-Scale Distance ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>902</th>\n",
              "      <td>Algebraic Information Geometry for\\nLearning M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2967</th>\n",
              "      <td>Translating Locative Prepositions\\n\\nPaul W. M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5001</th>\n",
              "      <td>Near-optimal sample compression\\nfor nearest n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55cad292-1739-45cb-8659-238206f9c8f6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-55cad292-1739-45cb-8659-238206f9c8f6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-55cad292-1739-45cb-8659-238206f9c8f6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dcb52df1-7e1e-40e1-a4f0-f5edee71ef04\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dcb52df1-7e1e-40e1-a4f0-f5edee71ef04')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dcb52df1-7e1e-40e1-a4f0-f5edee71ef04 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                             paper_text\n",
              "2332  Stability of K-Means Clustering\\nAlexander Rak...\n",
              "3690  Bayesian Partitioning of Large-Scale Distance ...\n",
              "902   Algebraic Information Geometry for\\nLearning M...\n",
              "2967  Translating Locative Prepositions\\n\\nPaul W. M...\n",
              "5001  Near-optimal sample compression\\nfor nearest n..."
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remove the columns\n",
        "papers = papers.drop(columns=['id', 'title', 'abstract',\n",
        "                              'event_type', 'pdf_name', 'year'], axis=1)\n",
        "\n",
        "# sample only 100 papers\n",
        "papers = papers.sample(100)\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gyUm--s58MJ"
      },
      "source": [
        "##### Remove punctuation/lower casing\n",
        "\n",
        "Next, let’s perform a simple preprocessing on the content of paper_text column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "6Cljuc6X58MK",
        "outputId": "f8e7a9d0-0084-4196-d49b-9d57e133446a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_text_processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2332</th>\n",
              "      <td>stability of k-means clustering\\nalexander rak...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3690</th>\n",
              "      <td>bayesian partitioning of large-scale distance ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>902</th>\n",
              "      <td>algebraic information geometry for\\nlearning m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2967</th>\n",
              "      <td>translating locative prepositions\\n\\npaul w mu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5001</th>\n",
              "      <td>near-optimal sample compression\\nfor nearest n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ],
            "text/plain": [
              "2332    stability of k-means clustering\\nalexander rak...\n",
              "3690    bayesian partitioning of large-scale distance ...\n",
              "902     algebraic information geometry for\\nlearning m...\n",
              "2967    translating locative prepositions\\n\\npaul w mu...\n",
              "5001    near-optimal sample compression\\nfor nearest n...\n",
              "Name: paper_text_processed, dtype: object"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the regular expression library\n",
        "import re\n",
        "\n",
        "# Remove punctuation\n",
        "papers['paper_text_processed'] = papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "\n",
        "# Convert the titles to lowercase\n",
        "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: x.lower())\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers['paper_text_processed'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES3Wb7x658MK"
      },
      "source": [
        "##### Tokenize words and further clean-up text\n",
        "\n",
        "Let’s tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amSJ5AqY58MK",
        "outputId": "d92f632f-10bc-40a9-b640-869be603e397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['stability', 'of', 'means', 'clustering', 'alexander', 'rakhlin', 'department', 'of', 'computer', 'science', 'uc', 'berkeley', 'berkeley', 'ca', 'rakhlin', 'csberkeleyedu', 'andrea', 'caponnetto', 'department', 'of', 'computer', 'science', 'university', 'of', 'chicago', 'chicago', 'il', 'and', 'disi', 'universit']\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data = papers.paper_text_processed.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "print(data_words[:1][0][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGZWQtBg58MK"
      },
      "source": [
        "** **\n",
        "#### Step 3: Phrase Modeling: Bigram and Trigram Models\n",
        "** **\n",
        "\n",
        "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring. Some examples in our example are: 'back_bumper', 'oil_leakage', 'maryland_college_park' etc.\n",
        "\n",
        "Gensim's Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold.\n",
        "\n",
        "*The higher the values of these param, the harder it is for words to be combined.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue6Rn0gS58ML"
      },
      "outputs": [],
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX3kajy658ML"
      },
      "source": [
        "#### Remove Stopwords, Make Bigrams and Lemmatize\n",
        "\n",
        "The phrase models are ready. Let’s define the functions to remove the stopwords, make trigrams and lemmatization and call them sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUgztoBF58ML",
        "outputId": "d617d16d-74e9-4228-ca76-fc2109103703"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# NLTK Stop words\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cDviZyX58ML"
      },
      "outputs": [],
      "source": [
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u04axqr58ML"
      },
      "source": [
        "Let's call the functions in order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8izI89D58MM",
        "outputId": "9c4f06df-4657-4ca1-9055-772d3581eb7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKZm-xwH58MM",
        "outputId": "41d2ea35-b99a-4656-fdb9-0c6bdf5f7759"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['stability', 'means_clustere', 'alexander', 'abstract', 'phrase', 'means_clustere', 'minimization', 'procedure', 'class', 'explicitly', 'calculate', 'cover', 'number', 'class', 'next', 'show', 'stability', 'means_clustere', 'characterize', 'geometry', 'respect', 'underlying', 'distribution', 'prove', 'case', 'unique', 'global', 'minimizer', 'cluster', 'solution']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YXmdO5P58MM"
      },
      "source": [
        "** **\n",
        "#### Step 4: Data transformation: Corpus and Dictionary\n",
        "** **\n",
        "\n",
        "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmOfhFtL58MM",
        "outputId": "1fe73c21-a423-43d8-d812-e08bdb2acf48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 6), (6, 1), (7, 1), (8, 1), (9, 1), (10, 7), (11, 1), (12, 1), (13, 3), (14, 1), (15, 1), (16, 1), (17, 2), (18, 1), (19, 1), (20, 5), (21, 2), (22, 5), (23, 1), (24, 2), (25, 2), (26, 1), (27, 1), (28, 2), (29, 1)]\n"
          ]
        }
      ],
      "source": [
        "import gensim.corpora as corpora\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1][0][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIDQb7cd58MM"
      },
      "source": [
        "** **\n",
        "#### Step 5: Base Model\n",
        "** **\n",
        "\n",
        "We have everything required to train the base LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well. Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior (we'll use default for the base model).\n",
        "\n",
        "chunksize controls how many documents are processed at a time in the training algorithm. Increasing chunksize will speed up training, at least as long as the chunk of documents easily fit into memory.\n",
        "\n",
        "passes controls how often we train the model on the entire corpus (set to 10). Another word for passes might be \"epochs\". iterations is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. It is important to set the number of \"passes\" and \"iterations\" high enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCJSv-0A58MN"
      },
      "outputs": [],
      "source": [
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=10,\n",
        "                                       random_state=100,\n",
        "                                       chunksize=100,\n",
        "                                       passes=10,\n",
        "                                       per_word_topics=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCBrrn1Y58MN"
      },
      "source": [
        "** **\n",
        "The above LDA model is built with 10 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.\n",
        "\n",
        "You can see the keywords for each topic and the weightage(importance) of each keyword using `lda_model.print_topics()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DU47R6K58MN",
        "outputId": "e8d45825-d30b-4fe9-ca93-b6f738314b18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0,\n",
            "  '0.018*\"model\" + 0.010*\"use\" + 0.009*\"learn\" + 0.008*\"time\" + 0.008*\"value\" '\n",
            "  '+ 0.007*\"state\" + 0.007*\"topic\" + 0.006*\"distribution\" + 0.005*\"system\" + '\n",
            "  '0.005*\"set\"'),\n",
            " (1,\n",
            "  '0.011*\"model\" + 0.011*\"datum\" + 0.010*\"image\" + 0.009*\"use\" + 0.007*\"set\" + '\n",
            "  '0.006*\"vector\" + 0.006*\"show\" + 0.006*\"matrix\" + 0.005*\"result\" + '\n",
            "  '0.005*\"learn\"'),\n",
            " (2,\n",
            "  '0.009*\"function\" + 0.008*\"network\" + 0.008*\"state\" + 0.007*\"model\" + '\n",
            "  '0.006*\"problem\" + 0.006*\"layer\" + 0.006*\"time\" + 0.005*\"constraint\" + '\n",
            "  '0.005*\"switch\" + 0.005*\"system\"'),\n",
            " (3,\n",
            "  '0.019*\"model\" + 0.011*\"category\" + 0.011*\"learn\" + 0.008*\"label\" + '\n",
            "  '0.008*\"use\" + 0.007*\"set\" + 0.006*\"datum\" + 0.006*\"show\" + '\n",
            "  '0.006*\"parameter\" + 0.006*\"rate\"'),\n",
            " (4,\n",
            "  '0.014*\"matrix\" + 0.009*\"model\" + 0.008*\"use\" + 0.007*\"example\" + '\n",
            "  '0.007*\"method\" + 0.006*\"set\" + 0.006*\"datum\" + 0.006*\"time\" + 0.005*\"show\" '\n",
            "  '+ 0.005*\"vector\"'),\n",
            " (5,\n",
            "  '0.014*\"model\" + 0.013*\"subscriber\" + 0.007*\"datum\" + 0.007*\"neuron\" + '\n",
            "  '0.006*\"show\" + 0.006*\"time\" + 0.006*\"churn\" + 0.005*\"vlvp\" + 0.005*\"block\" '\n",
            "  '+ 0.005*\"cost\"'),\n",
            " (6,\n",
            "  '0.020*\"time\" + 0.013*\"context\" + 0.010*\"process\" + 0.009*\"regret\" + '\n",
            "  '0.009*\"continuous\" + 0.008*\"model\" + 0.008*\"action\" + 0.007*\"show\" + '\n",
            "  '0.007*\"reward\" + 0.006*\"distribution\"'),\n",
            " (7,\n",
            "  '0.010*\"learn\" + 0.010*\"network\" + 0.009*\"model\" + 0.009*\"point\" + '\n",
            "  '0.007*\"use\" + 0.007*\"set\" + 0.007*\"time\" + 0.006*\"result\" + '\n",
            "  '0.006*\"function\" + 0.005*\"problem\"'),\n",
            " (8,\n",
            "  '0.013*\"use\" + 0.013*\"image\" + 0.009*\"object\" + 0.009*\"learn\" + '\n",
            "  '0.008*\"model\" + 0.006*\"example\" + 0.006*\"set\" + 0.006*\"result\" + '\n",
            "  '0.006*\"space\" + 0.005*\"number\"'),\n",
            " (9,\n",
            "  '0.010*\"set\" + 0.008*\"matrix\" + 0.008*\"function\" + 0.007*\"use\" + '\n",
            "  '0.007*\"result\" + 0.006*\"learn\" + 0.006*\"problem\" + 0.006*\"datum\" + '\n",
            "  '0.006*\"method\" + 0.006*\"show\"')]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYp3riTO58MN"
      },
      "source": [
        "#### Compute Model Perplexity and Coherence Score\n",
        "\n",
        "Let's calculate the baseline coherence score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nQYY0IO58MN",
        "outputId": "994acecb-1fdf-43d5-a7d4-21bc4e891162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coherence Score:  0.27102595915672734\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('Coherence Score: ', coherence_lda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFwrZqjo58MO"
      },
      "source": [
        "** **\n",
        "#### Step 6: Hyperparameter tuning\n",
        "** **\n",
        "First, let's differentiate between model hyperparameters and model parameters :\n",
        "\n",
        "- `Model hyperparameters` can be thought of as settings for a machine learning algorithm that are tuned by the data scientist before training. Examples would be the number of trees in the random forest, or in our case, number of topics K\n",
        "\n",
        "- `Model parameters` can be thought of as what the model learns during training, such as the weights for each word in a given topic.\n",
        "\n",
        "Now that we have the baseline coherence score for the default LDA model, let's perform a series of sensitivity tests to help determine the following model hyperparameters:\n",
        "- Number of Topics (K)\n",
        "- Dirichlet hyperparameter alpha: Document-Topic Density\n",
        "- Dirichlet hyperparameter beta: Word-Topic Density\n",
        "\n",
        "We'll perform these tests in sequence, one parameter at a time by keeping others constant and run them over the two difference validation corpus sets. We'll use `C_v` as our choice of metric for performance comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMa59aEp58MO"
      },
      "outputs": [],
      "source": [
        "# supporting function\n",
        "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
        "\n",
        "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=dictionary,\n",
        "                                           num_topics=k,\n",
        "                                           random_state=100,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha=a,\n",
        "                                           eta=b)\n",
        "\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "\n",
        "    return coherence_model_lda.get_coherence()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZt7daPX58MO"
      },
      "source": [
        "Let's call the function, and iterate it over the range of topics, alpha, and beta parameter values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkTHHcp458MO",
        "outputId": "413a64cc-3866-4218-f37c-045534581dfd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 194/540 [29:58<57:51, 10.03s/it]"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "grid = {}\n",
        "grid['Validation_Set'] = {}\n",
        "\n",
        "# Topics range\n",
        "min_topics = 2\n",
        "max_topics = 11\n",
        "step_size = 1\n",
        "topics_range = range(min_topics, max_topics, step_size)\n",
        "\n",
        "# Alpha parameter\n",
        "alpha = list(np.arange(0.01, 1, 0.3))\n",
        "alpha.append('symmetric')\n",
        "alpha.append('asymmetric')\n",
        "\n",
        "# Beta parameter\n",
        "beta = list(np.arange(0.01, 1, 0.3))\n",
        "beta.append('symmetric')\n",
        "\n",
        "# Validation sets\n",
        "num_of_docs = len(corpus)\n",
        "corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)),\n",
        "               corpus]\n",
        "\n",
        "corpus_title = ['75% Corpus', '100% Corpus']\n",
        "\n",
        "model_results = {'Validation_Set': [],\n",
        "                 'Topics': [],\n",
        "                 'Alpha': [],\n",
        "                 'Beta': [],\n",
        "                 'Coherence': []\n",
        "                }\n",
        "\n",
        "# Can take a long time to run\n",
        "if 1 == 1:\n",
        "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
        "\n",
        "    # iterate through validation corpuses\n",
        "    for i in range(len(corpus_sets)):\n",
        "        # iterate through number of topics\n",
        "        for k in topics_range:\n",
        "            # iterate through alpha values\n",
        "            for a in alpha:\n",
        "                # iterare through beta values\n",
        "                for b in beta:\n",
        "                    # get the coherence score for the given parameters\n",
        "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word,\n",
        "                                                  k=k, a=a, b=b)\n",
        "                    # Save the model results\n",
        "                    model_results['Validation_Set'].append(corpus_title[i])\n",
        "                    model_results['Topics'].append(k)\n",
        "                    model_results['Alpha'].append(a)\n",
        "                    model_results['Beta'].append(b)\n",
        "                    model_results['Coherence'].append(cv)\n",
        "\n",
        "                    pbar.update(1)\n",
        "    pd.DataFrame(model_results).to_csv('/NIPS Papers.zip', index=False)\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rYKRmhP58MP"
      },
      "source": [
        "** **\n",
        "#### Step 7: Final Model\n",
        "** **\n",
        "\n",
        "Based on external evaluation (Code to be added from Excel based analysis), let's train the final model with parameters yielding highest coherence score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAF0SZNs58MP"
      },
      "outputs": [],
      "source": [
        "num_topics = 8\n",
        "\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=num_topics,\n",
        "                                           random_state=100,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha=0.01,\n",
        "                                           eta=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-MvTn8Q58MP"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Glkl792P58MP"
      },
      "source": [
        "** **\n",
        "#### Step 8: Visualize Results\n",
        "** **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PT97W4he58MP"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pickle\n",
        "import pyLDAvis\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "LDAvis_data_filepath = os.path.join('./results/ldavis_tuned_'+str(num_topics))\n",
        "\n",
        "# # this is a bit time consuming - make the if statement True\n",
        "# # if you want to execute visualization prep yourself\n",
        "if 1 == 1:\n",
        "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)\n",
        "\n",
        "# load the pre-prepared pyLDAvis data from disk\n",
        "with open(LDAvis_data_filepath, 'rb') as f:\n",
        "    LDAvis_prepared = pickle.load(f)\n",
        "\n",
        "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_tuned_'+ str(num_topics) +'.html')\n",
        "\n",
        "LDAvis_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYB8qx_458MQ"
      },
      "source": [
        "** **\n",
        "#### Closing Notes\n",
        "\n",
        "We started with understanding why evaluating the topic model is essential. Next, we reviewed existing methods and scratched the surface of topic coherence, along with the available coherence measures. Then we built a default LDA model using Gensim implementation to establish the baseline coherence score and reviewed practical ways to optimize the LDA hyperparameters.\n",
        "\n",
        "Hopefully, this article has managed to shed light on the underlying topic evaluation strategies, and intuitions behind it.\n",
        "\n",
        "** **\n",
        "#### References:\n",
        "1. http://qpleple.com/perplexity-to-evaluate-topic-models/\n",
        "2. https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020\n",
        "3. https://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf\n",
        "4. https://github.com/mattilyra/pydataberlin-2017/blob/master/notebook/EvaluatingUnsupervisedModels.ipynb\n",
        "5. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
        "6. http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf\n",
        "7. http://palmetto.aksw.org/palmetto-webapp/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}